{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.models import resnet152\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(224), transforms.ToTensor() \n",
    "])\n",
    "\n",
    "root = \"/root/\"\n",
    "\n",
    "train_dataset = FashionMNIST(\n",
    "    root=root, train=True, transform=transforms, download=True\n",
    ")\n",
    "\n",
    "val_dataset = FashionMNIST(\n",
    "    root=root, train=False, transform=transforms, download=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 9,\n",
      "        1, 4, 6, 0, 9, 3, 8, 8])]\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(val_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\yuhei\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "class Resnet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self._initModel(num_classes)\n",
    "\n",
    "    def _initModel(self, num_classes):\n",
    "        pre_model = resnet152(pretrained=True)\n",
    "        num_fc_in_features = pre_model.fc.in_features\n",
    "        pre_model.fc = nn.Linear(num_fc_in_features, num_classes)\n",
    "        pre_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.model = pre_model\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.model(images)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Resnet(num_classes=10)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (8): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (9): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (10): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (11): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (12): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (13): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (14): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (15): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (16): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (17): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (18): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (19): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (20): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (21): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (22): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (23): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (24): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (25): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (26): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (27): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (28): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (29): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (30): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (31): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (32): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (33): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (34): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (35): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience):\n",
    "        self.check_count = 0\n",
    "        self.patience = patience\n",
    "\n",
    "    def checkCount(self, _bool):\n",
    "        if _bool:\n",
    "            self.check_count = 0\n",
    "        else:\n",
    "            self.check_count += 1\n",
    "\n",
    "        if self.check_count == self.patience:\n",
    "            return 0\n",
    "        else:\n",
    "            return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: Train Epoch: 1/1 Iteration: 1/1875 train_loss0.23555335402488708\n",
      "STEP: Train Epoch: 1/1 Iteration: 2/1875 train_loss0.21707481145858765\n",
      "STEP: Train Epoch: 1/1 Iteration: 3/1875 train_loss0.23578527569770813\n",
      "STEP: Train Epoch: 1/1 Iteration: 4/1875 train_loss0.21897951513528824\n",
      "STEP: Train Epoch: 1/1 Iteration: 5/1875 train_loss0.27478161454200745\n",
      "STEP: Train Epoch: 1/1 Iteration: 6/1875 train_loss0.25891172140836716\n",
      "STEP: Train Epoch: 1/1 Iteration: 7/1875 train_loss0.29184780589171816\n",
      "STEP: Train Epoch: 1/1 Iteration: 8/1875 train_loss0.2910811398178339\n",
      "STEP: Train Epoch: 1/1 Iteration: 9/1875 train_loss0.28937044574154747\n",
      "STEP: Train Epoch: 1/1 Iteration: 10/1875 train_loss0.2883211746811867\n",
      "STEP: Train Epoch: 1/1 Iteration: 11/1875 train_loss0.28545721958984027\n",
      "STEP: Train Epoch: 1/1 Iteration: 12/1875 train_loss0.27171996732552844\n",
      "STEP: Train Epoch: 1/1 Iteration: 13/1875 train_loss0.2617832147158109\n",
      "STEP: Train Epoch: 1/1 Iteration: 14/1875 train_loss0.2629323090825762\n",
      "STEP: Train Epoch: 1/1 Iteration: 15/1875 train_loss0.25601213574409487\n",
      "STEP: Train Epoch: 1/1 Iteration: 16/1875 train_loss0.25222501531243324\n",
      "STEP: Train Epoch: 1/1 Iteration: 17/1875 train_loss0.24959043369573705\n",
      "STEP: Train Epoch: 1/1 Iteration: 18/1875 train_loss0.2515226403872172\n",
      "STEP: Train Epoch: 1/1 Iteration: 19/1875 train_loss0.24767790264204928\n",
      "STEP: Train Epoch: 1/1 Iteration: 20/1875 train_loss0.23849470987915994\n",
      "STEP: Train Epoch: 1/1 Iteration: 21/1875 train_loss0.23978546758492789\n",
      "STEP: Train Epoch: 1/1 Iteration: 22/1875 train_loss0.2459641769528389\n",
      "STEP: Train Epoch: 1/1 Iteration: 23/1875 train_loss0.24411034713620725\n",
      "STEP: Train Epoch: 1/1 Iteration: 24/1875 train_loss0.2471008524298668\n",
      "STEP: Train Epoch: 1/1 Iteration: 25/1875 train_loss0.25104775667190554\n",
      "STEP: Train Epoch: 1/1 Iteration: 26/1875 train_loss0.2537649961618277\n",
      "STEP: Train Epoch: 1/1 Iteration: 27/1875 train_loss0.2503761814700233\n",
      "STEP: Train Epoch: 1/1 Iteration: 28/1875 train_loss0.24620946922472545\n",
      "STEP: Train Epoch: 1/1 Iteration: 29/1875 train_loss0.2505413191071872\n",
      "STEP: Train Epoch: 1/1 Iteration: 30/1875 train_loss0.2485289454460144\n",
      "STEP: Train Epoch: 1/1 Iteration: 31/1875 train_loss0.2516245649706933\n",
      "STEP: Train Epoch: 1/1 Iteration: 32/1875 train_loss0.24839988769963384\n",
      "STEP: Train Epoch: 1/1 Iteration: 33/1875 train_loss0.2573318684642965\n",
      "STEP: Train Epoch: 1/1 Iteration: 34/1875 train_loss0.2594473392647855\n",
      "STEP: Train Epoch: 1/1 Iteration: 35/1875 train_loss0.2638841037239347\n",
      "STEP: Train Epoch: 1/1 Iteration: 36/1875 train_loss0.26091549090213245\n",
      "STEP: Train Epoch: 1/1 Iteration: 37/1875 train_loss0.26250200376317306\n",
      "STEP: Train Epoch: 1/1 Iteration: 38/1875 train_loss0.2694605796744949\n",
      "STEP: Train Epoch: 1/1 Iteration: 39/1875 train_loss0.2672912107828336\n",
      "STEP: Train Epoch: 1/1 Iteration: 40/1875 train_loss0.2706108409911394\n",
      "STEP: Train Epoch: 1/1 Iteration: 41/1875 train_loss0.28081189168662557\n",
      "STEP: Train Epoch: 1/1 Iteration: 42/1875 train_loss0.2802756581278074\n",
      "STEP: Train Epoch: 1/1 Iteration: 43/1875 train_loss0.27749204843543296\n",
      "STEP: Train Epoch: 1/1 Iteration: 44/1875 train_loss0.2767467288808389\n",
      "STEP: Train Epoch: 1/1 Iteration: 45/1875 train_loss0.2730262228184276\n",
      "STEP: Train Epoch: 1/1 Iteration: 46/1875 train_loss0.27342450699728466\n",
      "STEP: Train Epoch: 1/1 Iteration: 47/1875 train_loss0.2710577168680252\n",
      "STEP: Train Epoch: 1/1 Iteration: 48/1875 train_loss0.2768814124477406\n",
      "STEP: Train Epoch: 1/1 Iteration: 49/1875 train_loss0.27756232585833995\n",
      "STEP: Train Epoch: 1/1 Iteration: 50/1875 train_loss0.279386420994997\n",
      "STEP: Train Epoch: 1/1 Iteration: 51/1875 train_loss0.28171812684512604\n",
      "STEP: Train Epoch: 1/1 Iteration: 52/1875 train_loss0.28103173366532874\n",
      "STEP: Train Epoch: 1/1 Iteration: 53/1875 train_loss0.28392855190443544\n",
      "STEP: Train Epoch: 1/1 Iteration: 54/1875 train_loss0.28755573335069196\n",
      "STEP: Train Epoch: 1/1 Iteration: 55/1875 train_loss0.2869245611808517\n",
      "STEP: Train Epoch: 1/1 Iteration: 56/1875 train_loss0.28460751407380613\n",
      "STEP: Train Epoch: 1/1 Iteration: 57/1875 train_loss0.2874223457878096\n",
      "STEP: Train Epoch: 1/1 Iteration: 58/1875 train_loss0.2845318268599181\n",
      "STEP: Train Epoch: 1/1 Iteration: 59/1875 train_loss0.28616713239985\n",
      "STEP: Train Epoch: 1/1 Iteration: 60/1875 train_loss0.28517587011059126\n",
      "STEP: Train Epoch: 1/1 Iteration: 61/1875 train_loss0.2853976172990486\n",
      "STEP: Train Epoch: 1/1 Iteration: 62/1875 train_loss0.28374479110202483\n",
      "STEP: Train Epoch: 1/1 Iteration: 63/1875 train_loss0.283330253428883\n",
      "STEP: Train Epoch: 1/1 Iteration: 64/1875 train_loss0.2828831023070961\n",
      "STEP: Train Epoch: 1/1 Iteration: 65/1875 train_loss0.2832572489976883\n",
      "STEP: Train Epoch: 1/1 Iteration: 66/1875 train_loss0.2818346545100212\n",
      "STEP: Train Epoch: 1/1 Iteration: 67/1875 train_loss0.28101466151315774\n",
      "STEP: Train Epoch: 1/1 Iteration: 68/1875 train_loss0.28449756630203304\n",
      "STEP: Train Epoch: 1/1 Iteration: 69/1875 train_loss0.282522719623386\n",
      "STEP: Train Epoch: 1/1 Iteration: 70/1875 train_loss0.2844299833689417\n",
      "STEP: Train Epoch: 1/1 Iteration: 71/1875 train_loss0.2854347596286048\n",
      "STEP: Train Epoch: 1/1 Iteration: 72/1875 train_loss0.2854264773842361\n",
      "STEP: Train Epoch: 1/1 Iteration: 73/1875 train_loss0.2842608145246767\n",
      "STEP: Train Epoch: 1/1 Iteration: 74/1875 train_loss0.28323205479898966\n",
      "STEP: Train Epoch: 1/1 Iteration: 75/1875 train_loss0.2815286934375763\n",
      "STEP: Train Epoch: 1/1 Iteration: 76/1875 train_loss0.2842708610390362\n",
      "STEP: Train Epoch: 1/1 Iteration: 77/1875 train_loss0.2849229071047399\n",
      "STEP: Train Epoch: 1/1 Iteration: 78/1875 train_loss0.2860950735899118\n",
      "STEP: Train Epoch: 1/1 Iteration: 79/1875 train_loss0.2844892379981053\n",
      "STEP: Train Epoch: 1/1 Iteration: 80/1875 train_loss0.2845051871612668\n",
      "STEP: Train Epoch: 1/1 Iteration: 81/1875 train_loss0.2822159202194508\n",
      "STEP: Train Epoch: 1/1 Iteration: 82/1875 train_loss0.2858170602561497\n",
      "STEP: Train Epoch: 1/1 Iteration: 83/1875 train_loss0.2859027882117823\n",
      "STEP: Train Epoch: 1/1 Iteration: 84/1875 train_loss0.28803529031574726\n",
      "STEP: Train Epoch: 1/1 Iteration: 85/1875 train_loss0.28819593480404687\n",
      "STEP: Train Epoch: 1/1 Iteration: 86/1875 train_loss0.2874171608235947\n",
      "STEP: Train Epoch: 1/1 Iteration: 87/1875 train_loss0.2865967631511305\n",
      "STEP: Train Epoch: 1/1 Iteration: 88/1875 train_loss0.2872591513971036\n",
      "STEP: Train Epoch: 1/1 Iteration: 89/1875 train_loss0.2879784851429168\n",
      "STEP: Train Epoch: 1/1 Iteration: 90/1875 train_loss0.2874822905494107\n",
      "STEP: Train Epoch: 1/1 Iteration: 91/1875 train_loss0.2848003818602352\n",
      "STEP: Train Epoch: 1/1 Iteration: 92/1875 train_loss0.2856985681244861\n",
      "STEP: Train Epoch: 1/1 Iteration: 93/1875 train_loss0.28565246643879083\n",
      "STEP: Train Epoch: 1/1 Iteration: 94/1875 train_loss0.28451868550891574\n",
      "STEP: Train Epoch: 1/1 Iteration: 95/1875 train_loss0.2869935924285337\n",
      "STEP: Train Epoch: 1/1 Iteration: 96/1875 train_loss0.28858043788932264\n",
      "STEP: Train Epoch: 1/1 Iteration: 97/1875 train_loss0.2913693580584428\n",
      "STEP: Train Epoch: 1/1 Iteration: 98/1875 train_loss0.2910387997268414\n",
      "STEP: Train Epoch: 1/1 Iteration: 99/1875 train_loss0.29153974828394974\n",
      "STEP: Train Epoch: 1/1 Iteration: 100/1875 train_loss0.29271615989506244\n",
      "STEP: Train Epoch: 1/1 Iteration: 101/1875 train_loss0.29284709345291154\n",
      "STEP: Train Epoch: 1/1 Iteration: 102/1875 train_loss0.292460422962904\n",
      "STEP: Train Epoch: 1/1 Iteration: 103/1875 train_loss0.2916876118883346\n",
      "STEP: Train Epoch: 1/1 Iteration: 104/1875 train_loss0.29139451345858663\n",
      "STEP: Train Epoch: 1/1 Iteration: 105/1875 train_loss0.2911158236009734\n",
      "STEP: Train Epoch: 1/1 Iteration: 106/1875 train_loss0.2927370881837494\n",
      "STEP: Train Epoch: 1/1 Iteration: 107/1875 train_loss0.2919958110585391\n",
      "STEP: Train Epoch: 1/1 Iteration: 108/1875 train_loss0.29347972344193196\n",
      "STEP: Train Epoch: 1/1 Iteration: 109/1875 train_loss0.29353451475911185\n",
      "STEP: Train Epoch: 1/1 Iteration: 110/1875 train_loss0.29325018816373566\n",
      "STEP: Train Epoch: 1/1 Iteration: 111/1875 train_loss0.2935686841070115\n",
      "STEP: Train Epoch: 1/1 Iteration: 112/1875 train_loss0.2915527602391584\n",
      "STEP: Train Epoch: 1/1 Iteration: 113/1875 train_loss0.29300224860157587\n",
      "STEP: Train Epoch: 1/1 Iteration: 114/1875 train_loss0.2953436304079859\n",
      "STEP: Train Epoch: 1/1 Iteration: 115/1875 train_loss0.2950021738591401\n",
      "STEP: Train Epoch: 1/1 Iteration: 116/1875 train_loss0.29537465063662366\n",
      "STEP: Train Epoch: 1/1 Iteration: 117/1875 train_loss0.2969163426986107\n",
      "STEP: Train Epoch: 1/1 Iteration: 118/1875 train_loss0.29703484551381254\n",
      "STEP: Train Epoch: 1/1 Iteration: 119/1875 train_loss0.29760343186995564\n",
      "STEP: Train Epoch: 1/1 Iteration: 120/1875 train_loss0.2964393079280853\n",
      "STEP: Train Epoch: 1/1 Iteration: 121/1875 train_loss0.2958670636592818\n",
      "STEP: Train Epoch: 1/1 Iteration: 122/1875 train_loss0.29545964874693603\n",
      "STEP: Train Epoch: 1/1 Iteration: 123/1875 train_loss0.2945494524589399\n",
      "STEP: Train Epoch: 1/1 Iteration: 124/1875 train_loss0.29464624545747237\n",
      "STEP: Train Epoch: 1/1 Iteration: 125/1875 train_loss0.2941488779783249\n",
      "STEP: Train Epoch: 1/1 Iteration: 126/1875 train_loss0.29366623976873973\n",
      "STEP: Train Epoch: 1/1 Iteration: 127/1875 train_loss0.2938628077037691\n",
      "STEP: Train Epoch: 1/1 Iteration: 128/1875 train_loss0.29273261374328285\n",
      "STEP: Train Epoch: 1/1 Iteration: 129/1875 train_loss0.29318044502143714\n",
      "STEP: Train Epoch: 1/1 Iteration: 130/1875 train_loss0.2934255326023469\n",
      "STEP: Train Epoch: 1/1 Iteration: 131/1875 train_loss0.2942232502098302\n",
      "STEP: Train Epoch: 1/1 Iteration: 132/1875 train_loss0.2952968869922739\n",
      "STEP: Train Epoch: 1/1 Iteration: 133/1875 train_loss0.29486673812669023\n",
      "STEP: Train Epoch: 1/1 Iteration: 134/1875 train_loss0.2943594652547765\n",
      "STEP: Train Epoch: 1/1 Iteration: 135/1875 train_loss0.29304698407649993\n",
      "STEP: Train Epoch: 1/1 Iteration: 136/1875 train_loss0.2925196474089342\n",
      "STEP: Train Epoch: 1/1 Iteration: 137/1875 train_loss0.29223227696697207\n",
      "STEP: Train Epoch: 1/1 Iteration: 138/1875 train_loss0.2922655207955319\n",
      "STEP: Train Epoch: 1/1 Iteration: 139/1875 train_loss0.29326947515817\n",
      "STEP: Train Epoch: 1/1 Iteration: 140/1875 train_loss0.2930292384965079\n",
      "STEP: Train Epoch: 1/1 Iteration: 141/1875 train_loss0.2927564691144524\n",
      "STEP: Train Epoch: 1/1 Iteration: 142/1875 train_loss0.2922554234383811\n",
      "STEP: Train Epoch: 1/1 Iteration: 143/1875 train_loss0.2915734855028299\n",
      "STEP: Train Epoch: 1/1 Iteration: 144/1875 train_loss0.2910840618941519\n",
      "STEP: Train Epoch: 1/1 Iteration: 145/1875 train_loss0.29111520976855837\n",
      "STEP: Train Epoch: 1/1 Iteration: 146/1875 train_loss0.2898695490846079\n",
      "STEP: Train Epoch: 1/1 Iteration: 147/1875 train_loss0.2896251840149464\n",
      "STEP: Train Epoch: 1/1 Iteration: 148/1875 train_loss0.29003890739703503\n",
      "STEP: Train Epoch: 1/1 Iteration: 149/1875 train_loss0.2914961123926528\n",
      "STEP: Train Epoch: 1/1 Iteration: 150/1875 train_loss0.29157724236448607\n",
      "STEP: Train Epoch: 1/1 Iteration: 151/1875 train_loss0.29225162041700437\n",
      "STEP: Train Epoch: 1/1 Iteration: 152/1875 train_loss0.29120770209517916\n",
      "STEP: Train Epoch: 1/1 Iteration: 153/1875 train_loss0.29263923911292566\n",
      "STEP: Train Epoch: 1/1 Iteration: 154/1875 train_loss0.2920070512631497\n",
      "STEP: Train Epoch: 1/1 Iteration: 155/1875 train_loss0.2923233110577829\n",
      "STEP: Train Epoch: 1/1 Iteration: 156/1875 train_loss0.2918257558097442\n",
      "STEP: Train Epoch: 1/1 Iteration: 157/1875 train_loss0.2916697432186193\n",
      "STEP: Train Epoch: 1/1 Iteration: 158/1875 train_loss0.29128428025147585\n",
      "STEP: Train Epoch: 1/1 Iteration: 159/1875 train_loss0.29171966358363255\n",
      "STEP: Train Epoch: 1/1 Iteration: 160/1875 train_loss0.2910046092700213\n",
      "STEP: Train Epoch: 1/1 Iteration: 161/1875 train_loss0.29166864973973045\n",
      "STEP: Train Epoch: 1/1 Iteration: 162/1875 train_loss0.292366313667577\n",
      "STEP: Train Epoch: 1/1 Iteration: 163/1875 train_loss0.29277442930483377\n",
      "STEP: Train Epoch: 1/1 Iteration: 164/1875 train_loss0.2928742776829295\n",
      "STEP: Train Epoch: 1/1 Iteration: 165/1875 train_loss0.2934193419236125\n",
      "STEP: Train Epoch: 1/1 Iteration: 166/1875 train_loss0.2939372177134795\n",
      "STEP: Train Epoch: 1/1 Iteration: 167/1875 train_loss0.2931065855179718\n",
      "STEP: Train Epoch: 1/1 Iteration: 168/1875 train_loss0.29296485809165806\n",
      "STEP: Train Epoch: 1/1 Iteration: 169/1875 train_loss0.2940643194776315\n",
      "STEP: Train Epoch: 1/1 Iteration: 170/1875 train_loss0.2947652926778092\n",
      "STEP: Train Epoch: 1/1 Iteration: 171/1875 train_loss0.29476843430110583\n",
      "STEP: Train Epoch: 1/1 Iteration: 172/1875 train_loss0.2943825841729724\n",
      "STEP: Train Epoch: 1/1 Iteration: 173/1875 train_loss0.2939521497643063\n",
      "STEP: Train Epoch: 1/1 Iteration: 174/1875 train_loss0.2938253894533919\n",
      "STEP: Train Epoch: 1/1 Iteration: 175/1875 train_loss0.29300208692039764\n",
      "STEP: Train Epoch: 1/1 Iteration: 176/1875 train_loss0.2935424520295452\n",
      "STEP: Train Epoch: 1/1 Iteration: 177/1875 train_loss0.29358300604557586\n",
      "STEP: Train Epoch: 1/1 Iteration: 178/1875 train_loss0.2941275077003441\n",
      "STEP: Train Epoch: 1/1 Iteration: 179/1875 train_loss0.29349331944847906\n",
      "STEP: Train Epoch: 1/1 Iteration: 180/1875 train_loss0.29268873983787164\n",
      "STEP: Train Epoch: 1/1 Iteration: 181/1875 train_loss0.29229909095151646\n",
      "STEP: Train Epoch: 1/1 Iteration: 182/1875 train_loss0.2915736974350044\n",
      "STEP: Train Epoch: 1/1 Iteration: 183/1875 train_loss0.29230589688312814\n",
      "STEP: Train Epoch: 1/1 Iteration: 184/1875 train_loss0.2924575806068985\n",
      "STEP: Train Epoch: 1/1 Iteration: 185/1875 train_loss0.2918463987675873\n",
      "STEP: Train Epoch: 1/1 Iteration: 186/1875 train_loss0.29113920814087313\n",
      "STEP: Train Epoch: 1/1 Iteration: 187/1875 train_loss0.29075426018652434\n",
      "STEP: Train Epoch: 1/1 Iteration: 188/1875 train_loss0.29118466262329135\n",
      "STEP: Train Epoch: 1/1 Iteration: 189/1875 train_loss0.2913455177473013\n",
      "STEP: Train Epoch: 1/1 Iteration: 190/1875 train_loss0.29090390836721974\n",
      "STEP: Train Epoch: 1/1 Iteration: 191/1875 train_loss0.2901833176378804\n",
      "STEP: Train Epoch: 1/1 Iteration: 192/1875 train_loss0.29016136956245947\n",
      "STEP: Train Epoch: 1/1 Iteration: 193/1875 train_loss0.2899908167478952\n",
      "STEP: Train Epoch: 1/1 Iteration: 194/1875 train_loss0.28994179574638296\n",
      "STEP: Train Epoch: 1/1 Iteration: 195/1875 train_loss0.2904054324214275\n",
      "STEP: Train Epoch: 1/1 Iteration: 196/1875 train_loss0.2917608223779469\n",
      "STEP: Train Epoch: 1/1 Iteration: 197/1875 train_loss0.2913516450124949\n",
      "STEP: Train Epoch: 1/1 Iteration: 198/1875 train_loss0.29152109831421064\n",
      "STEP: Train Epoch: 1/1 Iteration: 199/1875 train_loss0.29087051034123457\n",
      "STEP: Train Epoch: 1/1 Iteration: 200/1875 train_loss0.2917037433758378\n",
      "STEP: Train Epoch: 1/1 Iteration: 201/1875 train_loss0.2914354296688417\n",
      "STEP: Train Epoch: 1/1 Iteration: 202/1875 train_loss0.29188593908554256\n",
      "STEP: Train Epoch: 1/1 Iteration: 203/1875 train_loss0.2913015724476335\n",
      "STEP: Train Epoch: 1/1 Iteration: 204/1875 train_loss0.29120555100049456\n",
      "STEP: Train Epoch: 1/1 Iteration: 205/1875 train_loss0.2912530921217872\n",
      "STEP: Train Epoch: 1/1 Iteration: 206/1875 train_loss0.2903661080138776\n",
      "STEP: Train Epoch: 1/1 Iteration: 207/1875 train_loss0.29045303464655714\n",
      "STEP: Train Epoch: 1/1 Iteration: 208/1875 train_loss0.2899773755254081\n",
      "STEP: Train Epoch: 1/1 Iteration: 209/1875 train_loss0.2904571757861302\n",
      "STEP: Train Epoch: 1/1 Iteration: 210/1875 train_loss0.2908665545994327\n",
      "STEP: Train Epoch: 1/1 Iteration: 211/1875 train_loss0.2908870752600697\n",
      "STEP: Train Epoch: 1/1 Iteration: 212/1875 train_loss0.29136986155414357\n",
      "STEP: Train Epoch: 1/1 Iteration: 213/1875 train_loss0.29200784655663886\n",
      "STEP: Train Epoch: 1/1 Iteration: 214/1875 train_loss0.2920479350766846\n",
      "STEP: Train Epoch: 1/1 Iteration: 215/1875 train_loss0.291336665562419\n",
      "STEP: Train Epoch: 1/1 Iteration: 216/1875 train_loss0.29213228956278825\n",
      "STEP: Train Epoch: 1/1 Iteration: 217/1875 train_loss0.29262209923998\n",
      "STEP: Train Epoch: 1/1 Iteration: 218/1875 train_loss0.29283404387875434\n",
      "STEP: Train Epoch: 1/1 Iteration: 219/1875 train_loss0.29273640670596734\n",
      "STEP: Train Epoch: 1/1 Iteration: 220/1875 train_loss0.29371114424006506\n",
      "STEP: Train Epoch: 1/1 Iteration: 221/1875 train_loss0.29432804616193425\n",
      "STEP: Train Epoch: 1/1 Iteration: 222/1875 train_loss0.29376669144174\n",
      "STEP: Train Epoch: 1/1 Iteration: 223/1875 train_loss0.29417643161498913\n",
      "STEP: Train Epoch: 1/1 Iteration: 224/1875 train_loss0.29501624322230263\n",
      "STEP: Train Epoch: 1/1 Iteration: 225/1875 train_loss0.2945178375972642\n",
      "STEP: Train Epoch: 1/1 Iteration: 226/1875 train_loss0.29429824831607065\n",
      "STEP: Train Epoch: 1/1 Iteration: 227/1875 train_loss0.29486817262640086\n",
      "STEP: Train Epoch: 1/1 Iteration: 228/1875 train_loss0.2955371678333011\n",
      "STEP: Train Epoch: 1/1 Iteration: 229/1875 train_loss0.29519479019022404\n",
      "STEP: Train Epoch: 1/1 Iteration: 230/1875 train_loss0.2952207143863906\n",
      "STEP: Train Epoch: 1/1 Iteration: 231/1875 train_loss0.2944134077081433\n",
      "STEP: Train Epoch: 1/1 Iteration: 232/1875 train_loss0.2939915351312736\n",
      "STEP: Train Epoch: 1/1 Iteration: 233/1875 train_loss0.2935978764500229\n",
      "STEP: Train Epoch: 1/1 Iteration: 234/1875 train_loss0.2935108033637715\n",
      "STEP: Train Epoch: 1/1 Iteration: 235/1875 train_loss0.29351625613709714\n",
      "STEP: Train Epoch: 1/1 Iteration: 236/1875 train_loss0.2930546998977661\n",
      "STEP: Train Epoch: 1/1 Iteration: 237/1875 train_loss0.29271719142605984\n",
      "STEP: Train Epoch: 1/1 Iteration: 238/1875 train_loss0.2938170890722956\n",
      "STEP: Train Epoch: 1/1 Iteration: 239/1875 train_loss0.29406715655177207\n",
      "STEP: Train Epoch: 1/1 Iteration: 240/1875 train_loss0.2941889670367042\n",
      "STEP: Train Epoch: 1/1 Iteration: 241/1875 train_loss0.29489888188997243\n",
      "STEP: Train Epoch: 1/1 Iteration: 242/1875 train_loss0.2958951956353897\n",
      "STEP: Train Epoch: 1/1 Iteration: 243/1875 train_loss0.2956098139040755\n",
      "STEP: Train Epoch: 1/1 Iteration: 244/1875 train_loss0.29632710152473607\n",
      "STEP: Train Epoch: 1/1 Iteration: 245/1875 train_loss0.29664500501691077\n",
      "STEP: Train Epoch: 1/1 Iteration: 246/1875 train_loss0.2968596186095137\n",
      "STEP: Train Epoch: 1/1 Iteration: 247/1875 train_loss0.29724861228996924\n",
      "STEP: Train Epoch: 1/1 Iteration: 248/1875 train_loss0.29693031154813304\n",
      "STEP: Train Epoch: 1/1 Iteration: 249/1875 train_loss0.29671484926139496\n",
      "STEP: Train Epoch: 1/1 Iteration: 250/1875 train_loss0.29664660048484803\n",
      "STEP: Train Epoch: 1/1 Iteration: 251/1875 train_loss0.29697889505154584\n",
      "STEP: Train Epoch: 1/1 Iteration: 252/1875 train_loss0.2965122906579858\n",
      "STEP: Train Epoch: 1/1 Iteration: 253/1875 train_loss0.2968493366076541\n",
      "STEP: Train Epoch: 1/1 Iteration: 254/1875 train_loss0.29641373895519363\n",
      "STEP: Train Epoch: 1/1 Iteration: 255/1875 train_loss0.2961766348165624\n",
      "STEP: Train Epoch: 1/1 Iteration: 256/1875 train_loss0.29656637692824006\n",
      "STEP: Train Epoch: 1/1 Iteration: 257/1875 train_loss0.2964082522614921\n",
      "STEP: Train Epoch: 1/1 Iteration: 258/1875 train_loss0.2959012753048608\n",
      "STEP: Train Epoch: 1/1 Iteration: 259/1875 train_loss0.29566392780040684\n",
      "STEP: Train Epoch: 1/1 Iteration: 260/1875 train_loss0.29679149922270043\n",
      "STEP: Train Epoch: 1/1 Iteration: 261/1875 train_loss0.2965677934930699\n",
      "STEP: Train Epoch: 1/1 Iteration: 262/1875 train_loss0.29617744923092937\n",
      "STEP: Train Epoch: 1/1 Iteration: 263/1875 train_loss0.2956866753418636\n",
      "STEP: Train Epoch: 1/1 Iteration: 264/1875 train_loss0.2957013568869143\n",
      "STEP: Train Epoch: 1/1 Iteration: 265/1875 train_loss0.29638500438546234\n",
      "STEP: Train Epoch: 1/1 Iteration: 266/1875 train_loss0.2961467250173253\n",
      "STEP: Train Epoch: 1/1 Iteration: 267/1875 train_loss0.2964814900235737\n",
      "STEP: Train Epoch: 1/1 Iteration: 268/1875 train_loss0.29659432673187397\n",
      "STEP: Train Epoch: 1/1 Iteration: 269/1875 train_loss0.29585177307346056\n",
      "STEP: Train Epoch: 1/1 Iteration: 270/1875 train_loss0.2961762396549737\n",
      "STEP: Train Epoch: 1/1 Iteration: 271/1875 train_loss0.29553331151549667\n",
      "STEP: Train Epoch: 1/1 Iteration: 272/1875 train_loss0.2955384628902025\n",
      "STEP: Train Epoch: 1/1 Iteration: 273/1875 train_loss0.29499787438810965\n",
      "STEP: Train Epoch: 1/1 Iteration: 274/1875 train_loss0.29547499965903534\n",
      "STEP: Train Epoch: 1/1 Iteration: 275/1875 train_loss0.29546028546311637\n",
      "STEP: Train Epoch: 1/1 Iteration: 276/1875 train_loss0.29546783786213054\n",
      "STEP: Train Epoch: 1/1 Iteration: 277/1875 train_loss0.2947808905951813\n",
      "STEP: Train Epoch: 1/1 Iteration: 278/1875 train_loss0.29538779566399487\n",
      "STEP: Train Epoch: 1/1 Iteration: 279/1875 train_loss0.29539892253696276\n",
      "STEP: Train Epoch: 1/1 Iteration: 280/1875 train_loss0.2955020759254694\n",
      "STEP: Train Epoch: 1/1 Iteration: 281/1875 train_loss0.2954689375128186\n",
      "STEP: Train Epoch: 1/1 Iteration: 282/1875 train_loss0.29560768831494855\n",
      "STEP: Train Epoch: 1/1 Iteration: 283/1875 train_loss0.29556547400176314\n",
      "STEP: Train Epoch: 1/1 Iteration: 284/1875 train_loss0.2952869645726513\n",
      "STEP: Train Epoch: 1/1 Iteration: 285/1875 train_loss0.2949645928646389\n",
      "STEP: Train Epoch: 1/1 Iteration: 286/1875 train_loss0.2945896216726803\n",
      "STEP: Train Epoch: 1/1 Iteration: 287/1875 train_loss0.2953910979763556\n",
      "STEP: Train Epoch: 1/1 Iteration: 288/1875 train_loss0.29463889447247815\n",
      "STEP: Train Epoch: 1/1 Iteration: 289/1875 train_loss0.2948030694081709\n",
      "STEP: Train Epoch: 1/1 Iteration: 290/1875 train_loss0.29429217846742994\n",
      "STEP: Train Epoch: 1/1 Iteration: 291/1875 train_loss0.2949101376216027\n",
      "STEP: Train Epoch: 1/1 Iteration: 292/1875 train_loss0.29489342850467115\n",
      "STEP: Train Epoch: 1/1 Iteration: 293/1875 train_loss0.2949269309436502\n",
      "STEP: Train Epoch: 1/1 Iteration: 294/1875 train_loss0.294829421024136\n",
      "STEP: Train Epoch: 1/1 Iteration: 295/1875 train_loss0.2950025966864521\n",
      "STEP: Train Epoch: 1/1 Iteration: 296/1875 train_loss0.29462490439717026\n",
      "STEP: Train Epoch: 1/1 Iteration: 297/1875 train_loss0.29525216277500593\n",
      "STEP: Train Epoch: 1/1 Iteration: 298/1875 train_loss0.29544147356544564\n",
      "STEP: Train Epoch: 1/1 Iteration: 299/1875 train_loss0.2956172192724652\n",
      "STEP: Train Epoch: 1/1 Iteration: 300/1875 train_loss0.29518603913486\n",
      "STEP: Train Epoch: 1/1 Iteration: 301/1875 train_loss0.295076039072089\n",
      "STEP: Train Epoch: 1/1 Iteration: 302/1875 train_loss0.2945852514282362\n",
      "STEP: Train Epoch: 1/1 Iteration: 303/1875 train_loss0.29422628021377933\n",
      "STEP: Train Epoch: 1/1 Iteration: 304/1875 train_loss0.29416941858730034\n",
      "STEP: Train Epoch: 1/1 Iteration: 305/1875 train_loss0.2944938057514488\n",
      "STEP: Train Epoch: 1/1 Iteration: 306/1875 train_loss0.2944148774776194\n",
      "STEP: Train Epoch: 1/1 Iteration: 307/1875 train_loss0.29438083128645676\n",
      "STEP: Train Epoch: 1/1 Iteration: 308/1875 train_loss0.2956351143150748\n",
      "STEP: Train Epoch: 1/1 Iteration: 309/1875 train_loss0.295378149544344\n",
      "STEP: Train Epoch: 1/1 Iteration: 310/1875 train_loss0.2954592255094359\n",
      "STEP: Train Epoch: 1/1 Iteration: 311/1875 train_loss0.2956341378798055\n",
      "STEP: Train Epoch: 1/1 Iteration: 312/1875 train_loss0.29581468344594425\n",
      "STEP: Train Epoch: 1/1 Iteration: 313/1875 train_loss0.29557015161259104\n",
      "STEP: Train Epoch: 1/1 Iteration: 314/1875 train_loss0.2953354606915052\n",
      "STEP: Train Epoch: 1/1 Iteration: 315/1875 train_loss0.2951050820568251\n",
      "STEP: Train Epoch: 1/1 Iteration: 316/1875 train_loss0.29530752938287924\n",
      "STEP: Train Epoch: 1/1 Iteration: 317/1875 train_loss0.2955999143821208\n",
      "STEP: Train Epoch: 1/1 Iteration: 318/1875 train_loss0.295437762736337\n",
      "STEP: Train Epoch: 1/1 Iteration: 319/1875 train_loss0.2947354119586347\n",
      "STEP: Train Epoch: 1/1 Iteration: 320/1875 train_loss0.2945577293634415\n",
      "STEP: Train Epoch: 1/1 Iteration: 321/1875 train_loss0.29502109380154595\n",
      "STEP: Train Epoch: 1/1 Iteration: 322/1875 train_loss0.2950575524790687\n",
      "STEP: Train Epoch: 1/1 Iteration: 323/1875 train_loss0.2959486505379987\n",
      "STEP: Train Epoch: 1/1 Iteration: 324/1875 train_loss0.2960483977272187\n",
      "STEP: Train Epoch: 1/1 Iteration: 325/1875 train_loss0.2964395006803366\n",
      "STEP: Train Epoch: 1/1 Iteration: 326/1875 train_loss0.2965713233852679\n",
      "STEP: Train Epoch: 1/1 Iteration: 327/1875 train_loss0.29716294101618845\n",
      "STEP: Train Epoch: 1/1 Iteration: 328/1875 train_loss0.2967718350360306\n",
      "STEP: Train Epoch: 1/1 Iteration: 329/1875 train_loss0.29684783916886454\n",
      "STEP: Train Epoch: 1/1 Iteration: 330/1875 train_loss0.2965531851757656\n",
      "STEP: Train Epoch: 1/1 Iteration: 331/1875 train_loss0.29623510848359397\n",
      "STEP: Train Epoch: 1/1 Iteration: 332/1875 train_loss0.295794553830322\n",
      "STEP: Train Epoch: 1/1 Iteration: 333/1875 train_loss0.2965783044114127\n",
      "STEP: Train Epoch: 1/1 Iteration: 334/1875 train_loss0.2981799264957091\n",
      "STEP: Train Epoch: 1/1 Iteration: 335/1875 train_loss0.29787420043304785\n",
      "STEP: Train Epoch: 1/1 Iteration: 336/1875 train_loss0.2973728949824969\n",
      "STEP: Train Epoch: 1/1 Iteration: 337/1875 train_loss0.2971603810698767\n",
      "STEP: Train Epoch: 1/1 Iteration: 338/1875 train_loss0.29679770886721696\n",
      "STEP: Train Epoch: 1/1 Iteration: 339/1875 train_loss0.29703814613256485\n",
      "STEP: Train Epoch: 1/1 Iteration: 340/1875 train_loss0.2968623812146047\n",
      "STEP: Train Epoch: 1/1 Iteration: 341/1875 train_loss0.2977746136464681\n",
      "STEP: Train Epoch: 1/1 Iteration: 342/1875 train_loss0.2975834589778331\n",
      "STEP: Train Epoch: 1/1 Iteration: 343/1875 train_loss0.29740083113356164\n",
      "STEP: Train Epoch: 1/1 Iteration: 344/1875 train_loss0.2970252862243458\n",
      "STEP: Train Epoch: 1/1 Iteration: 345/1875 train_loss0.29670887403730034\n",
      "STEP: Train Epoch: 1/1 Iteration: 346/1875 train_loss0.2965656990519149\n",
      "STEP: Train Epoch: 1/1 Iteration: 347/1875 train_loss0.2969098959858892\n",
      "STEP: Train Epoch: 1/1 Iteration: 348/1875 train_loss0.2973786711093338\n",
      "STEP: Train Epoch: 1/1 Iteration: 349/1875 train_loss0.29755085437721374\n",
      "STEP: Train Epoch: 1/1 Iteration: 350/1875 train_loss0.29764991228069576\n",
      "STEP: Train Epoch: 1/1 Iteration: 351/1875 train_loss0.29762387814854624\n",
      "STEP: Train Epoch: 1/1 Iteration: 352/1875 train_loss0.29720205800946464\n",
      "STEP: Train Epoch: 1/1 Iteration: 353/1875 train_loss0.2973395327248587\n",
      "STEP: Train Epoch: 1/1 Iteration: 354/1875 train_loss0.2978307686030528\n",
      "STEP: Train Epoch: 1/1 Iteration: 355/1875 train_loss0.2974794704309652\n",
      "STEP: Train Epoch: 1/1 Iteration: 356/1875 train_loss0.2969609953756078\n",
      "STEP: Train Epoch: 1/1 Iteration: 357/1875 train_loss0.2965978945211536\n",
      "STEP: Train Epoch: 1/1 Iteration: 358/1875 train_loss0.296551930775356\n",
      "STEP: Train Epoch: 1/1 Iteration: 359/1875 train_loss0.29710116702284983\n",
      "STEP: Train Epoch: 1/1 Iteration: 360/1875 train_loss0.2968357679951522\n",
      "STEP: Train Epoch: 1/1 Iteration: 361/1875 train_loss0.297614036937995\n",
      "STEP: Train Epoch: 1/1 Iteration: 362/1875 train_loss0.2976855732171246\n",
      "STEP: Train Epoch: 1/1 Iteration: 363/1875 train_loss0.2977716193095712\n",
      "STEP: Train Epoch: 1/1 Iteration: 364/1875 train_loss0.2976083964477856\n",
      "STEP: Train Epoch: 1/1 Iteration: 365/1875 train_loss0.2974119509940278\n",
      "STEP: Train Epoch: 1/1 Iteration: 366/1875 train_loss0.29685573515996255\n",
      "STEP: Train Epoch: 1/1 Iteration: 367/1875 train_loss0.29673384759341664\n",
      "STEP: Train Epoch: 1/1 Iteration: 368/1875 train_loss0.29656227030183957\n",
      "STEP: Train Epoch: 1/1 Iteration: 369/1875 train_loss0.2966857480647441\n",
      "STEP: Train Epoch: 1/1 Iteration: 370/1875 train_loss0.29717419010561863\n",
      "STEP: Train Epoch: 1/1 Iteration: 371/1875 train_loss0.2967103632553568\n",
      "STEP: Train Epoch: 1/1 Iteration: 372/1875 train_loss0.2965976412818637\n",
      "STEP: Train Epoch: 1/1 Iteration: 373/1875 train_loss0.2965290466118754\n",
      "STEP: Train Epoch: 1/1 Iteration: 374/1875 train_loss0.29667148890501677\n",
      "STEP: Train Epoch: 1/1 Iteration: 375/1875 train_loss0.29721033640702565\n",
      "STEP: Train Epoch: 1/1 Iteration: 376/1875 train_loss0.29717609290271363\n",
      "STEP: Train Epoch: 1/1 Iteration: 377/1875 train_loss0.29722954807136037\n",
      "STEP: Train Epoch: 1/1 Iteration: 378/1875 train_loss0.2969066258618441\n",
      "STEP: Train Epoch: 1/1 Iteration: 379/1875 train_loss0.2966132252071652\n",
      "STEP: Train Epoch: 1/1 Iteration: 380/1875 train_loss0.2971657073811481\n",
      "STEP: Train Epoch: 1/1 Iteration: 381/1875 train_loss0.29699558543720894\n",
      "STEP: Train Epoch: 1/1 Iteration: 382/1875 train_loss0.2967790880128351\n",
      "STEP: Train Epoch: 1/1 Iteration: 383/1875 train_loss0.2964973385511428\n",
      "STEP: Train Epoch: 1/1 Iteration: 384/1875 train_loss0.2964907999072845\n",
      "STEP: Train Epoch: 1/1 Iteration: 385/1875 train_loss0.29613555255648377\n",
      "STEP: Train Epoch: 1/1 Iteration: 386/1875 train_loss0.29692257628076435\n",
      "STEP: Train Epoch: 1/1 Iteration: 387/1875 train_loss0.29711776821650276\n",
      "STEP: Train Epoch: 1/1 Iteration: 388/1875 train_loss0.29722767772594677\n",
      "STEP: Train Epoch: 1/1 Iteration: 389/1875 train_loss0.2973801458785957\n",
      "STEP: Train Epoch: 1/1 Iteration: 390/1875 train_loss0.2975171454823934\n",
      "STEP: Train Epoch: 1/1 Iteration: 391/1875 train_loss0.2976168087300132\n",
      "STEP: Train Epoch: 1/1 Iteration: 392/1875 train_loss0.2973375956014711\n",
      "STEP: Train Epoch: 1/1 Iteration: 393/1875 train_loss0.2974904930318585\n",
      "STEP: Train Epoch: 1/1 Iteration: 394/1875 train_loss0.2971636874951082\n",
      "STEP: Train Epoch: 1/1 Iteration: 395/1875 train_loss0.2975341405672363\n",
      "STEP: Train Epoch: 1/1 Iteration: 396/1875 train_loss0.29752154667118585\n",
      "STEP: Train Epoch: 1/1 Iteration: 397/1875 train_loss0.29771930298366833\n",
      "STEP: Train Epoch: 1/1 Iteration: 398/1875 train_loss0.2977637174216347\n",
      "STEP: Train Epoch: 1/1 Iteration: 399/1875 train_loss0.2981165703152654\n",
      "STEP: Train Epoch: 1/1 Iteration: 400/1875 train_loss0.29817475061863663\n",
      "STEP: Train Epoch: 1/1 Iteration: 401/1875 train_loss0.29833558926409914\n",
      "STEP: Train Epoch: 1/1 Iteration: 402/1875 train_loss0.29882331463086664\n",
      "STEP: Train Epoch: 1/1 Iteration: 403/1875 train_loss0.2991289545599343\n",
      "STEP: Train Epoch: 1/1 Iteration: 404/1875 train_loss0.2992064790191627\n",
      "STEP: Train Epoch: 1/1 Iteration: 405/1875 train_loss0.2992779016862681\n",
      "STEP: Train Epoch: 1/1 Iteration: 406/1875 train_loss0.2989414253551972\n",
      "STEP: Train Epoch: 1/1 Iteration: 407/1875 train_loss0.2993894245027031\n",
      "STEP: Train Epoch: 1/1 Iteration: 408/1875 train_loss0.29920691036272284\n",
      "STEP: Train Epoch: 1/1 Iteration: 409/1875 train_loss0.2990716725660711\n",
      "STEP: Train Epoch: 1/1 Iteration: 410/1875 train_loss0.2992944979813041\n",
      "STEP: Train Epoch: 1/1 Iteration: 411/1875 train_loss0.2992349255404043\n",
      "STEP: Train Epoch: 1/1 Iteration: 412/1875 train_loss0.2996064892526969\n",
      "STEP: Train Epoch: 1/1 Iteration: 413/1875 train_loss0.29962533596930147\n",
      "STEP: Train Epoch: 1/1 Iteration: 414/1875 train_loss0.3000146786227894\n",
      "STEP: Train Epoch: 1/1 Iteration: 415/1875 train_loss0.29989939403821186\n",
      "STEP: Train Epoch: 1/1 Iteration: 416/1875 train_loss0.29976837200900686\n",
      "STEP: Train Epoch: 1/1 Iteration: 417/1875 train_loss0.30017098134084286\n",
      "STEP: Train Epoch: 1/1 Iteration: 418/1875 train_loss0.3001104282420218\n",
      "STEP: Train Epoch: 1/1 Iteration: 419/1875 train_loss0.30055599899735824\n",
      "STEP: Train Epoch: 1/1 Iteration: 420/1875 train_loss0.3003688371252446\n",
      "STEP: Train Epoch: 1/1 Iteration: 421/1875 train_loss0.30058227203379334\n",
      "STEP: Train Epoch: 1/1 Iteration: 422/1875 train_loss0.30066347288153183\n",
      "STEP: Train Epoch: 1/1 Iteration: 423/1875 train_loss0.3003450328788013\n",
      "STEP: Train Epoch: 1/1 Iteration: 424/1875 train_loss0.30043086736410296\n",
      "STEP: Train Epoch: 1/1 Iteration: 425/1875 train_loss0.30055163358940795\n",
      "STEP: Train Epoch: 1/1 Iteration: 426/1875 train_loss0.3003434271249973\n",
      "STEP: Train Epoch: 1/1 Iteration: 427/1875 train_loss0.30020231922839785\n",
      "STEP: Train Epoch: 1/1 Iteration: 428/1875 train_loss0.299873790172773\n",
      "STEP: Train Epoch: 1/1 Iteration: 429/1875 train_loss0.2997029209400946\n",
      "STEP: Train Epoch: 1/1 Iteration: 430/1875 train_loss0.2999877261214478\n",
      "STEP: Train Epoch: 1/1 Iteration: 431/1875 train_loss0.30010737292036377\n",
      "STEP: Train Epoch: 1/1 Iteration: 432/1875 train_loss0.2998288435585521\n",
      "STEP: Train Epoch: 1/1 Iteration: 433/1875 train_loss0.29969568619414233\n",
      "STEP: Train Epoch: 1/1 Iteration: 434/1875 train_loss0.29978019339011014\n",
      "STEP: Train Epoch: 1/1 Iteration: 435/1875 train_loss0.2997760940899794\n",
      "STEP: Train Epoch: 1/1 Iteration: 436/1875 train_loss0.2996341118913725\n",
      "STEP: Train Epoch: 1/1 Iteration: 437/1875 train_loss0.2994450613072044\n",
      "STEP: Train Epoch: 1/1 Iteration: 438/1875 train_loss0.29944220545901556\n",
      "STEP: Train Epoch: 1/1 Iteration: 439/1875 train_loss0.299268173736972\n",
      "STEP: Train Epoch: 1/1 Iteration: 440/1875 train_loss0.29877106162973427\n",
      "STEP: Train Epoch: 1/1 Iteration: 441/1875 train_loss0.2988847198974248\n",
      "STEP: Train Epoch: 1/1 Iteration: 442/1875 train_loss0.29900111145091274\n",
      "STEP: Train Epoch: 1/1 Iteration: 443/1875 train_loss0.2992941007421733\n",
      "STEP: Train Epoch: 1/1 Iteration: 444/1875 train_loss0.299281357460328\n",
      "STEP: Train Epoch: 1/1 Iteration: 445/1875 train_loss0.2989612417107218\n",
      "STEP: Train Epoch: 1/1 Iteration: 446/1875 train_loss0.29917960909650465\n",
      "STEP: Train Epoch: 1/1 Iteration: 447/1875 train_loss0.29907239668521307\n",
      "STEP: Train Epoch: 1/1 Iteration: 448/1875 train_loss0.2985445162713794\n",
      "STEP: Train Epoch: 1/1 Iteration: 449/1875 train_loss0.29853360565640613\n",
      "STEP: Train Epoch: 1/1 Iteration: 450/1875 train_loss0.29903519850638177\n",
      "STEP: Train Epoch: 1/1 Iteration: 451/1875 train_loss0.29882681292136865\n",
      "STEP: Train Epoch: 1/1 Iteration: 452/1875 train_loss0.2986892435897504\n",
      "STEP: Train Epoch: 1/1 Iteration: 453/1875 train_loss0.2988899855917653\n",
      "STEP: Train Epoch: 1/1 Iteration: 454/1875 train_loss0.29841236103867647\n",
      "STEP: Train Epoch: 1/1 Iteration: 455/1875 train_loss0.2986467662420902\n",
      "STEP: Train Epoch: 1/1 Iteration: 456/1875 train_loss0.2984382774876921\n",
      "STEP: Train Epoch: 1/1 Iteration: 457/1875 train_loss0.29834325027935354\n",
      "STEP: Train Epoch: 1/1 Iteration: 458/1875 train_loss0.2984639065234422\n",
      "STEP: Train Epoch: 1/1 Iteration: 459/1875 train_loss0.29871042504549544\n",
      "STEP: Train Epoch: 1/1 Iteration: 460/1875 train_loss0.29872864245072656\n",
      "STEP: Train Epoch: 1/1 Iteration: 461/1875 train_loss0.2986130309079061\n",
      "STEP: Train Epoch: 1/1 Iteration: 462/1875 train_loss0.29886655812655694\n",
      "STEP: Train Epoch: 1/1 Iteration: 463/1875 train_loss0.29898694801279074\n",
      "STEP: Train Epoch: 1/1 Iteration: 464/1875 train_loss0.29910169920787727\n",
      "STEP: Train Epoch: 1/1 Iteration: 465/1875 train_loss0.2992325564225515\n",
      "STEP: Train Epoch: 1/1 Iteration: 466/1875 train_loss0.29950527040999336\n",
      "STEP: Train Epoch: 1/1 Iteration: 467/1875 train_loss0.2994971385094032\n",
      "STEP: Train Epoch: 1/1 Iteration: 468/1875 train_loss0.29951698441281277\n",
      "STEP: Train Epoch: 1/1 Iteration: 469/1875 train_loss0.29937261924433556\n",
      "STEP: Train Epoch: 1/1 Iteration: 470/1875 train_loss0.29954358095184286\n",
      "STEP: Train Epoch: 1/1 Iteration: 471/1875 train_loss0.2994786631592773\n",
      "STEP: Train Epoch: 1/1 Iteration: 472/1875 train_loss0.29985620968548926\n",
      "STEP: Train Epoch: 1/1 Iteration: 473/1875 train_loss0.29984462610888934\n",
      "STEP: Train Epoch: 1/1 Iteration: 474/1875 train_loss0.30002304990206086\n",
      "STEP: Train Epoch: 1/1 Iteration: 475/1875 train_loss0.30004248822990215\n",
      "STEP: Train Epoch: 1/1 Iteration: 476/1875 train_loss0.29975909483032065\n",
      "STEP: Train Epoch: 1/1 Iteration: 477/1875 train_loss0.2997619542810652\n",
      "STEP: Train Epoch: 1/1 Iteration: 478/1875 train_loss0.3000359109380754\n",
      "STEP: Train Epoch: 1/1 Iteration: 479/1875 train_loss0.30005054290956645\n",
      "STEP: Train Epoch: 1/1 Iteration: 480/1875 train_loss0.30019747111946343\n",
      "STEP: Train Epoch: 1/1 Iteration: 481/1875 train_loss0.29980647873729777\n",
      "STEP: Train Epoch: 1/1 Iteration: 482/1875 train_loss0.2998230192052873\n",
      "STEP: Train Epoch: 1/1 Iteration: 483/1875 train_loss0.29969222646198906\n",
      "STEP: Train Epoch: 1/1 Iteration: 484/1875 train_loss0.2994397244423874\n",
      "STEP: Train Epoch: 1/1 Iteration: 485/1875 train_loss0.29912258300584615\n",
      "STEP: Train Epoch: 1/1 Iteration: 486/1875 train_loss0.29892903186166236\n",
      "STEP: Train Epoch: 1/1 Iteration: 487/1875 train_loss0.29870264942029173\n",
      "STEP: Train Epoch: 1/1 Iteration: 488/1875 train_loss0.29860575213173374\n",
      "STEP: Train Epoch: 1/1 Iteration: 489/1875 train_loss0.2985481202663332\n",
      "STEP: Train Epoch: 1/1 Iteration: 490/1875 train_loss0.29864819077204685\n",
      "STEP: Train Epoch: 1/1 Iteration: 491/1875 train_loss0.2987513531553284\n",
      "STEP: Train Epoch: 1/1 Iteration: 492/1875 train_loss0.29859032242642186\n",
      "STEP: Train Epoch: 1/1 Iteration: 493/1875 train_loss0.2981906706131012\n",
      "STEP: Train Epoch: 1/1 Iteration: 494/1875 train_loss0.29838437831353565\n",
      "STEP: Train Epoch: 1/1 Iteration: 495/1875 train_loss0.29808161177418446\n",
      "STEP: Train Epoch: 1/1 Iteration: 496/1875 train_loss0.2979748072102666\n",
      "STEP: Train Epoch: 1/1 Iteration: 497/1875 train_loss0.29790345155137404\n",
      "STEP: Train Epoch: 1/1 Iteration: 498/1875 train_loss0.297930285065289\n",
      "STEP: Train Epoch: 1/1 Iteration: 499/1875 train_loss0.2979166923102014\n",
      "STEP: Train Epoch: 1/1 Iteration: 500/1875 train_loss0.29773896262049676\n",
      "STEP: Train Epoch: 1/1 Iteration: 501/1875 train_loss0.2974055911193113\n",
      "STEP: Train Epoch: 1/1 Iteration: 502/1875 train_loss0.297114455545566\n",
      "STEP: Train Epoch: 1/1 Iteration: 503/1875 train_loss0.2969719743278344\n",
      "STEP: Train Epoch: 1/1 Iteration: 504/1875 train_loss0.29654689758483854\n",
      "STEP: Train Epoch: 1/1 Iteration: 505/1875 train_loss0.296143609034543\n",
      "STEP: Train Epoch: 1/1 Iteration: 506/1875 train_loss0.2961454120402044\n",
      "STEP: Train Epoch: 1/1 Iteration: 507/1875 train_loss0.2962726986943147\n",
      "STEP: Train Epoch: 1/1 Iteration: 508/1875 train_loss0.2964679445543392\n",
      "STEP: Train Epoch: 1/1 Iteration: 509/1875 train_loss0.2964200345700522\n",
      "STEP: Train Epoch: 1/1 Iteration: 510/1875 train_loss0.2961953601562509\n",
      "STEP: Train Epoch: 1/1 Iteration: 511/1875 train_loss0.29615538619794024\n",
      "STEP: Train Epoch: 1/1 Iteration: 512/1875 train_loss0.2962114523543278\n",
      "STEP: Train Epoch: 1/1 Iteration: 513/1875 train_loss0.2962470421346075\n",
      "STEP: Train Epoch: 1/1 Iteration: 514/1875 train_loss0.2962082997979125\n",
      "STEP: Train Epoch: 1/1 Iteration: 515/1875 train_loss0.2958997132564054\n",
      "STEP: Train Epoch: 1/1 Iteration: 516/1875 train_loss0.29577660689115987\n",
      "STEP: Train Epoch: 1/1 Iteration: 517/1875 train_loss0.2963062150248937\n",
      "STEP: Train Epoch: 1/1 Iteration: 518/1875 train_loss0.2959919288668163\n",
      "STEP: Train Epoch: 1/1 Iteration: 519/1875 train_loss0.2957636897590349\n",
      "STEP: Train Epoch: 1/1 Iteration: 520/1875 train_loss0.2961068167566107\n",
      "STEP: Train Epoch: 1/1 Iteration: 521/1875 train_loss0.29613913374018075\n",
      "STEP: Train Epoch: 1/1 Iteration: 522/1875 train_loss0.29620786683484057\n",
      "STEP: Train Epoch: 1/1 Iteration: 523/1875 train_loss0.2961208214466932\n",
      "STEP: Train Epoch: 1/1 Iteration: 524/1875 train_loss0.29591774267947857\n",
      "STEP: Train Epoch: 1/1 Iteration: 525/1875 train_loss0.29577174574136733\n",
      "STEP: Train Epoch: 1/1 Iteration: 526/1875 train_loss0.2953997267544723\n",
      "STEP: Train Epoch: 1/1 Iteration: 527/1875 train_loss0.2954190542083764\n",
      "STEP: Train Epoch: 1/1 Iteration: 528/1875 train_loss0.2954743063274884\n",
      "STEP: Train Epoch: 1/1 Iteration: 529/1875 train_loss0.29554970955409265\n",
      "STEP: Train Epoch: 1/1 Iteration: 530/1875 train_loss0.29542043823397385\n",
      "STEP: Train Epoch: 1/1 Iteration: 531/1875 train_loss0.2952281514326507\n",
      "STEP: Train Epoch: 1/1 Iteration: 532/1875 train_loss0.29522453896925416\n",
      "STEP: Train Epoch: 1/1 Iteration: 533/1875 train_loss0.29549657961608766\n",
      "STEP: Train Epoch: 1/1 Iteration: 534/1875 train_loss0.2954397276183416\n",
      "STEP: Train Epoch: 1/1 Iteration: 535/1875 train_loss0.29518833682637347\n",
      "STEP: Train Epoch: 1/1 Iteration: 536/1875 train_loss0.2951197003556499\n",
      "STEP: Train Epoch: 1/1 Iteration: 537/1875 train_loss0.2956658451778041\n",
      "STEP: Train Epoch: 1/1 Iteration: 538/1875 train_loss0.2961488430246323\n",
      "STEP: Train Epoch: 1/1 Iteration: 539/1875 train_loss0.29606615395349123\n",
      "STEP: Train Epoch: 1/1 Iteration: 540/1875 train_loss0.29604234876180135\n",
      "STEP: Train Epoch: 1/1 Iteration: 541/1875 train_loss0.296528217022939\n",
      "STEP: Train Epoch: 1/1 Iteration: 542/1875 train_loss0.2963169996121493\n",
      "STEP: Train Epoch: 1/1 Iteration: 543/1875 train_loss0.2960743113638727\n",
      "STEP: Train Epoch: 1/1 Iteration: 544/1875 train_loss0.2960399567724808\n",
      "STEP: Train Epoch: 1/1 Iteration: 545/1875 train_loss0.295906099472024\n",
      "STEP: Train Epoch: 1/1 Iteration: 546/1875 train_loss0.29578249908356\n",
      "STEP: Train Epoch: 1/1 Iteration: 547/1875 train_loss0.295999933953586\n",
      "STEP: Train Epoch: 1/1 Iteration: 548/1875 train_loss0.29593256342965757\n",
      "STEP: Train Epoch: 1/1 Iteration: 549/1875 train_loss0.29627964376914695\n",
      "STEP: Train Epoch: 1/1 Iteration: 550/1875 train_loss0.29627131607044827\n",
      "STEP: Train Epoch: 1/1 Iteration: 551/1875 train_loss0.29590214488726996\n",
      "STEP: Train Epoch: 1/1 Iteration: 552/1875 train_loss0.2960984325171381\n",
      "STEP: Train Epoch: 1/1 Iteration: 553/1875 train_loss0.29591009751043096\n",
      "STEP: Train Epoch: 1/1 Iteration: 554/1875 train_loss0.2960945010346626\n",
      "STEP: Train Epoch: 1/1 Iteration: 555/1875 train_loss0.29595924068141627\n",
      "STEP: Train Epoch: 1/1 Iteration: 556/1875 train_loss0.29623784215973437\n",
      "STEP: Train Epoch: 1/1 Iteration: 557/1875 train_loss0.296511189995377\n",
      "STEP: Train Epoch: 1/1 Iteration: 558/1875 train_loss0.2963479760513511\n",
      "STEP: Train Epoch: 1/1 Iteration: 559/1875 train_loss0.29635024006763383\n",
      "STEP: Train Epoch: 1/1 Iteration: 560/1875 train_loss0.29608502372034956\n",
      "STEP: Train Epoch: 1/1 Iteration: 561/1875 train_loss0.2959429670304538\n",
      "STEP: Train Epoch: 1/1 Iteration: 562/1875 train_loss0.29550637254405276\n",
      "STEP: Train Epoch: 1/1 Iteration: 563/1875 train_loss0.29551123278496744\n",
      "STEP: Train Epoch: 1/1 Iteration: 564/1875 train_loss0.2956866090126494\n",
      "STEP: Train Epoch: 1/1 Iteration: 565/1875 train_loss0.2958009523364295\n",
      "STEP: Train Epoch: 1/1 Iteration: 566/1875 train_loss0.29554919340374614\n",
      "STEP: Train Epoch: 1/1 Iteration: 567/1875 train_loss0.2952667085266618\n",
      "STEP: Train Epoch: 1/1 Iteration: 568/1875 train_loss0.29518963428030554\n",
      "STEP: Train Epoch: 1/1 Iteration: 569/1875 train_loss0.2952845206579131\n",
      "STEP: Train Epoch: 1/1 Iteration: 570/1875 train_loss0.2953273119633658\n",
      "STEP: Train Epoch: 1/1 Iteration: 571/1875 train_loss0.2954598776408962\n",
      "STEP: Train Epoch: 1/1 Iteration: 572/1875 train_loss0.2952100685113794\n",
      "STEP: Train Epoch: 1/1 Iteration: 573/1875 train_loss0.2951006806125608\n",
      "STEP: Train Epoch: 1/1 Iteration: 574/1875 train_loss0.2954764146007312\n",
      "STEP: Train Epoch: 1/1 Iteration: 575/1875 train_loss0.29548676558162856\n",
      "STEP: Train Epoch: 1/1 Iteration: 576/1875 train_loss0.295321251395055\n",
      "STEP: Train Epoch: 1/1 Iteration: 577/1875 train_loss0.29534746840776976\n",
      "STEP: Train Epoch: 1/1 Iteration: 578/1875 train_loss0.2953628699414458\n",
      "STEP: Train Epoch: 1/1 Iteration: 579/1875 train_loss0.2951572305797913\n",
      "STEP: Train Epoch: 1/1 Iteration: 580/1875 train_loss0.2951331721297626\n",
      "STEP: Train Epoch: 1/1 Iteration: 581/1875 train_loss0.2950056661529508\n",
      "STEP: Train Epoch: 1/1 Iteration: 582/1875 train_loss0.29505007911179076\n",
      "STEP: Train Epoch: 1/1 Iteration: 583/1875 train_loss0.29525828233713963\n",
      "STEP: Train Epoch: 1/1 Iteration: 584/1875 train_loss0.2951863182530011\n",
      "STEP: Train Epoch: 1/1 Iteration: 585/1875 train_loss0.2951609191222069\n",
      "STEP: Train Epoch: 1/1 Iteration: 586/1875 train_loss0.2952384997044814\n",
      "STEP: Train Epoch: 1/1 Iteration: 587/1875 train_loss0.2953881316737381\n",
      "STEP: Train Epoch: 1/1 Iteration: 588/1875 train_loss0.2952740273508085\n",
      "STEP: Train Epoch: 1/1 Iteration: 589/1875 train_loss0.29523736820358576\n",
      "STEP: Train Epoch: 1/1 Iteration: 590/1875 train_loss0.295419661877519\n",
      "STEP: Train Epoch: 1/1 Iteration: 591/1875 train_loss0.29542358222144915\n",
      "STEP: Train Epoch: 1/1 Iteration: 592/1875 train_loss0.2950686427663911\n",
      "STEP: Train Epoch: 1/1 Iteration: 593/1875 train_loss0.29522988641191694\n",
      "STEP: Train Epoch: 1/1 Iteration: 594/1875 train_loss0.2950890777312746\n",
      "STEP: Train Epoch: 1/1 Iteration: 595/1875 train_loss0.29524009118811423\n",
      "STEP: Train Epoch: 1/1 Iteration: 596/1875 train_loss0.29506911601146196\n",
      "STEP: Train Epoch: 1/1 Iteration: 597/1875 train_loss0.29569691493233646\n",
      "STEP: Train Epoch: 1/1 Iteration: 598/1875 train_loss0.2957024640437951\n",
      "STEP: Train Epoch: 1/1 Iteration: 599/1875 train_loss0.2955727796249079\n",
      "STEP: Train Epoch: 1/1 Iteration: 600/1875 train_loss0.29538785861184197\n",
      "STEP: Train Epoch: 1/1 Iteration: 601/1875 train_loss0.2952958889020461\n",
      "STEP: Train Epoch: 1/1 Iteration: 602/1875 train_loss0.2958737986156117\n",
      "STEP: Train Epoch: 1/1 Iteration: 603/1875 train_loss0.29548183923732385\n",
      "STEP: Train Epoch: 1/1 Iteration: 604/1875 train_loss0.29594533969432313\n",
      "STEP: Train Epoch: 1/1 Iteration: 605/1875 train_loss0.29564192701469766\n",
      "STEP: Train Epoch: 1/1 Iteration: 606/1875 train_loss0.2954389790300489\n",
      "STEP: Train Epoch: 1/1 Iteration: 607/1875 train_loss0.2956593332710926\n",
      "STEP: Train Epoch: 1/1 Iteration: 608/1875 train_loss0.2957332189145841\n",
      "STEP: Train Epoch: 1/1 Iteration: 609/1875 train_loss0.2958230474605936\n",
      "STEP: Train Epoch: 1/1 Iteration: 610/1875 train_loss0.2957362678451616\n",
      "STEP: Train Epoch: 1/1 Iteration: 611/1875 train_loss0.2956873387488523\n",
      "STEP: Train Epoch: 1/1 Iteration: 612/1875 train_loss0.29591985721311537\n",
      "STEP: Train Epoch: 1/1 Iteration: 613/1875 train_loss0.2959855133470081\n",
      "STEP: Train Epoch: 1/1 Iteration: 614/1875 train_loss0.29604322856431675\n",
      "STEP: Train Epoch: 1/1 Iteration: 615/1875 train_loss0.29603459013671407\n",
      "STEP: Train Epoch: 1/1 Iteration: 616/1875 train_loss0.2959777491965464\n",
      "STEP: Train Epoch: 1/1 Iteration: 617/1875 train_loss0.29583144658871563\n",
      "STEP: Train Epoch: 1/1 Iteration: 618/1875 train_loss0.29593736191301284\n",
      "STEP: Train Epoch: 1/1 Iteration: 619/1875 train_loss0.2958859270002229\n",
      "STEP: Train Epoch: 1/1 Iteration: 620/1875 train_loss0.2956654950976372\n",
      "STEP: Train Epoch: 1/1 Iteration: 621/1875 train_loss0.2957374553845509\n",
      "STEP: Train Epoch: 1/1 Iteration: 622/1875 train_loss0.295639505723665\n",
      "STEP: Train Epoch: 1/1 Iteration: 623/1875 train_loss0.29567028321385574\n",
      "STEP: Train Epoch: 1/1 Iteration: 624/1875 train_loss0.29566144799956906\n",
      "STEP: Train Epoch: 1/1 Iteration: 625/1875 train_loss0.2956677480697632\n",
      "STEP: Train Epoch: 1/1 Iteration: 626/1875 train_loss0.2954415690165739\n",
      "STEP: Train Epoch: 1/1 Iteration: 627/1875 train_loss0.29542712948063726\n",
      "STEP: Train Epoch: 1/1 Iteration: 628/1875 train_loss0.2960330786741084\n",
      "STEP: Train Epoch: 1/1 Iteration: 629/1875 train_loss0.29607269890452415\n",
      "STEP: Train Epoch: 1/1 Iteration: 630/1875 train_loss0.2961099015578391\n",
      "STEP: Train Epoch: 1/1 Iteration: 631/1875 train_loss0.2961042589742673\n",
      "STEP: Train Epoch: 1/1 Iteration: 632/1875 train_loss0.296304416689503\n",
      "STEP: Train Epoch: 1/1 Iteration: 633/1875 train_loss0.29611165586146887\n",
      "STEP: Train Epoch: 1/1 Iteration: 634/1875 train_loss0.296026763691323\n",
      "STEP: Train Epoch: 1/1 Iteration: 635/1875 train_loss0.2957381622411135\n",
      "STEP: Train Epoch: 1/1 Iteration: 636/1875 train_loss0.29572654308153773\n",
      "STEP: Train Epoch: 1/1 Iteration: 637/1875 train_loss0.29586342256662423\n",
      "STEP: Train Epoch: 1/1 Iteration: 638/1875 train_loss0.2959901571250448\n",
      "STEP: Train Epoch: 1/1 Iteration: 639/1875 train_loss0.2958476394764694\n",
      "STEP: Train Epoch: 1/1 Iteration: 640/1875 train_loss0.2959758658078499\n",
      "STEP: Train Epoch: 1/1 Iteration: 641/1875 train_loss0.29587367129353764\n",
      "STEP: Train Epoch: 1/1 Iteration: 642/1875 train_loss0.29577869621346303\n",
      "STEP: Train Epoch: 1/1 Iteration: 643/1875 train_loss0.2959098302256635\n",
      "STEP: Train Epoch: 1/1 Iteration: 644/1875 train_loss0.2958239332790145\n",
      "STEP: Train Epoch: 1/1 Iteration: 645/1875 train_loss0.2960512156287829\n",
      "STEP: Train Epoch: 1/1 Iteration: 646/1875 train_loss0.29600754109191085\n",
      "STEP: Train Epoch: 1/1 Iteration: 647/1875 train_loss0.29621742673828577\n",
      "STEP: Train Epoch: 1/1 Iteration: 648/1875 train_loss0.29610023112898626\n",
      "STEP: Train Epoch: 1/1 Iteration: 649/1875 train_loss0.2959761888087255\n",
      "STEP: Train Epoch: 1/1 Iteration: 650/1875 train_loss0.29570948102153266\n",
      "STEP: Train Epoch: 1/1 Iteration: 651/1875 train_loss0.2955266243324668\n",
      "STEP: Train Epoch: 1/1 Iteration: 652/1875 train_loss0.2956434743924558\n",
      "STEP: Train Epoch: 1/1 Iteration: 653/1875 train_loss0.2955722084659712\n",
      "STEP: Train Epoch: 1/1 Iteration: 654/1875 train_loss0.2952603470385986\n",
      "STEP: Train Epoch: 1/1 Iteration: 655/1875 train_loss0.29528156750984774\n",
      "STEP: Train Epoch: 1/1 Iteration: 656/1875 train_loss0.29544088162663507\n",
      "STEP: Train Epoch: 1/1 Iteration: 657/1875 train_loss0.2956179787307025\n",
      "STEP: Train Epoch: 1/1 Iteration: 658/1875 train_loss0.2959486983920303\n",
      "STEP: Train Epoch: 1/1 Iteration: 659/1875 train_loss0.29625894624474197\n",
      "STEP: Train Epoch: 1/1 Iteration: 660/1875 train_loss0.2960236444166212\n",
      "STEP: Train Epoch: 1/1 Iteration: 661/1875 train_loss0.29581252310802647\n",
      "STEP: Train Epoch: 1/1 Iteration: 662/1875 train_loss0.2958157669606526\n",
      "STEP: Train Epoch: 1/1 Iteration: 663/1875 train_loss0.2957855993221608\n",
      "STEP: Train Epoch: 1/1 Iteration: 664/1875 train_loss0.2958047932844205\n",
      "STEP: Train Epoch: 1/1 Iteration: 665/1875 train_loss0.2959361569101649\n",
      "STEP: Train Epoch: 1/1 Iteration: 666/1875 train_loss0.2960421053004694\n",
      "STEP: Train Epoch: 1/1 Iteration: 667/1875 train_loss0.29600034470947784\n",
      "STEP: Train Epoch: 1/1 Iteration: 668/1875 train_loss0.29584567335819056\n",
      "STEP: Train Epoch: 1/1 Iteration: 669/1875 train_loss0.29575558073584807\n",
      "STEP: Train Epoch: 1/1 Iteration: 670/1875 train_loss0.2956372577307829\n",
      "STEP: Train Epoch: 1/1 Iteration: 671/1875 train_loss0.295790903658874\n",
      "STEP: Train Epoch: 1/1 Iteration: 672/1875 train_loss0.29575922131715787\n",
      "STEP: Train Epoch: 1/1 Iteration: 673/1875 train_loss0.29551167949967716\n",
      "STEP: Train Epoch: 1/1 Iteration: 674/1875 train_loss0.2957381331734912\n",
      "STEP: Train Epoch: 1/1 Iteration: 675/1875 train_loss0.29543171526105316\n",
      "STEP: Train Epoch: 1/1 Iteration: 676/1875 train_loss0.295251066019461\n",
      "STEP: Train Epoch: 1/1 Iteration: 677/1875 train_loss0.29514835093671143\n",
      "STEP: Train Epoch: 1/1 Iteration: 678/1875 train_loss0.29529199719560884\n",
      "STEP: Train Epoch: 1/1 Iteration: 679/1875 train_loss0.29552558993863076\n",
      "STEP: Train Epoch: 1/1 Iteration: 680/1875 train_loss0.2955696902621318\n",
      "STEP: Train Epoch: 1/1 Iteration: 681/1875 train_loss0.29551978747356306\n",
      "STEP: Train Epoch: 1/1 Iteration: 682/1875 train_loss0.295630401388204\n",
      "STEP: Train Epoch: 1/1 Iteration: 683/1875 train_loss0.29550904023568764\n",
      "STEP: Train Epoch: 1/1 Iteration: 684/1875 train_loss0.29520569514069295\n",
      "STEP: Train Epoch: 1/1 Iteration: 685/1875 train_loss0.29523298225916217\n",
      "STEP: Train Epoch: 1/1 Iteration: 686/1875 train_loss0.29496514809157687\n",
      "STEP: Train Epoch: 1/1 Iteration: 687/1875 train_loss0.2947929082804372\n",
      "STEP: Train Epoch: 1/1 Iteration: 688/1875 train_loss0.2947929073450004\n",
      "STEP: Train Epoch: 1/1 Iteration: 689/1875 train_loss0.29461336857132364\n",
      "STEP: Train Epoch: 1/1 Iteration: 690/1875 train_loss0.2945210058512031\n",
      "STEP: Train Epoch: 1/1 Iteration: 691/1875 train_loss0.29473930285696354\n",
      "STEP: Train Epoch: 1/1 Iteration: 692/1875 train_loss0.2947694228456027\n",
      "STEP: Train Epoch: 1/1 Iteration: 693/1875 train_loss0.2946856443650623\n",
      "STEP: Train Epoch: 1/1 Iteration: 694/1875 train_loss0.2946729120564907\n",
      "STEP: Train Epoch: 1/1 Iteration: 695/1875 train_loss0.2945778461347381\n",
      "STEP: Train Epoch: 1/1 Iteration: 696/1875 train_loss0.29451527940002326\n",
      "STEP: Train Epoch: 1/1 Iteration: 697/1875 train_loss0.29435862097327986\n",
      "STEP: Train Epoch: 1/1 Iteration: 698/1875 train_loss0.2940765700615238\n",
      "STEP: Train Epoch: 1/1 Iteration: 699/1875 train_loss0.29383192086680254\n",
      "STEP: Train Epoch: 1/1 Iteration: 700/1875 train_loss0.2939988757244178\n",
      "STEP: Train Epoch: 1/1 Iteration: 701/1875 train_loss0.293851968969156\n",
      "STEP: Train Epoch: 1/1 Iteration: 702/1875 train_loss0.2938632424443196\n",
      "STEP: Train Epoch: 1/1 Iteration: 703/1875 train_loss0.29388585392051875\n",
      "STEP: Train Epoch: 1/1 Iteration: 704/1875 train_loss0.2939161948478696\n",
      "STEP: Train Epoch: 1/1 Iteration: 705/1875 train_loss0.29392269260071696\n",
      "STEP: Train Epoch: 1/1 Iteration: 706/1875 train_loss0.2940276772754726\n",
      "STEP: Train Epoch: 1/1 Iteration: 707/1875 train_loss0.294014198165466\n",
      "STEP: Train Epoch: 1/1 Iteration: 708/1875 train_loss0.2940075867888281\n",
      "STEP: Train Epoch: 1/1 Iteration: 709/1875 train_loss0.29408363531355464\n",
      "STEP: Train Epoch: 1/1 Iteration: 710/1875 train_loss0.29407084934728245\n",
      "STEP: Train Epoch: 1/1 Iteration: 711/1875 train_loss0.29419357383049993\n",
      "STEP: Train Epoch: 1/1 Iteration: 712/1875 train_loss0.294070441694407\n",
      "STEP: Train Epoch: 1/1 Iteration: 713/1875 train_loss0.2945358188506764\n",
      "STEP: Train Epoch: 1/1 Iteration: 714/1875 train_loss0.2944123262659508\n",
      "STEP: Train Epoch: 1/1 Iteration: 715/1875 train_loss0.2941776497380717\n",
      "STEP: Train Epoch: 1/1 Iteration: 716/1875 train_loss0.29417440553284224\n",
      "STEP: Train Epoch: 1/1 Iteration: 717/1875 train_loss0.2946329220734547\n",
      "STEP: Train Epoch: 1/1 Iteration: 718/1875 train_loss0.2944570783743619\n",
      "STEP: Train Epoch: 1/1 Iteration: 719/1875 train_loss0.294666342281831\n",
      "STEP: Train Epoch: 1/1 Iteration: 720/1875 train_loss0.29439153890642855\n",
      "STEP: Train Epoch: 1/1 Iteration: 721/1875 train_loss0.29488916341706223\n",
      "STEP: Train Epoch: 1/1 Iteration: 722/1875 train_loss0.29471547980057566\n",
      "STEP: Train Epoch: 1/1 Iteration: 723/1875 train_loss0.29464467813777395\n",
      "STEP: Train Epoch: 1/1 Iteration: 724/1875 train_loss0.2945865144263644\n",
      "STEP: Train Epoch: 1/1 Iteration: 725/1875 train_loss0.29451834059994797\n",
      "STEP: Train Epoch: 1/1 Iteration: 726/1875 train_loss0.29429876964446927\n",
      "STEP: Train Epoch: 1/1 Iteration: 727/1875 train_loss0.29427685456394004\n",
      "STEP: Train Epoch: 1/1 Iteration: 728/1875 train_loss0.2940898682144317\n",
      "STEP: Train Epoch: 1/1 Iteration: 729/1875 train_loss0.29401980166736125\n",
      "STEP: Train Epoch: 1/1 Iteration: 730/1875 train_loss0.2941777765750885\n",
      "STEP: Train Epoch: 1/1 Iteration: 731/1875 train_loss0.2942596706988547\n",
      "STEP: Train Epoch: 1/1 Iteration: 732/1875 train_loss0.29423749874361227\n",
      "STEP: Train Epoch: 1/1 Iteration: 733/1875 train_loss0.2943101743640223\n",
      "STEP: Train Epoch: 1/1 Iteration: 734/1875 train_loss0.29415763888193414\n",
      "STEP: Train Epoch: 1/1 Iteration: 735/1875 train_loss0.2946533530139599\n",
      "STEP: Train Epoch: 1/1 Iteration: 736/1875 train_loss0.2947929500926124\n",
      "STEP: Train Epoch: 1/1 Iteration: 737/1875 train_loss0.29468375680970693\n",
      "STEP: Train Epoch: 1/1 Iteration: 738/1875 train_loss0.29470579477587366\n",
      "STEP: Train Epoch: 1/1 Iteration: 739/1875 train_loss0.29470189234319977\n",
      "STEP: Train Epoch: 1/1 Iteration: 740/1875 train_loss0.29470139143837465\n",
      "STEP: Train Epoch: 1/1 Iteration: 741/1875 train_loss0.29481827229382057\n",
      "STEP: Train Epoch: 1/1 Iteration: 742/1875 train_loss0.2947134399871942\n",
      "STEP: Train Epoch: 1/1 Iteration: 743/1875 train_loss0.2946015075952176\n",
      "STEP: Train Epoch: 1/1 Iteration: 744/1875 train_loss0.29455258246631394\n",
      "STEP: Train Epoch: 1/1 Iteration: 745/1875 train_loss0.2943156688005332\n",
      "STEP: Train Epoch: 1/1 Iteration: 746/1875 train_loss0.29449516618699234\n",
      "STEP: Train Epoch: 1/1 Iteration: 747/1875 train_loss0.2945839312261686\n",
      "STEP: Train Epoch: 1/1 Iteration: 748/1875 train_loss0.29474615082543165\n",
      "STEP: Train Epoch: 1/1 Iteration: 749/1875 train_loss0.2948850680973247\n",
      "STEP: Train Epoch: 1/1 Iteration: 750/1875 train_loss0.29499444019794463\n",
      "STEP: Train Epoch: 1/1 Iteration: 751/1875 train_loss0.29506437105440425\n",
      "STEP: Train Epoch: 1/1 Iteration: 752/1875 train_loss0.2949521101694158\n",
      "STEP: Train Epoch: 1/1 Iteration: 753/1875 train_loss0.29470853009823944\n",
      "STEP: Train Epoch: 1/1 Iteration: 754/1875 train_loss0.29494379313578656\n",
      "STEP: Train Epoch: 1/1 Iteration: 755/1875 train_loss0.2951319450870255\n",
      "STEP: Train Epoch: 1/1 Iteration: 756/1875 train_loss0.294970326431135\n",
      "STEP: Train Epoch: 1/1 Iteration: 757/1875 train_loss0.29504000722299006\n",
      "STEP: Train Epoch: 1/1 Iteration: 758/1875 train_loss0.2951356691841557\n",
      "STEP: Train Epoch: 1/1 Iteration: 759/1875 train_loss0.2949373530560174\n",
      "STEP: Train Epoch: 1/1 Iteration: 760/1875 train_loss0.294859298877418\n",
      "STEP: Train Epoch: 1/1 Iteration: 761/1875 train_loss0.2947530252136946\n",
      "STEP: Train Epoch: 1/1 Iteration: 762/1875 train_loss0.2945941002095935\n",
      "STEP: Train Epoch: 1/1 Iteration: 763/1875 train_loss0.29468813420396645\n",
      "STEP: Train Epoch: 1/1 Iteration: 764/1875 train_loss0.2945348285236128\n",
      "STEP: Train Epoch: 1/1 Iteration: 765/1875 train_loss0.29465703226966794\n",
      "STEP: Train Epoch: 1/1 Iteration: 766/1875 train_loss0.2947252270505416\n",
      "STEP: Train Epoch: 1/1 Iteration: 767/1875 train_loss0.2948325395176122\n",
      "STEP: Train Epoch: 1/1 Iteration: 768/1875 train_loss0.2947833799892881\n",
      "STEP: Train Epoch: 1/1 Iteration: 769/1875 train_loss0.2949435530213448\n",
      "STEP: Train Epoch: 1/1 Iteration: 770/1875 train_loss0.29496505240147763\n",
      "STEP: Train Epoch: 1/1 Iteration: 771/1875 train_loss0.2951361889408565\n",
      "STEP: Train Epoch: 1/1 Iteration: 772/1875 train_loss0.2952301373193764\n",
      "STEP: Train Epoch: 1/1 Iteration: 773/1875 train_loss0.29526117292713533\n",
      "STEP: Train Epoch: 1/1 Iteration: 774/1875 train_loss0.29526921520515004\n",
      "STEP: Train Epoch: 1/1 Iteration: 775/1875 train_loss0.2952634785540642\n",
      "STEP: Train Epoch: 1/1 Iteration: 776/1875 train_loss0.2952937672644393\n",
      "STEP: Train Epoch: 1/1 Iteration: 777/1875 train_loss0.29537043545965674\n",
      "STEP: Train Epoch: 1/1 Iteration: 778/1875 train_loss0.2956575506048368\n",
      "STEP: Train Epoch: 1/1 Iteration: 779/1875 train_loss0.29589240957859697\n",
      "STEP: Train Epoch: 1/1 Iteration: 780/1875 train_loss0.2956969264703683\n",
      "STEP: Train Epoch: 1/1 Iteration: 781/1875 train_loss0.2960605761477492\n",
      "STEP: Train Epoch: 1/1 Iteration: 782/1875 train_loss0.29611749528809583\n",
      "STEP: Train Epoch: 1/1 Iteration: 783/1875 train_loss0.2960196941957772\n",
      "STEP: Train Epoch: 1/1 Iteration: 784/1875 train_loss0.29593511660374244\n",
      "STEP: Train Epoch: 1/1 Iteration: 785/1875 train_loss0.29607155957988873\n",
      "STEP: Train Epoch: 1/1 Iteration: 786/1875 train_loss0.2960707746711548\n",
      "STEP: Train Epoch: 1/1 Iteration: 787/1875 train_loss0.2960297845995108\n",
      "STEP: Train Epoch: 1/1 Iteration: 788/1875 train_loss0.29591397565746974\n",
      "STEP: Train Epoch: 1/1 Iteration: 789/1875 train_loss0.2959480370537712\n",
      "STEP: Train Epoch: 1/1 Iteration: 790/1875 train_loss0.29601853877494605\n",
      "STEP: Train Epoch: 1/1 Iteration: 791/1875 train_loss0.2960517380372462\n",
      "STEP: Train Epoch: 1/1 Iteration: 792/1875 train_loss0.2959591263453617\n",
      "STEP: Train Epoch: 1/1 Iteration: 793/1875 train_loss0.2958485775287786\n",
      "STEP: Train Epoch: 1/1 Iteration: 794/1875 train_loss0.2959458688250567\n",
      "STEP: Train Epoch: 1/1 Iteration: 795/1875 train_loss0.29591168347957\n",
      "STEP: Train Epoch: 1/1 Iteration: 796/1875 train_loss0.2958368984841971\n",
      "STEP: Train Epoch: 1/1 Iteration: 797/1875 train_loss0.29577068893830183\n",
      "STEP: Train Epoch: 1/1 Iteration: 798/1875 train_loss0.2957909057257618\n",
      "STEP: Train Epoch: 1/1 Iteration: 799/1875 train_loss0.2955912946815485\n",
      "STEP: Train Epoch: 1/1 Iteration: 800/1875 train_loss0.29549990556202826\n",
      "STEP: Train Epoch: 1/1 Iteration: 801/1875 train_loss0.29540057168731676\n",
      "STEP: Train Epoch: 1/1 Iteration: 802/1875 train_loss0.2952095039300342\n",
      "STEP: Train Epoch: 1/1 Iteration: 803/1875 train_loss0.29527784955419906\n",
      "STEP: Train Epoch: 1/1 Iteration: 804/1875 train_loss0.2955670350356333\n",
      "STEP: Train Epoch: 1/1 Iteration: 805/1875 train_loss0.29555005674591717\n",
      "STEP: Train Epoch: 1/1 Iteration: 806/1875 train_loss0.2953726442498398\n",
      "STEP: Train Epoch: 1/1 Iteration: 807/1875 train_loss0.2954574553699889\n",
      "STEP: Train Epoch: 1/1 Iteration: 808/1875 train_loss0.295385168229073\n",
      "STEP: Train Epoch: 1/1 Iteration: 809/1875 train_loss0.2954431003505279\n",
      "STEP: Train Epoch: 1/1 Iteration: 810/1875 train_loss0.2952245451969865\n",
      "STEP: Train Epoch: 1/1 Iteration: 811/1875 train_loss0.29509173912419934\n",
      "STEP: Train Epoch: 1/1 Iteration: 812/1875 train_loss0.29525054793052485\n",
      "STEP: Train Epoch: 1/1 Iteration: 813/1875 train_loss0.2951113353466196\n",
      "STEP: Train Epoch: 1/1 Iteration: 814/1875 train_loss0.29490799775360843\n",
      "STEP: Train Epoch: 1/1 Iteration: 815/1875 train_loss0.2948430962357784\n",
      "STEP: Train Epoch: 1/1 Iteration: 816/1875 train_loss0.29471414300667886\n",
      "STEP: Train Epoch: 1/1 Iteration: 817/1875 train_loss0.29455905149587075\n",
      "STEP: Train Epoch: 1/1 Iteration: 818/1875 train_loss0.29470691678722216\n",
      "STEP: Train Epoch: 1/1 Iteration: 819/1875 train_loss0.2947433521386614\n",
      "STEP: Train Epoch: 1/1 Iteration: 820/1875 train_loss0.2948954642909329\n",
      "STEP: Train Epoch: 1/1 Iteration: 821/1875 train_loss0.29479255335775273\n",
      "STEP: Train Epoch: 1/1 Iteration: 822/1875 train_loss0.29458244299003966\n",
      "STEP: Train Epoch: 1/1 Iteration: 823/1875 train_loss0.29480636661209136\n",
      "STEP: Train Epoch: 1/1 Iteration: 824/1875 train_loss0.2951085633119044\n",
      "STEP: Train Epoch: 1/1 Iteration: 825/1875 train_loss0.2951373479041186\n",
      "STEP: Train Epoch: 1/1 Iteration: 826/1875 train_loss0.29491758270789004\n",
      "STEP: Train Epoch: 1/1 Iteration: 827/1875 train_loss0.2948511285209598\n",
      "STEP: Train Epoch: 1/1 Iteration: 828/1875 train_loss0.2949460506403216\n",
      "STEP: Train Epoch: 1/1 Iteration: 829/1875 train_loss0.2948826137296541\n",
      "STEP: Train Epoch: 1/1 Iteration: 830/1875 train_loss0.2947777834103768\n",
      "STEP: Train Epoch: 1/1 Iteration: 831/1875 train_loss0.29464398894714533\n",
      "STEP: Train Epoch: 1/1 Iteration: 832/1875 train_loss0.2948517477533852\n",
      "STEP: Train Epoch: 1/1 Iteration: 833/1875 train_loss0.29459970412241454\n",
      "STEP: Train Epoch: 1/1 Iteration: 834/1875 train_loss0.2948179066592126\n",
      "STEP: Train Epoch: 1/1 Iteration: 835/1875 train_loss0.29491427757604394\n",
      "STEP: Train Epoch: 1/1 Iteration: 836/1875 train_loss0.2947587415878995\n",
      "STEP: Train Epoch: 1/1 Iteration: 837/1875 train_loss0.29473283229086417\n",
      "STEP: Train Epoch: 1/1 Iteration: 838/1875 train_loss0.29469317678797785\n",
      "STEP: Train Epoch: 1/1 Iteration: 839/1875 train_loss0.2945980657251714\n",
      "STEP: Train Epoch: 1/1 Iteration: 840/1875 train_loss0.2946708769581857\n",
      "STEP: Train Epoch: 1/1 Iteration: 841/1875 train_loss0.2950625480833717\n",
      "STEP: Train Epoch: 1/1 Iteration: 842/1875 train_loss0.2951243646761442\n",
      "STEP: Train Epoch: 1/1 Iteration: 843/1875 train_loss0.2951146430940249\n",
      "STEP: Train Epoch: 1/1 Iteration: 844/1875 train_loss0.2950508799757014\n",
      "STEP: Train Epoch: 1/1 Iteration: 845/1875 train_loss0.2952268681670787\n",
      "STEP: Train Epoch: 1/1 Iteration: 846/1875 train_loss0.29526517113085055\n",
      "STEP: Train Epoch: 1/1 Iteration: 847/1875 train_loss0.2953823291583354\n",
      "STEP: Train Epoch: 1/1 Iteration: 848/1875 train_loss0.295124579616383\n",
      "STEP: Train Epoch: 1/1 Iteration: 849/1875 train_loss0.29486713422411887\n",
      "STEP: Train Epoch: 1/1 Iteration: 850/1875 train_loss0.294773155520944\n",
      "STEP: Train Epoch: 1/1 Iteration: 851/1875 train_loss0.2946383559990994\n",
      "STEP: Train Epoch: 1/1 Iteration: 852/1875 train_loss0.2949464321661163\n",
      "STEP: Train Epoch: 1/1 Iteration: 853/1875 train_loss0.29495420131356326\n",
      "STEP: Train Epoch: 1/1 Iteration: 854/1875 train_loss0.2950457669358343\n",
      "STEP: Train Epoch: 1/1 Iteration: 855/1875 train_loss0.295066771399208\n",
      "STEP: Train Epoch: 1/1 Iteration: 856/1875 train_loss0.29526465948427394\n",
      "STEP: Train Epoch: 1/1 Iteration: 857/1875 train_loss0.2952389177913009\n",
      "STEP: Train Epoch: 1/1 Iteration: 858/1875 train_loss0.2951063034244073\n",
      "STEP: Train Epoch: 1/1 Iteration: 859/1875 train_loss0.29487863686087523\n",
      "STEP: Train Epoch: 1/1 Iteration: 860/1875 train_loss0.29480806001743604\n",
      "STEP: Train Epoch: 1/1 Iteration: 861/1875 train_loss0.2945744475473095\n",
      "STEP: Train Epoch: 1/1 Iteration: 862/1875 train_loss0.29454763775600634\n",
      "STEP: Train Epoch: 1/1 Iteration: 863/1875 train_loss0.2943590028105716\n",
      "STEP: Train Epoch: 1/1 Iteration: 864/1875 train_loss0.2945287072668887\n",
      "STEP: Train Epoch: 1/1 Iteration: 865/1875 train_loss0.29452799578450317\n",
      "STEP: Train Epoch: 1/1 Iteration: 866/1875 train_loss0.29493399049579133\n",
      "STEP: Train Epoch: 1/1 Iteration: 867/1875 train_loss0.2950323882245275\n",
      "STEP: Train Epoch: 1/1 Iteration: 868/1875 train_loss0.29517720055161256\n",
      "STEP: Train Epoch: 1/1 Iteration: 869/1875 train_loss0.2952498446451232\n",
      "STEP: Train Epoch: 1/1 Iteration: 870/1875 train_loss0.29511822724993203\n",
      "STEP: Train Epoch: 1/1 Iteration: 871/1875 train_loss0.29485265644594394\n",
      "STEP: Train Epoch: 1/1 Iteration: 872/1875 train_loss0.2947616957965384\n",
      "STEP: Train Epoch: 1/1 Iteration: 873/1875 train_loss0.2947000719581273\n",
      "STEP: Train Epoch: 1/1 Iteration: 874/1875 train_loss0.2946582784795243\n",
      "STEP: Train Epoch: 1/1 Iteration: 875/1875 train_loss0.29446988772494453\n",
      "STEP: Train Epoch: 1/1 Iteration: 876/1875 train_loss0.2948993694751638\n",
      "STEP: Train Epoch: 1/1 Iteration: 877/1875 train_loss0.29508527028485876\n",
      "STEP: Train Epoch: 1/1 Iteration: 878/1875 train_loss0.294947284492314\n",
      "STEP: Train Epoch: 1/1 Iteration: 879/1875 train_loss0.2950635338382238\n",
      "STEP: Train Epoch: 1/1 Iteration: 880/1875 train_loss0.29490285665643484\n",
      "STEP: Train Epoch: 1/1 Iteration: 881/1875 train_loss0.29488909989595413\n",
      "STEP: Train Epoch: 1/1 Iteration: 882/1875 train_loss0.2949759138836747\n",
      "STEP: Train Epoch: 1/1 Iteration: 883/1875 train_loss0.29511606143966806\n",
      "STEP: Train Epoch: 1/1 Iteration: 884/1875 train_loss0.29532715287986655\n",
      "STEP: Train Epoch: 1/1 Iteration: 885/1875 train_loss0.2952958833447284\n",
      "STEP: Train Epoch: 1/1 Iteration: 886/1875 train_loss0.2951046649795488\n",
      "STEP: Train Epoch: 1/1 Iteration: 887/1875 train_loss0.29526342516250514\n",
      "STEP: Train Epoch: 1/1 Iteration: 888/1875 train_loss0.29536485372463595\n",
      "STEP: Train Epoch: 1/1 Iteration: 889/1875 train_loss0.29539412570985285\n",
      "STEP: Train Epoch: 1/1 Iteration: 890/1875 train_loss0.2951622325048018\n",
      "STEP: Train Epoch: 1/1 Iteration: 891/1875 train_loss0.2951791350020035\n",
      "STEP: Train Epoch: 1/1 Iteration: 892/1875 train_loss0.29499329666653024\n",
      "STEP: Train Epoch: 1/1 Iteration: 893/1875 train_loss0.29493331325147615\n",
      "STEP: Train Epoch: 1/1 Iteration: 894/1875 train_loss0.29503435743201767\n",
      "STEP: Train Epoch: 1/1 Iteration: 895/1875 train_loss0.2951085896132379\n",
      "STEP: Train Epoch: 1/1 Iteration: 896/1875 train_loss0.2950952138552176\n",
      "STEP: Train Epoch: 1/1 Iteration: 897/1875 train_loss0.2952619705311829\n",
      "STEP: Train Epoch: 1/1 Iteration: 898/1875 train_loss0.29519898129133443\n",
      "STEP: Train Epoch: 1/1 Iteration: 899/1875 train_loss0.295314501552481\n",
      "STEP: Train Epoch: 1/1 Iteration: 900/1875 train_loss0.29526506369312605\n",
      "STEP: Train Epoch: 1/1 Iteration: 901/1875 train_loss0.29544849425851966\n",
      "STEP: Train Epoch: 1/1 Iteration: 902/1875 train_loss0.2954034087167611\n",
      "STEP: Train Epoch: 1/1 Iteration: 903/1875 train_loss0.295221897850797\n",
      "STEP: Train Epoch: 1/1 Iteration: 904/1875 train_loss0.2950684115686248\n",
      "STEP: Train Epoch: 1/1 Iteration: 905/1875 train_loss0.2950364401985927\n",
      "STEP: Train Epoch: 1/1 Iteration: 906/1875 train_loss0.295201667130125\n",
      "STEP: Train Epoch: 1/1 Iteration: 907/1875 train_loss0.2950905659857238\n",
      "STEP: Train Epoch: 1/1 Iteration: 908/1875 train_loss0.29503467158795976\n",
      "STEP: Train Epoch: 1/1 Iteration: 909/1875 train_loss0.29496819884142333\n",
      "STEP: Train Epoch: 1/1 Iteration: 910/1875 train_loss0.2950553936781464\n",
      "STEP: Train Epoch: 1/1 Iteration: 911/1875 train_loss0.29482964594755423\n",
      "STEP: Train Epoch: 1/1 Iteration: 912/1875 train_loss0.2946497620933019\n",
      "STEP: Train Epoch: 1/1 Iteration: 913/1875 train_loss0.2946964610788282\n",
      "STEP: Train Epoch: 1/1 Iteration: 914/1875 train_loss0.29463820812380026\n",
      "STEP: Train Epoch: 1/1 Iteration: 915/1875 train_loss0.2954447864590447\n",
      "STEP: Train Epoch: 1/1 Iteration: 916/1875 train_loss0.2954290353135901\n",
      "STEP: Train Epoch: 1/1 Iteration: 917/1875 train_loss0.2955797503873982\n",
      "STEP: Train Epoch: 1/1 Iteration: 918/1875 train_loss0.2955262069559954\n",
      "STEP: Train Epoch: 1/1 Iteration: 919/1875 train_loss0.29536678009667516\n",
      "STEP: Train Epoch: 1/1 Iteration: 920/1875 train_loss0.2956349674207361\n",
      "STEP: Train Epoch: 1/1 Iteration: 921/1875 train_loss0.2957121650329371\n",
      "STEP: Train Epoch: 1/1 Iteration: 922/1875 train_loss0.29597283587053647\n",
      "STEP: Train Epoch: 1/1 Iteration: 923/1875 train_loss0.2962701169618548\n",
      "STEP: Train Epoch: 1/1 Iteration: 924/1875 train_loss0.29620439789028136\n",
      "STEP: Train Epoch: 1/1 Iteration: 925/1875 train_loss0.2962812322619799\n",
      "STEP: Train Epoch: 1/1 Iteration: 926/1875 train_loss0.296099506407075\n",
      "STEP: Train Epoch: 1/1 Iteration: 927/1875 train_loss0.29612162218156884\n",
      "STEP: Train Epoch: 1/1 Iteration: 928/1875 train_loss0.2962206864231748\n",
      "STEP: Train Epoch: 1/1 Iteration: 929/1875 train_loss0.296189446856375\n",
      "STEP: Train Epoch: 1/1 Iteration: 930/1875 train_loss0.2961049265839079\n",
      "STEP: Train Epoch: 1/1 Iteration: 931/1875 train_loss0.2958683571453125\n",
      "STEP: Train Epoch: 1/1 Iteration: 932/1875 train_loss0.295950812347585\n",
      "STEP: Train Epoch: 1/1 Iteration: 933/1875 train_loss0.2958449498058515\n",
      "STEP: Train Epoch: 1/1 Iteration: 934/1875 train_loss0.2957891310531271\n",
      "STEP: Train Epoch: 1/1 Iteration: 935/1875 train_loss0.2956731025388534\n",
      "STEP: Train Epoch: 1/1 Iteration: 936/1875 train_loss0.2954790420177528\n",
      "STEP: Train Epoch: 1/1 Iteration: 937/1875 train_loss0.29549120017501307\n",
      "STEP: Train Epoch: 1/1 Iteration: 938/1875 train_loss0.2955370784552494\n",
      "STEP: Train Epoch: 1/1 Iteration: 939/1875 train_loss0.295500630887743\n",
      "STEP: Train Epoch: 1/1 Iteration: 940/1875 train_loss0.2955098498929688\n",
      "STEP: Train Epoch: 1/1 Iteration: 941/1875 train_loss0.2954702235987919\n",
      "STEP: Train Epoch: 1/1 Iteration: 942/1875 train_loss0.29536398652853985\n",
      "STEP: Train Epoch: 1/1 Iteration: 943/1875 train_loss0.2954963359622283\n",
      "STEP: Train Epoch: 1/1 Iteration: 944/1875 train_loss0.29538562413210334\n",
      "STEP: Train Epoch: 1/1 Iteration: 945/1875 train_loss0.2952255895607686\n",
      "STEP: Train Epoch: 1/1 Iteration: 946/1875 train_loss0.2951319273836895\n",
      "STEP: Train Epoch: 1/1 Iteration: 947/1875 train_loss0.295075262557428\n",
      "STEP: Train Epoch: 1/1 Iteration: 948/1875 train_loss0.2949247519693923\n",
      "STEP: Train Epoch: 1/1 Iteration: 949/1875 train_loss0.2947430064306998\n",
      "STEP: Train Epoch: 1/1 Iteration: 950/1875 train_loss0.294668005587239\n",
      "STEP: Train Epoch: 1/1 Iteration: 951/1875 train_loss0.2945575737693082\n",
      "STEP: Train Epoch: 1/1 Iteration: 952/1875 train_loss0.29451584391014163\n",
      "STEP: Train Epoch: 1/1 Iteration: 953/1875 train_loss0.29456437979546324\n",
      "STEP: Train Epoch: 1/1 Iteration: 954/1875 train_loss0.29446813599274846\n",
      "STEP: Train Epoch: 1/1 Iteration: 955/1875 train_loss0.2943677316083334\n",
      "STEP: Train Epoch: 1/1 Iteration: 956/1875 train_loss0.2943072086706695\n",
      "STEP: Train Epoch: 1/1 Iteration: 957/1875 train_loss0.29434371653987074\n",
      "STEP: Train Epoch: 1/1 Iteration: 958/1875 train_loss0.29510270353865525\n",
      "STEP: Train Epoch: 1/1 Iteration: 959/1875 train_loss0.2949560282983049\n",
      "STEP: Train Epoch: 1/1 Iteration: 960/1875 train_loss0.2947493498989691\n",
      "STEP: Train Epoch: 1/1 Iteration: 961/1875 train_loss0.2946875917489769\n",
      "STEP: Train Epoch: 1/1 Iteration: 962/1875 train_loss0.2946242941485373\n",
      "STEP: Train Epoch: 1/1 Iteration: 963/1875 train_loss0.2946748811972104\n",
      "STEP: Train Epoch: 1/1 Iteration: 964/1875 train_loss0.2947852306562588\n",
      "STEP: Train Epoch: 1/1 Iteration: 965/1875 train_loss0.2947634470431916\n",
      "STEP: Train Epoch: 1/1 Iteration: 966/1875 train_loss0.2947663244469319\n",
      "STEP: Train Epoch: 1/1 Iteration: 967/1875 train_loss0.2946417980134919\n",
      "STEP: Train Epoch: 1/1 Iteration: 968/1875 train_loss0.294548154154346\n",
      "STEP: Train Epoch: 1/1 Iteration: 969/1875 train_loss0.2943839066045818\n",
      "STEP: Train Epoch: 1/1 Iteration: 970/1875 train_loss0.2942541478067329\n",
      "STEP: Train Epoch: 1/1 Iteration: 971/1875 train_loss0.29411838681702757\n",
      "STEP: Train Epoch: 1/1 Iteration: 972/1875 train_loss0.2941538069872454\n",
      "STEP: Train Epoch: 1/1 Iteration: 973/1875 train_loss0.29436541530174437\n",
      "STEP: Train Epoch: 1/1 Iteration: 974/1875 train_loss0.2943650992468642\n",
      "STEP: Train Epoch: 1/1 Iteration: 975/1875 train_loss0.29428633861052683\n",
      "STEP: Train Epoch: 1/1 Iteration: 976/1875 train_loss0.29422064540816134\n",
      "STEP: Train Epoch: 1/1 Iteration: 977/1875 train_loss0.29415140995715755\n",
      "STEP: Train Epoch: 1/1 Iteration: 978/1875 train_loss0.2940944889236569\n",
      "STEP: Train Epoch: 1/1 Iteration: 979/1875 train_loss0.29424402490638735\n",
      "STEP: Train Epoch: 1/1 Iteration: 980/1875 train_loss0.29420992085824205\n",
      "STEP: Train Epoch: 1/1 Iteration: 981/1875 train_loss0.29412054444428737\n",
      "STEP: Train Epoch: 1/1 Iteration: 982/1875 train_loss0.2943995671456797\n",
      "STEP: Train Epoch: 1/1 Iteration: 983/1875 train_loss0.2943067529891539\n",
      "STEP: Train Epoch: 1/1 Iteration: 984/1875 train_loss0.29447095397466083\n",
      "STEP: Train Epoch: 1/1 Iteration: 985/1875 train_loss0.2942449284991637\n",
      "STEP: Train Epoch: 1/1 Iteration: 986/1875 train_loss0.29413832131675\n",
      "STEP: Train Epoch: 1/1 Iteration: 987/1875 train_loss0.2941566377485656\n",
      "STEP: Train Epoch: 1/1 Iteration: 988/1875 train_loss0.2940898574888706\n",
      "STEP: Train Epoch: 1/1 Iteration: 989/1875 train_loss0.29399403342101404\n",
      "STEP: Train Epoch: 1/1 Iteration: 990/1875 train_loss0.2939016073490634\n",
      "STEP: Train Epoch: 1/1 Iteration: 991/1875 train_loss0.2938861596566276\n",
      "STEP: Train Epoch: 1/1 Iteration: 992/1875 train_loss0.29399397661308607\n",
      "STEP: Train Epoch: 1/1 Iteration: 993/1875 train_loss0.29421676722358003\n",
      "STEP: Train Epoch: 1/1 Iteration: 994/1875 train_loss0.29433179898581036\n",
      "STEP: Train Epoch: 1/1 Iteration: 995/1875 train_loss0.2944348756841679\n",
      "STEP: Train Epoch: 1/1 Iteration: 996/1875 train_loss0.29436547431840476\n",
      "STEP: Train Epoch: 1/1 Iteration: 997/1875 train_loss0.29442825799839667\n",
      "STEP: Train Epoch: 1/1 Iteration: 998/1875 train_loss0.29440858082207505\n",
      "STEP: Train Epoch: 1/1 Iteration: 999/1875 train_loss0.29450899344664794\n",
      "STEP: Train Epoch: 1/1 Iteration: 1000/1875 train_loss0.29440001456439496\n",
      "STEP: Train Epoch: 1/1 Iteration: 1001/1875 train_loss0.2943386256277978\n",
      "STEP: Train Epoch: 1/1 Iteration: 1002/1875 train_loss0.2941838390753417\n",
      "STEP: Train Epoch: 1/1 Iteration: 1003/1875 train_loss0.2941454707506528\n",
      "STEP: Train Epoch: 1/1 Iteration: 1004/1875 train_loss0.29421694178980184\n",
      "STEP: Train Epoch: 1/1 Iteration: 1005/1875 train_loss0.2942591827603715\n",
      "STEP: Train Epoch: 1/1 Iteration: 1006/1875 train_loss0.2945018694782352\n",
      "STEP: Train Epoch: 1/1 Iteration: 1007/1875 train_loss0.29455900804880475\n",
      "STEP: Train Epoch: 1/1 Iteration: 1008/1875 train_loss0.2944258933679925\n",
      "STEP: Train Epoch: 1/1 Iteration: 1009/1875 train_loss0.29437509861408306\n",
      "STEP: Train Epoch: 1/1 Iteration: 1010/1875 train_loss0.294396836273741\n",
      "STEP: Train Epoch: 1/1 Iteration: 1011/1875 train_loss0.29437974817557105\n",
      "STEP: Train Epoch: 1/1 Iteration: 1012/1875 train_loss0.2944906274438375\n",
      "STEP: Train Epoch: 1/1 Iteration: 1013/1875 train_loss0.2945502094137469\n",
      "STEP: Train Epoch: 1/1 Iteration: 1014/1875 train_loss0.29437466718153604\n",
      "STEP: Train Epoch: 1/1 Iteration: 1015/1875 train_loss0.294224867033841\n",
      "STEP: Train Epoch: 1/1 Iteration: 1016/1875 train_loss0.2942342242504668\n",
      "STEP: Train Epoch: 1/1 Iteration: 1017/1875 train_loss0.2941921382058218\n",
      "STEP: Train Epoch: 1/1 Iteration: 1018/1875 train_loss0.2943722952618346\n",
      "STEP: Train Epoch: 1/1 Iteration: 1019/1875 train_loss0.2943193662593596\n",
      "STEP: Train Epoch: 1/1 Iteration: 1020/1875 train_loss0.29473989528476024\n",
      "STEP: Train Epoch: 1/1 Iteration: 1021/1875 train_loss0.2948474674307751\n",
      "STEP: Train Epoch: 1/1 Iteration: 1022/1875 train_loss0.29467303679851403\n",
      "STEP: Train Epoch: 1/1 Iteration: 1023/1875 train_loss0.29456745211184315\n",
      "STEP: Train Epoch: 1/1 Iteration: 1024/1875 train_loss0.294529929109558\n",
      "STEP: Train Epoch: 1/1 Iteration: 1025/1875 train_loss0.294397818206287\n",
      "STEP: Train Epoch: 1/1 Iteration: 1026/1875 train_loss0.29439725461788113\n",
      "STEP: Train Epoch: 1/1 Iteration: 1027/1875 train_loss0.294400268550799\n",
      "STEP: Train Epoch: 1/1 Iteration: 1028/1875 train_loss0.29429280180266626\n",
      "STEP: Train Epoch: 1/1 Iteration: 1029/1875 train_loss0.2942213116555798\n",
      "STEP: Train Epoch: 1/1 Iteration: 1030/1875 train_loss0.29414503599688846\n",
      "STEP: Train Epoch: 1/1 Iteration: 1031/1875 train_loss0.2942304204134081\n",
      "STEP: Train Epoch: 1/1 Iteration: 1032/1875 train_loss0.29422033733377856\n",
      "STEP: Train Epoch: 1/1 Iteration: 1033/1875 train_loss0.2941678197764127\n",
      "STEP: Train Epoch: 1/1 Iteration: 1034/1875 train_loss0.29408377458699897\n",
      "STEP: Train Epoch: 1/1 Iteration: 1035/1875 train_loss0.29405374735017903\n",
      "STEP: Train Epoch: 1/1 Iteration: 1036/1875 train_loss0.2940457693470146\n",
      "STEP: Train Epoch: 1/1 Iteration: 1037/1875 train_loss0.29405312903275715\n",
      "STEP: Train Epoch: 1/1 Iteration: 1038/1875 train_loss0.29398157867479185\n",
      "STEP: Train Epoch: 1/1 Iteration: 1039/1875 train_loss0.29425106817162644\n",
      "STEP: Train Epoch: 1/1 Iteration: 1040/1875 train_loss0.2943440457017949\n",
      "STEP: Train Epoch: 1/1 Iteration: 1041/1875 train_loss0.29439816705517535\n",
      "STEP: Train Epoch: 1/1 Iteration: 1042/1875 train_loss0.2943730386842807\n",
      "STEP: Train Epoch: 1/1 Iteration: 1043/1875 train_loss0.29427281885710566\n",
      "STEP: Train Epoch: 1/1 Iteration: 1044/1875 train_loss0.2941488601567074\n",
      "STEP: Train Epoch: 1/1 Iteration: 1045/1875 train_loss0.2940490062370825\n",
      "STEP: Train Epoch: 1/1 Iteration: 1046/1875 train_loss0.2939186504263837\n",
      "STEP: Train Epoch: 1/1 Iteration: 1047/1875 train_loss0.29384581859240444\n",
      "STEP: Train Epoch: 1/1 Iteration: 1048/1875 train_loss0.29377300177793236\n",
      "STEP: Train Epoch: 1/1 Iteration: 1049/1875 train_loss0.2937941905697035\n",
      "STEP: Train Epoch: 1/1 Iteration: 1050/1875 train_loss0.29378400080970357\n",
      "STEP: Train Epoch: 1/1 Iteration: 1051/1875 train_loss0.29382635336138885\n",
      "STEP: Train Epoch: 1/1 Iteration: 1052/1875 train_loss0.29363012415827905\n",
      "STEP: Train Epoch: 1/1 Iteration: 1053/1875 train_loss0.29371064193794416\n",
      "STEP: Train Epoch: 1/1 Iteration: 1054/1875 train_loss0.29381725542685566\n",
      "STEP: Train Epoch: 1/1 Iteration: 1055/1875 train_loss0.2937536724935776\n",
      "STEP: Train Epoch: 1/1 Iteration: 1056/1875 train_loss0.29394337125686987\n",
      "STEP: Train Epoch: 1/1 Iteration: 1057/1875 train_loss0.29395013035415135\n",
      "STEP: Train Epoch: 1/1 Iteration: 1058/1875 train_loss0.2938590276077925\n",
      "STEP: Train Epoch: 1/1 Iteration: 1059/1875 train_loss0.29380944110910434\n",
      "STEP: Train Epoch: 1/1 Iteration: 1060/1875 train_loss0.29371613590222484\n",
      "STEP: Train Epoch: 1/1 Iteration: 1061/1875 train_loss0.2936350303113067\n",
      "STEP: Train Epoch: 1/1 Iteration: 1062/1875 train_loss0.29345412083012234\n",
      "STEP: Train Epoch: 1/1 Iteration: 1063/1875 train_loss0.2933615415180952\n",
      "STEP: Train Epoch: 1/1 Iteration: 1064/1875 train_loss0.2932353659525962\n",
      "STEP: Train Epoch: 1/1 Iteration: 1065/1875 train_loss0.2932179499749846\n",
      "STEP: Train Epoch: 1/1 Iteration: 1066/1875 train_loss0.2931078860490787\n",
      "STEP: Train Epoch: 1/1 Iteration: 1067/1875 train_loss0.29325864363493154\n",
      "STEP: Train Epoch: 1/1 Iteration: 1068/1875 train_loss0.2931001680215981\n",
      "STEP: Train Epoch: 1/1 Iteration: 1069/1875 train_loss0.29314681479900767\n",
      "STEP: Train Epoch: 1/1 Iteration: 1070/1875 train_loss0.29315658105171727\n",
      "STEP: Train Epoch: 1/1 Iteration: 1071/1875 train_loss0.2932236319760871\n",
      "STEP: Train Epoch: 1/1 Iteration: 1072/1875 train_loss0.29321017021090906\n",
      "STEP: Train Epoch: 1/1 Iteration: 1073/1875 train_loss0.29332959937332725\n",
      "STEP: Train Epoch: 1/1 Iteration: 1074/1875 train_loss0.2934307677660907\n",
      "STEP: Train Epoch: 1/1 Iteration: 1075/1875 train_loss0.2934093382122905\n",
      "STEP: Train Epoch: 1/1 Iteration: 1076/1875 train_loss0.2932603674902681\n",
      "STEP: Train Epoch: 1/1 Iteration: 1077/1875 train_loss0.2932852355481412\n",
      "STEP: Train Epoch: 1/1 Iteration: 1078/1875 train_loss0.29328907077271554\n",
      "STEP: Train Epoch: 1/1 Iteration: 1079/1875 train_loss0.2932873145298233\n",
      "STEP: Train Epoch: 1/1 Iteration: 1080/1875 train_loss0.29313210142017515\n",
      "STEP: Train Epoch: 1/1 Iteration: 1081/1875 train_loss0.29320846188548094\n",
      "STEP: Train Epoch: 1/1 Iteration: 1082/1875 train_loss0.29327937385649205\n",
      "STEP: Train Epoch: 1/1 Iteration: 1083/1875 train_loss0.29318009669843476\n",
      "STEP: Train Epoch: 1/1 Iteration: 1084/1875 train_loss0.2931220810722931\n",
      "STEP: Train Epoch: 1/1 Iteration: 1085/1875 train_loss0.29294807649290505\n",
      "STEP: Train Epoch: 1/1 Iteration: 1086/1875 train_loss0.2928840055252197\n",
      "STEP: Train Epoch: 1/1 Iteration: 1087/1875 train_loss0.2930012946226998\n",
      "STEP: Train Epoch: 1/1 Iteration: 1088/1875 train_loss0.292890741934945\n",
      "STEP: Train Epoch: 1/1 Iteration: 1089/1875 train_loss0.29288835456138146\n",
      "STEP: Train Epoch: 1/1 Iteration: 1090/1875 train_loss0.2928411463736941\n",
      "STEP: Train Epoch: 1/1 Iteration: 1091/1875 train_loss0.29278184882579433\n",
      "STEP: Train Epoch: 1/1 Iteration: 1092/1875 train_loss0.2926998502701084\n",
      "STEP: Train Epoch: 1/1 Iteration: 1093/1875 train_loss0.29276360233961063\n",
      "STEP: Train Epoch: 1/1 Iteration: 1094/1875 train_loss0.2925612542172128\n",
      "STEP: Train Epoch: 1/1 Iteration: 1095/1875 train_loss0.2926081691498626\n",
      "STEP: Train Epoch: 1/1 Iteration: 1096/1875 train_loss0.2926437231868397\n",
      "STEP: Train Epoch: 1/1 Iteration: 1097/1875 train_loss0.2925526639998448\n",
      "STEP: Train Epoch: 1/1 Iteration: 1098/1875 train_loss0.29238566809854655\n",
      "STEP: Train Epoch: 1/1 Iteration: 1099/1875 train_loss0.29230139407247063\n",
      "STEP: Train Epoch: 1/1 Iteration: 1100/1875 train_loss0.29231644897975706\n",
      "STEP: Train Epoch: 1/1 Iteration: 1101/1875 train_loss0.29230340242683617\n",
      "STEP: Train Epoch: 1/1 Iteration: 1102/1875 train_loss0.29218122392162654\n",
      "STEP: Train Epoch: 1/1 Iteration: 1103/1875 train_loss0.29216832448732605\n",
      "STEP: Train Epoch: 1/1 Iteration: 1104/1875 train_loss0.29207882516599004\n",
      "STEP: Train Epoch: 1/1 Iteration: 1105/1875 train_loss0.29205858329167733\n",
      "STEP: Train Epoch: 1/1 Iteration: 1106/1875 train_loss0.2921976417127936\n",
      "STEP: Train Epoch: 1/1 Iteration: 1107/1875 train_loss0.29223271503761855\n",
      "STEP: Train Epoch: 1/1 Iteration: 1108/1875 train_loss0.2921045595437073\n",
      "STEP: Train Epoch: 1/1 Iteration: 1109/1875 train_loss0.29233975214690533\n",
      "STEP: Train Epoch: 1/1 Iteration: 1110/1875 train_loss0.2921479581309868\n",
      "STEP: Train Epoch: 1/1 Iteration: 1111/1875 train_loss0.2921702275530483\n",
      "STEP: Train Epoch: 1/1 Iteration: 1112/1875 train_loss0.29201545461797884\n",
      "STEP: Train Epoch: 1/1 Iteration: 1113/1875 train_loss0.2920720977817477\n",
      "STEP: Train Epoch: 1/1 Iteration: 1114/1875 train_loss0.2920414308617744\n",
      "STEP: Train Epoch: 1/1 Iteration: 1115/1875 train_loss0.29198103417462834\n",
      "STEP: Train Epoch: 1/1 Iteration: 1116/1875 train_loss0.2919141787060914\n",
      "STEP: Train Epoch: 1/1 Iteration: 1117/1875 train_loss0.29189289441947547\n",
      "STEP: Train Epoch: 1/1 Iteration: 1118/1875 train_loss0.29195675569751395\n",
      "STEP: Train Epoch: 1/1 Iteration: 1119/1875 train_loss0.29189109386771356\n",
      "STEP: Train Epoch: 1/1 Iteration: 1120/1875 train_loss0.29184100039835487\n",
      "STEP: Train Epoch: 1/1 Iteration: 1121/1875 train_loss0.2917465510828808\n",
      "STEP: Train Epoch: 1/1 Iteration: 1122/1875 train_loss0.2917724063556887\n",
      "STEP: Train Epoch: 1/1 Iteration: 1123/1875 train_loss0.29204434489471404\n",
      "STEP: Train Epoch: 1/1 Iteration: 1124/1875 train_loss0.29195505625889817\n",
      "STEP: Train Epoch: 1/1 Iteration: 1125/1875 train_loss0.2921944983402888\n",
      "STEP: Train Epoch: 1/1 Iteration: 1126/1875 train_loss0.29205813438864836\n",
      "STEP: Train Epoch: 1/1 Iteration: 1127/1875 train_loss0.2921838942178076\n",
      "STEP: Train Epoch: 1/1 Iteration: 1128/1875 train_loss0.2923440875762955\n",
      "STEP: Train Epoch: 1/1 Iteration: 1129/1875 train_loss0.29229902941321145\n",
      "STEP: Train Epoch: 1/1 Iteration: 1130/1875 train_loss0.2922578796876215\n",
      "STEP: Train Epoch: 1/1 Iteration: 1131/1875 train_loss0.2922307367771732\n",
      "STEP: Train Epoch: 1/1 Iteration: 1132/1875 train_loss0.292147837223834\n",
      "STEP: Train Epoch: 1/1 Iteration: 1133/1875 train_loss0.2920553555871571\n",
      "STEP: Train Epoch: 1/1 Iteration: 1134/1875 train_loss0.29195746111911863\n",
      "STEP: Train Epoch: 1/1 Iteration: 1135/1875 train_loss0.2923175125395149\n",
      "STEP: Train Epoch: 1/1 Iteration: 1136/1875 train_loss0.2922084282585223\n",
      "STEP: Train Epoch: 1/1 Iteration: 1137/1875 train_loss0.2923306968380929\n",
      "STEP: Train Epoch: 1/1 Iteration: 1138/1875 train_loss0.2922973215763305\n",
      "STEP: Train Epoch: 1/1 Iteration: 1139/1875 train_loss0.29223522277020697\n",
      "STEP: Train Epoch: 1/1 Iteration: 1140/1875 train_loss0.2922201113481271\n",
      "STEP: Train Epoch: 1/1 Iteration: 1141/1875 train_loss0.2920396157412232\n",
      "STEP: Train Epoch: 1/1 Iteration: 1142/1875 train_loss0.29199468118714367\n",
      "STEP: Train Epoch: 1/1 Iteration: 1143/1875 train_loss0.29198762711890414\n",
      "STEP: Train Epoch: 1/1 Iteration: 1144/1875 train_loss0.2920456065477191\n",
      "STEP: Train Epoch: 1/1 Iteration: 1145/1875 train_loss0.29192011378477756\n",
      "STEP: Train Epoch: 1/1 Iteration: 1146/1875 train_loss0.2918863275314293\n",
      "STEP: Train Epoch: 1/1 Iteration: 1147/1875 train_loss0.2920122314325705\n",
      "STEP: Train Epoch: 1/1 Iteration: 1148/1875 train_loss0.2920613098720848\n",
      "STEP: Train Epoch: 1/1 Iteration: 1149/1875 train_loss0.2919197399477216\n",
      "STEP: Train Epoch: 1/1 Iteration: 1150/1875 train_loss0.2919353364084078\n",
      "STEP: Train Epoch: 1/1 Iteration: 1151/1875 train_loss0.2922607627410044\n",
      "STEP: Train Epoch: 1/1 Iteration: 1152/1875 train_loss0.29238928269801867\n",
      "STEP: Train Epoch: 1/1 Iteration: 1153/1875 train_loss0.2923224828523444\n",
      "STEP: Train Epoch: 1/1 Iteration: 1154/1875 train_loss0.29233279959839387\n",
      "STEP: Train Epoch: 1/1 Iteration: 1155/1875 train_loss0.29237316489477694\n",
      "STEP: Train Epoch: 1/1 Iteration: 1156/1875 train_loss0.2924654678051653\n",
      "STEP: Train Epoch: 1/1 Iteration: 1157/1875 train_loss0.2923639144816065\n",
      "STEP: Train Epoch: 1/1 Iteration: 1158/1875 train_loss0.2924025980996962\n",
      "STEP: Train Epoch: 1/1 Iteration: 1159/1875 train_loss0.29221318090034004\n",
      "STEP: Train Epoch: 1/1 Iteration: 1160/1875 train_loss0.29216115045650254\n",
      "STEP: Train Epoch: 1/1 Iteration: 1161/1875 train_loss0.29211937430139046\n",
      "STEP: Train Epoch: 1/1 Iteration: 1162/1875 train_loss0.2922626741089427\n",
      "STEP: Train Epoch: 1/1 Iteration: 1163/1875 train_loss0.2921578043996519\n",
      "STEP: Train Epoch: 1/1 Iteration: 1164/1875 train_loss0.29218889385000946\n",
      "STEP: Train Epoch: 1/1 Iteration: 1165/1875 train_loss0.2921071908120945\n",
      "STEP: Train Epoch: 1/1 Iteration: 1166/1875 train_loss0.2922219122994825\n",
      "STEP: Train Epoch: 1/1 Iteration: 1167/1875 train_loss0.29220969663436536\n",
      "STEP: Train Epoch: 1/1 Iteration: 1168/1875 train_loss0.29232488211550534\n",
      "STEP: Train Epoch: 1/1 Iteration: 1169/1875 train_loss0.2923745052670698\n",
      "STEP: Train Epoch: 1/1 Iteration: 1170/1875 train_loss0.29231257897156937\n",
      "STEP: Train Epoch: 1/1 Iteration: 1171/1875 train_loss0.29224937227829006\n",
      "STEP: Train Epoch: 1/1 Iteration: 1172/1875 train_loss0.2924500303565439\n",
      "STEP: Train Epoch: 1/1 Iteration: 1173/1875 train_loss0.29245358555619777\n",
      "STEP: Train Epoch: 1/1 Iteration: 1174/1875 train_loss0.2924893503751625\n",
      "STEP: Train Epoch: 1/1 Iteration: 1175/1875 train_loss0.2925699029577539\n",
      "STEP: Train Epoch: 1/1 Iteration: 1176/1875 train_loss0.292559273388921\n",
      "STEP: Train Epoch: 1/1 Iteration: 1177/1875 train_loss0.2929545675381423\n",
      "STEP: Train Epoch: 1/1 Iteration: 1178/1875 train_loss0.2932217338190419\n",
      "STEP: Train Epoch: 1/1 Iteration: 1179/1875 train_loss0.2933896000704794\n",
      "STEP: Train Epoch: 1/1 Iteration: 1180/1875 train_loss0.29335966630507326\n",
      "STEP: Train Epoch: 1/1 Iteration: 1181/1875 train_loss0.29349934386056326\n",
      "STEP: Train Epoch: 1/1 Iteration: 1182/1875 train_loss0.2937161672589339\n",
      "STEP: Train Epoch: 1/1 Iteration: 1183/1875 train_loss0.2935600974112875\n",
      "STEP: Train Epoch: 1/1 Iteration: 1184/1875 train_loss0.2937130165173093\n",
      "STEP: Train Epoch: 1/1 Iteration: 1185/1875 train_loss0.2938895369192216\n",
      "STEP: Train Epoch: 1/1 Iteration: 1186/1875 train_loss0.29392939030144105\n",
      "STEP: Train Epoch: 1/1 Iteration: 1187/1875 train_loss0.29408209552484693\n",
      "STEP: Train Epoch: 1/1 Iteration: 1188/1875 train_loss0.293961989527148\n",
      "STEP: Train Epoch: 1/1 Iteration: 1189/1875 train_loss0.2938477447909703\n",
      "STEP: Train Epoch: 1/1 Iteration: 1190/1875 train_loss0.2938845933047162\n",
      "STEP: Train Epoch: 1/1 Iteration: 1191/1875 train_loss0.2938349773023531\n",
      "STEP: Train Epoch: 1/1 Iteration: 1192/1875 train_loss0.2937078971303249\n",
      "STEP: Train Epoch: 1/1 Iteration: 1193/1875 train_loss0.29364100979814384\n",
      "STEP: Train Epoch: 1/1 Iteration: 1194/1875 train_loss0.2936009065976594\n",
      "STEP: Train Epoch: 1/1 Iteration: 1195/1875 train_loss0.29356187178624726\n",
      "STEP: Train Epoch: 1/1 Iteration: 1196/1875 train_loss0.2936561436619747\n",
      "STEP: Train Epoch: 1/1 Iteration: 1197/1875 train_loss0.2938123200035832\n",
      "STEP: Train Epoch: 1/1 Iteration: 1198/1875 train_loss0.29375024235034625\n",
      "STEP: Train Epoch: 1/1 Iteration: 1199/1875 train_loss0.29377259693735336\n",
      "STEP: Train Epoch: 1/1 Iteration: 1200/1875 train_loss0.29376265658065676\n",
      "STEP: Train Epoch: 1/1 Iteration: 1201/1875 train_loss0.2937204119230587\n",
      "STEP: Train Epoch: 1/1 Iteration: 1202/1875 train_loss0.2937846063836145\n",
      "STEP: Train Epoch: 1/1 Iteration: 1203/1875 train_loss0.29360557887594796\n",
      "STEP: Train Epoch: 1/1 Iteration: 1204/1875 train_loss0.2935745795736974\n",
      "STEP: Train Epoch: 1/1 Iteration: 1205/1875 train_loss0.29349281165238733\n",
      "STEP: Train Epoch: 1/1 Iteration: 1206/1875 train_loss0.29365418286095213\n",
      "STEP: Train Epoch: 1/1 Iteration: 1207/1875 train_loss0.29367791424260814\n",
      "STEP: Train Epoch: 1/1 Iteration: 1208/1875 train_loss0.29365276925833217\n",
      "STEP: Train Epoch: 1/1 Iteration: 1209/1875 train_loss0.2937611075313847\n",
      "STEP: Train Epoch: 1/1 Iteration: 1210/1875 train_loss0.29369487021703367\n",
      "STEP: Train Epoch: 1/1 Iteration: 1211/1875 train_loss0.293696929369211\n",
      "STEP: Train Epoch: 1/1 Iteration: 1212/1875 train_loss0.29370655304454146\n",
      "STEP: Train Epoch: 1/1 Iteration: 1213/1875 train_loss0.29392321866622956\n",
      "STEP: Train Epoch: 1/1 Iteration: 1214/1875 train_loss0.29404820198012144\n",
      "STEP: Train Epoch: 1/1 Iteration: 1215/1875 train_loss0.2940086955702845\n",
      "STEP: Train Epoch: 1/1 Iteration: 1216/1875 train_loss0.2939526731283159\n",
      "STEP: Train Epoch: 1/1 Iteration: 1217/1875 train_loss0.2939352450146965\n",
      "STEP: Train Epoch: 1/1 Iteration: 1218/1875 train_loss0.29388169663513236\n",
      "STEP: Train Epoch: 1/1 Iteration: 1219/1875 train_loss0.29407929509291403\n",
      "STEP: Train Epoch: 1/1 Iteration: 1220/1875 train_loss0.2939616589700101\n",
      "STEP: Train Epoch: 1/1 Iteration: 1221/1875 train_loss0.29394814727811125\n",
      "STEP: Train Epoch: 1/1 Iteration: 1222/1875 train_loss0.29405394052010514\n",
      "STEP: Train Epoch: 1/1 Iteration: 1223/1875 train_loss0.29443955067499483\n",
      "STEP: Train Epoch: 1/1 Iteration: 1224/1875 train_loss0.29450647853728795\n",
      "STEP: Train Epoch: 1/1 Iteration: 1225/1875 train_loss0.2943636624423825\n",
      "STEP: Train Epoch: 1/1 Iteration: 1226/1875 train_loss0.29427464087577276\n",
      "STEP: Train Epoch: 1/1 Iteration: 1227/1875 train_loss0.29419830805806485\n",
      "STEP: Train Epoch: 1/1 Iteration: 1228/1875 train_loss0.294186975843355\n",
      "STEP: Train Epoch: 1/1 Iteration: 1229/1875 train_loss0.2941385399916969\n",
      "STEP: Train Epoch: 1/1 Iteration: 1230/1875 train_loss0.2941479363819448\n",
      "STEP: Train Epoch: 1/1 Iteration: 1231/1875 train_loss0.2942912973445378\n",
      "STEP: Train Epoch: 1/1 Iteration: 1232/1875 train_loss0.29433098445077993\n",
      "STEP: Train Epoch: 1/1 Iteration: 1233/1875 train_loss0.2944388568449678\n",
      "STEP: Train Epoch: 1/1 Iteration: 1234/1875 train_loss0.2944015818172568\n",
      "STEP: Train Epoch: 1/1 Iteration: 1235/1875 train_loss0.29434001473521415\n",
      "STEP: Train Epoch: 1/1 Iteration: 1236/1875 train_loss0.2943889439274771\n",
      "STEP: Train Epoch: 1/1 Iteration: 1237/1875 train_loss0.29433361661944935\n",
      "STEP: Train Epoch: 1/1 Iteration: 1238/1875 train_loss0.29424235482304467\n",
      "STEP: Train Epoch: 1/1 Iteration: 1239/1875 train_loss0.2943643665631227\n",
      "STEP: Train Epoch: 1/1 Iteration: 1240/1875 train_loss0.2942294430708693\n",
      "STEP: Train Epoch: 1/1 Iteration: 1241/1875 train_loss0.29411122609923485\n",
      "STEP: Train Epoch: 1/1 Iteration: 1242/1875 train_loss0.2941002462917096\n",
      "STEP: Train Epoch: 1/1 Iteration: 1243/1875 train_loss0.2940732220417055\n",
      "STEP: Train Epoch: 1/1 Iteration: 1244/1875 train_loss0.2940222181236054\n",
      "STEP: Train Epoch: 1/1 Iteration: 1245/1875 train_loss0.2939627804669989\n",
      "STEP: Train Epoch: 1/1 Iteration: 1246/1875 train_loss0.2938950252308126\n",
      "STEP: Train Epoch: 1/1 Iteration: 1247/1875 train_loss0.29384953868609004\n",
      "STEP: Train Epoch: 1/1 Iteration: 1248/1875 train_loss0.29381834483968144\n",
      "STEP: Train Epoch: 1/1 Iteration: 1249/1875 train_loss0.29383074328172104\n",
      "STEP: Train Epoch: 1/1 Iteration: 1250/1875 train_loss0.2939017698764801\n",
      "STEP: Train Epoch: 1/1 Iteration: 1251/1875 train_loss0.2938618551341178\n",
      "STEP: Train Epoch: 1/1 Iteration: 1252/1875 train_loss0.29384620961385033\n",
      "STEP: Train Epoch: 1/1 Iteration: 1253/1875 train_loss0.2938780894254744\n",
      "STEP: Train Epoch: 1/1 Iteration: 1254/1875 train_loss0.2938369680582241\n",
      "STEP: Train Epoch: 1/1 Iteration: 1255/1875 train_loss0.2937564882624197\n",
      "STEP: Train Epoch: 1/1 Iteration: 1256/1875 train_loss0.2936662241792793\n",
      "STEP: Train Epoch: 1/1 Iteration: 1257/1875 train_loss0.29369246738006316\n",
      "STEP: Train Epoch: 1/1 Iteration: 1258/1875 train_loss0.2935859760675218\n",
      "STEP: Train Epoch: 1/1 Iteration: 1259/1875 train_loss0.29356344644279114\n",
      "STEP: Train Epoch: 1/1 Iteration: 1260/1875 train_loss0.2936591341145455\n",
      "STEP: Train Epoch: 1/1 Iteration: 1261/1875 train_loss0.2936246758836494\n",
      "STEP: Train Epoch: 1/1 Iteration: 1262/1875 train_loss0.2936237170796387\n",
      "STEP: Train Epoch: 1/1 Iteration: 1263/1875 train_loss0.29352468232125023\n",
      "STEP: Train Epoch: 1/1 Iteration: 1264/1875 train_loss0.29379478788970015\n",
      "STEP: Train Epoch: 1/1 Iteration: 1265/1875 train_loss0.2936969615489598\n",
      "STEP: Train Epoch: 1/1 Iteration: 1266/1875 train_loss0.29368553090528665\n",
      "STEP: Train Epoch: 1/1 Iteration: 1267/1875 train_loss0.2935957931799501\n",
      "STEP: Train Epoch: 1/1 Iteration: 1268/1875 train_loss0.29361420588961534\n",
      "STEP: Train Epoch: 1/1 Iteration: 1269/1875 train_loss0.29351105844533004\n",
      "STEP: Train Epoch: 1/1 Iteration: 1270/1875 train_loss0.2934453286171898\n",
      "STEP: Train Epoch: 1/1 Iteration: 1271/1875 train_loss0.293484622983366\n",
      "STEP: Train Epoch: 1/1 Iteration: 1272/1875 train_loss0.29334672685689145\n",
      "STEP: Train Epoch: 1/1 Iteration: 1273/1875 train_loss0.2932723995418129\n",
      "STEP: Train Epoch: 1/1 Iteration: 1274/1875 train_loss0.2932224506556146\n",
      "STEP: Train Epoch: 1/1 Iteration: 1275/1875 train_loss0.293245885126731\n",
      "STEP: Train Epoch: 1/1 Iteration: 1276/1875 train_loss0.2931454684643723\n",
      "STEP: Train Epoch: 1/1 Iteration: 1277/1875 train_loss0.29299535543986754\n",
      "STEP: Train Epoch: 1/1 Iteration: 1278/1875 train_loss0.29297750998807254\n",
      "STEP: Train Epoch: 1/1 Iteration: 1279/1875 train_loss0.29302607433021394\n",
      "STEP: Train Epoch: 1/1 Iteration: 1280/1875 train_loss0.2928834349906538\n",
      "STEP: Train Epoch: 1/1 Iteration: 1281/1875 train_loss0.29267864913231084\n",
      "STEP: Train Epoch: 1/1 Iteration: 1282/1875 train_loss0.2926341788756099\n",
      "STEP: Train Epoch: 1/1 Iteration: 1283/1875 train_loss0.2927460756634862\n",
      "STEP: Train Epoch: 1/1 Iteration: 1284/1875 train_loss0.2926935243211408\n",
      "STEP: Train Epoch: 1/1 Iteration: 1285/1875 train_loss0.29273098162031125\n",
      "STEP: Train Epoch: 1/1 Iteration: 1286/1875 train_loss0.292742624344423\n",
      "STEP: Train Epoch: 1/1 Iteration: 1287/1875 train_loss0.2926917350615817\n",
      "STEP: Train Epoch: 1/1 Iteration: 1288/1875 train_loss0.2927542943724356\n",
      "STEP: Train Epoch: 1/1 Iteration: 1289/1875 train_loss0.2926651168747569\n",
      "STEP: Train Epoch: 1/1 Iteration: 1290/1875 train_loss0.2926035048180195\n",
      "STEP: Train Epoch: 1/1 Iteration: 1291/1875 train_loss0.29267286647062124\n",
      "STEP: Train Epoch: 1/1 Iteration: 1292/1875 train_loss0.29264863372719835\n",
      "STEP: Train Epoch: 1/1 Iteration: 1293/1875 train_loss0.2927165662269097\n",
      "STEP: Train Epoch: 1/1 Iteration: 1294/1875 train_loss0.2927895364966215\n",
      "STEP: Train Epoch: 1/1 Iteration: 1295/1875 train_loss0.2927270535552248\n",
      "STEP: Train Epoch: 1/1 Iteration: 1296/1875 train_loss0.2928588218328739\n",
      "STEP: Train Epoch: 1/1 Iteration: 1297/1875 train_loss0.2929820688103908\n",
      "STEP: Train Epoch: 1/1 Iteration: 1298/1875 train_loss0.2928774436269937\n",
      "STEP: Train Epoch: 1/1 Iteration: 1299/1875 train_loss0.29284229686338914\n",
      "STEP: Train Epoch: 1/1 Iteration: 1300/1875 train_loss0.29277357503628504\n",
      "STEP: Train Epoch: 1/1 Iteration: 1301/1875 train_loss0.29272557150065714\n",
      "STEP: Train Epoch: 1/1 Iteration: 1302/1875 train_loss0.29272688648158457\n",
      "STEP: Train Epoch: 1/1 Iteration: 1303/1875 train_loss0.2928058989245962\n",
      "STEP: Train Epoch: 1/1 Iteration: 1304/1875 train_loss0.2927292717343619\n",
      "STEP: Train Epoch: 1/1 Iteration: 1305/1875 train_loss0.2928556604275187\n",
      "STEP: Train Epoch: 1/1 Iteration: 1306/1875 train_loss0.29285049932377033\n",
      "STEP: Train Epoch: 1/1 Iteration: 1307/1875 train_loss0.29276003084746177\n",
      "STEP: Train Epoch: 1/1 Iteration: 1308/1875 train_loss0.2926795307862991\n",
      "STEP: Train Epoch: 1/1 Iteration: 1309/1875 train_loss0.29266137239716045\n",
      "STEP: Train Epoch: 1/1 Iteration: 1310/1875 train_loss0.2928287876912087\n",
      "STEP: Train Epoch: 1/1 Iteration: 1311/1875 train_loss0.29272248308812954\n",
      "STEP: Train Epoch: 1/1 Iteration: 1312/1875 train_loss0.29257578104478876\n",
      "STEP: Train Epoch: 1/1 Iteration: 1313/1875 train_loss0.29252014133117393\n",
      "STEP: Train Epoch: 1/1 Iteration: 1314/1875 train_loss0.2924527151941051\n",
      "STEP: Train Epoch: 1/1 Iteration: 1315/1875 train_loss0.2923333834941736\n",
      "STEP: Train Epoch: 1/1 Iteration: 1316/1875 train_loss0.2923354789498616\n",
      "STEP: Train Epoch: 1/1 Iteration: 1317/1875 train_loss0.2925364569652664\n",
      "STEP: Train Epoch: 1/1 Iteration: 1318/1875 train_loss0.29244870550461305\n",
      "STEP: Train Epoch: 1/1 Iteration: 1319/1875 train_loss0.2924932894580083\n",
      "STEP: Train Epoch: 1/1 Iteration: 1320/1875 train_loss0.2923839836113268\n",
      "STEP: Train Epoch: 1/1 Iteration: 1321/1875 train_loss0.29231753458468457\n",
      "STEP: Train Epoch: 1/1 Iteration: 1322/1875 train_loss0.2923084961436407\n",
      "STEP: Train Epoch: 1/1 Iteration: 1323/1875 train_loss0.29228071858705174\n",
      "STEP: Train Epoch: 1/1 Iteration: 1324/1875 train_loss0.29212718977370605\n",
      "STEP: Train Epoch: 1/1 Iteration: 1325/1875 train_loss0.2921202979264957\n",
      "STEP: Train Epoch: 1/1 Iteration: 1326/1875 train_loss0.29221832028942724\n",
      "STEP: Train Epoch: 1/1 Iteration: 1327/1875 train_loss0.29222011703579565\n",
      "STEP: Train Epoch: 1/1 Iteration: 1328/1875 train_loss0.29230396479160625\n",
      "STEP: Train Epoch: 1/1 Iteration: 1329/1875 train_loss0.29227502522268317\n",
      "STEP: Train Epoch: 1/1 Iteration: 1330/1875 train_loss0.2921591654349875\n",
      "STEP: Train Epoch: 1/1 Iteration: 1331/1875 train_loss0.29211911850606787\n",
      "STEP: Train Epoch: 1/1 Iteration: 1332/1875 train_loss0.29206527913179436\n",
      "STEP: Train Epoch: 1/1 Iteration: 1333/1875 train_loss0.29201631390385535\n",
      "STEP: Train Epoch: 1/1 Iteration: 1334/1875 train_loss0.2920473615444828\n",
      "STEP: Train Epoch: 1/1 Iteration: 1335/1875 train_loss0.2920594787148389\n",
      "STEP: Train Epoch: 1/1 Iteration: 1336/1875 train_loss0.29214476343818724\n",
      "STEP: Train Epoch: 1/1 Iteration: 1337/1875 train_loss0.29211184056301615\n",
      "STEP: Train Epoch: 1/1 Iteration: 1338/1875 train_loss0.2920688267933659\n",
      "STEP: Train Epoch: 1/1 Iteration: 1339/1875 train_loss0.29195420777173386\n",
      "STEP: Train Epoch: 1/1 Iteration: 1340/1875 train_loss0.2919217103716931\n",
      "STEP: Train Epoch: 1/1 Iteration: 1341/1875 train_loss0.2919648316168656\n",
      "STEP: Train Epoch: 1/1 Iteration: 1342/1875 train_loss0.2920369581429041\n",
      "STEP: Train Epoch: 1/1 Iteration: 1343/1875 train_loss0.2919922002971905\n",
      "STEP: Train Epoch: 1/1 Iteration: 1344/1875 train_loss0.29194501504008213\n",
      "STEP: Train Epoch: 1/1 Iteration: 1345/1875 train_loss0.29188499869882617\n",
      "STEP: Train Epoch: 1/1 Iteration: 1346/1875 train_loss0.29231052521760137\n",
      "STEP: Train Epoch: 1/1 Iteration: 1347/1875 train_loss0.29225842636313676\n",
      "STEP: Train Epoch: 1/1 Iteration: 1348/1875 train_loss0.2922111168788409\n",
      "STEP: Train Epoch: 1/1 Iteration: 1349/1875 train_loss0.29241349373435294\n",
      "STEP: Train Epoch: 1/1 Iteration: 1350/1875 train_loss0.29238725341442556\n",
      "STEP: Train Epoch: 1/1 Iteration: 1351/1875 train_loss0.29233536533845916\n",
      "STEP: Train Epoch: 1/1 Iteration: 1352/1875 train_loss0.2922280614895294\n",
      "STEP: Train Epoch: 1/1 Iteration: 1353/1875 train_loss0.2922244467383632\n",
      "STEP: Train Epoch: 1/1 Iteration: 1354/1875 train_loss0.29212640716035354\n",
      "STEP: Train Epoch: 1/1 Iteration: 1355/1875 train_loss0.2920498749248106\n",
      "STEP: Train Epoch: 1/1 Iteration: 1356/1875 train_loss0.2919886935224817\n",
      "STEP: Train Epoch: 1/1 Iteration: 1357/1875 train_loss0.2919163405497173\n",
      "STEP: Train Epoch: 1/1 Iteration: 1358/1875 train_loss0.2918211698726873\n",
      "STEP: Train Epoch: 1/1 Iteration: 1359/1875 train_loss0.29180080704445827\n",
      "STEP: Train Epoch: 1/1 Iteration: 1360/1875 train_loss0.29170761075939106\n",
      "STEP: Train Epoch: 1/1 Iteration: 1361/1875 train_loss0.2918880158182696\n",
      "STEP: Train Epoch: 1/1 Iteration: 1362/1875 train_loss0.291875548041241\n",
      "STEP: Train Epoch: 1/1 Iteration: 1363/1875 train_loss0.291719305080991\n",
      "STEP: Train Epoch: 1/1 Iteration: 1364/1875 train_loss0.29178488158274035\n",
      "STEP: Train Epoch: 1/1 Iteration: 1365/1875 train_loss0.29178578001975797\n",
      "STEP: Train Epoch: 1/1 Iteration: 1366/1875 train_loss0.29169887579066645\n",
      "STEP: Train Epoch: 1/1 Iteration: 1367/1875 train_loss0.2917123460023505\n",
      "STEP: Train Epoch: 1/1 Iteration: 1368/1875 train_loss0.2917074231711374\n",
      "STEP: Train Epoch: 1/1 Iteration: 1369/1875 train_loss0.29162752317976265\n",
      "STEP: Train Epoch: 1/1 Iteration: 1370/1875 train_loss0.29153044565179703\n",
      "STEP: Train Epoch: 1/1 Iteration: 1371/1875 train_loss0.29140606186577517\n",
      "STEP: Train Epoch: 1/1 Iteration: 1372/1875 train_loss0.29141190483943497\n",
      "STEP: Train Epoch: 1/1 Iteration: 1373/1875 train_loss0.29132780246979245\n",
      "STEP: Train Epoch: 1/1 Iteration: 1374/1875 train_loss0.29141478005015825\n",
      "STEP: Train Epoch: 1/1 Iteration: 1375/1875 train_loss0.29136544885283167\n",
      "STEP: Train Epoch: 1/1 Iteration: 1376/1875 train_loss0.29153904685788506\n",
      "STEP: Train Epoch: 1/1 Iteration: 1377/1875 train_loss0.2914194835629923\n",
      "STEP: Train Epoch: 1/1 Iteration: 1378/1875 train_loss0.2914086399675605\n",
      "STEP: Train Epoch: 1/1 Iteration: 1379/1875 train_loss0.2913815836861805\n",
      "STEP: Train Epoch: 1/1 Iteration: 1380/1875 train_loss0.2913628518567893\n",
      "STEP: Train Epoch: 1/1 Iteration: 1381/1875 train_loss0.2913632425024383\n",
      "STEP: Train Epoch: 1/1 Iteration: 1382/1875 train_loss0.29138135569178203\n",
      "STEP: Train Epoch: 1/1 Iteration: 1383/1875 train_loss0.2912709607701649\n",
      "STEP: Train Epoch: 1/1 Iteration: 1384/1875 train_loss0.2913019419069041\n",
      "STEP: Train Epoch: 1/1 Iteration: 1385/1875 train_loss0.2914076977997803\n",
      "STEP: Train Epoch: 1/1 Iteration: 1386/1875 train_loss0.2913348821084543\n",
      "STEP: Train Epoch: 1/1 Iteration: 1387/1875 train_loss0.29127196897635077\n",
      "STEP: Train Epoch: 1/1 Iteration: 1388/1875 train_loss0.29125915510547734\n",
      "STEP: Train Epoch: 1/1 Iteration: 1389/1875 train_loss0.29135780432177716\n",
      "STEP: Train Epoch: 1/1 Iteration: 1390/1875 train_loss0.29135863843018844\n",
      "STEP: Train Epoch: 1/1 Iteration: 1391/1875 train_loss0.29132028669520604\n",
      "STEP: Train Epoch: 1/1 Iteration: 1392/1875 train_loss0.29129961052583\n",
      "STEP: Train Epoch: 1/1 Iteration: 1393/1875 train_loss0.2914341133494366\n",
      "STEP: Train Epoch: 1/1 Iteration: 1394/1875 train_loss0.2914362217431923\n",
      "STEP: Train Epoch: 1/1 Iteration: 1395/1875 train_loss0.2914609470341261\n",
      "STEP: Train Epoch: 1/1 Iteration: 1396/1875 train_loss0.291391778309005\n",
      "STEP: Train Epoch: 1/1 Iteration: 1397/1875 train_loss0.29134275348631716\n",
      "STEP: Train Epoch: 1/1 Iteration: 1398/1875 train_loss0.2912625606892596\n",
      "STEP: Train Epoch: 1/1 Iteration: 1399/1875 train_loss0.29124752579495217\n",
      "STEP: Train Epoch: 1/1 Iteration: 1400/1875 train_loss0.2911158041216965\n",
      "STEP: Train Epoch: 1/1 Iteration: 1401/1875 train_loss0.29096069062882496\n",
      "STEP: Train Epoch: 1/1 Iteration: 1402/1875 train_loss0.29079099717854223\n",
      "STEP: Train Epoch: 1/1 Iteration: 1403/1875 train_loss0.2907501502229393\n",
      "STEP: Train Epoch: 1/1 Iteration: 1404/1875 train_loss0.290986486839295\n",
      "STEP: Train Epoch: 1/1 Iteration: 1405/1875 train_loss0.2909968927436346\n",
      "STEP: Train Epoch: 1/1 Iteration: 1406/1875 train_loss0.2910896681479654\n",
      "STEP: Train Epoch: 1/1 Iteration: 1407/1875 train_loss0.2910545458361856\n",
      "STEP: Train Epoch: 1/1 Iteration: 1408/1875 train_loss0.2910297966167987\n",
      "STEP: Train Epoch: 1/1 Iteration: 1409/1875 train_loss0.29093546720024666\n",
      "STEP: Train Epoch: 1/1 Iteration: 1410/1875 train_loss0.29084968069000233\n",
      "STEP: Train Epoch: 1/1 Iteration: 1411/1875 train_loss0.29100123359237595\n",
      "STEP: Train Epoch: 1/1 Iteration: 1412/1875 train_loss0.2910818900720404\n",
      "STEP: Train Epoch: 1/1 Iteration: 1413/1875 train_loss0.29093757030136896\n",
      "STEP: Train Epoch: 1/1 Iteration: 1414/1875 train_loss0.2908487688879123\n",
      "STEP: Train Epoch: 1/1 Iteration: 1415/1875 train_loss0.2907511879706446\n",
      "STEP: Train Epoch: 1/1 Iteration: 1416/1875 train_loss0.29084219733952926\n",
      "STEP: Train Epoch: 1/1 Iteration: 1417/1875 train_loss0.2908533772541737\n",
      "STEP: Train Epoch: 1/1 Iteration: 1418/1875 train_loss0.29079937449247906\n",
      "STEP: Train Epoch: 1/1 Iteration: 1419/1875 train_loss0.2909172276012636\n",
      "STEP: Train Epoch: 1/1 Iteration: 1420/1875 train_loss0.2909780409331368\n",
      "STEP: Train Epoch: 1/1 Iteration: 1421/1875 train_loss0.2908643920650938\n",
      "STEP: Train Epoch: 1/1 Iteration: 1422/1875 train_loss0.29090075329883985\n",
      "STEP: Train Epoch: 1/1 Iteration: 1423/1875 train_loss0.29083982723531426\n",
      "STEP: Train Epoch: 1/1 Iteration: 1424/1875 train_loss0.29084061638561987\n",
      "STEP: Train Epoch: 1/1 Iteration: 1425/1875 train_loss0.29076061792410257\n",
      "STEP: Train Epoch: 1/1 Iteration: 1426/1875 train_loss0.29091338827803753\n",
      "STEP: Train Epoch: 1/1 Iteration: 1427/1875 train_loss0.29080225596075177\n",
      "STEP: Train Epoch: 1/1 Iteration: 1428/1875 train_loss0.2907401932291269\n",
      "STEP: Train Epoch: 1/1 Iteration: 1429/1875 train_loss0.2907049418358618\n",
      "STEP: Train Epoch: 1/1 Iteration: 1430/1875 train_loss0.29081183594710136\n",
      "STEP: Train Epoch: 1/1 Iteration: 1431/1875 train_loss0.29073080219390884\n",
      "STEP: Train Epoch: 1/1 Iteration: 1432/1875 train_loss0.29074562837203244\n",
      "STEP: Train Epoch: 1/1 Iteration: 1433/1875 train_loss0.29079099236457856\n",
      "STEP: Train Epoch: 1/1 Iteration: 1434/1875 train_loss0.2907768711642553\n",
      "STEP: Train Epoch: 1/1 Iteration: 1435/1875 train_loss0.29077329214934894\n",
      "STEP: Train Epoch: 1/1 Iteration: 1436/1875 train_loss0.2907540706339134\n",
      "STEP: Train Epoch: 1/1 Iteration: 1437/1875 train_loss0.29082207503957347\n",
      "STEP: Train Epoch: 1/1 Iteration: 1438/1875 train_loss0.29075325647128103\n",
      "STEP: Train Epoch: 1/1 Iteration: 1439/1875 train_loss0.29070402372077037\n",
      "STEP: Train Epoch: 1/1 Iteration: 1440/1875 train_loss0.29070991407950514\n",
      "STEP: Train Epoch: 1/1 Iteration: 1441/1875 train_loss0.2905451646507676\n",
      "STEP: Train Epoch: 1/1 Iteration: 1442/1875 train_loss0.29052800785533145\n",
      "STEP: Train Epoch: 1/1 Iteration: 1443/1875 train_loss0.2906248430300011\n",
      "STEP: Train Epoch: 1/1 Iteration: 1444/1875 train_loss0.2905764213654824\n",
      "STEP: Train Epoch: 1/1 Iteration: 1445/1875 train_loss0.29047159129901945\n",
      "STEP: Train Epoch: 1/1 Iteration: 1446/1875 train_loss0.2904551786848474\n",
      "STEP: Train Epoch: 1/1 Iteration: 1447/1875 train_loss0.2904211439478064\n",
      "STEP: Train Epoch: 1/1 Iteration: 1448/1875 train_loss0.2903453712030031\n",
      "STEP: Train Epoch: 1/1 Iteration: 1449/1875 train_loss0.2903249677800264\n",
      "STEP: Train Epoch: 1/1 Iteration: 1450/1875 train_loss0.29030969595215445\n",
      "STEP: Train Epoch: 1/1 Iteration: 1451/1875 train_loss0.2903627344335896\n",
      "STEP: Train Epoch: 1/1 Iteration: 1452/1875 train_loss0.2903373212086617\n",
      "STEP: Train Epoch: 1/1 Iteration: 1453/1875 train_loss0.29019961639577285\n",
      "STEP: Train Epoch: 1/1 Iteration: 1454/1875 train_loss0.29008685947719964\n",
      "STEP: Train Epoch: 1/1 Iteration: 1455/1875 train_loss0.2901841386858233\n",
      "STEP: Train Epoch: 1/1 Iteration: 1456/1875 train_loss0.2903207768698909\n",
      "STEP: Train Epoch: 1/1 Iteration: 1457/1875 train_loss0.29026724594116576\n",
      "STEP: Train Epoch: 1/1 Iteration: 1458/1875 train_loss0.29034133262252654\n",
      "STEP: Train Epoch: 1/1 Iteration: 1459/1875 train_loss0.2902423295424134\n",
      "STEP: Train Epoch: 1/1 Iteration: 1460/1875 train_loss0.29035571661127146\n",
      "STEP: Train Epoch: 1/1 Iteration: 1461/1875 train_loss0.2903208264623355\n",
      "STEP: Train Epoch: 1/1 Iteration: 1462/1875 train_loss0.29026674726537127\n",
      "STEP: Train Epoch: 1/1 Iteration: 1463/1875 train_loss0.2903987235636008\n",
      "STEP: Train Epoch: 1/1 Iteration: 1464/1875 train_loss0.290336335155492\n",
      "STEP: Train Epoch: 1/1 Iteration: 1465/1875 train_loss0.2903940341845409\n",
      "STEP: Train Epoch: 1/1 Iteration: 1466/1875 train_loss0.2904220168398742\n",
      "STEP: Train Epoch: 1/1 Iteration: 1467/1875 train_loss0.290467463678794\n",
      "STEP: Train Epoch: 1/1 Iteration: 1468/1875 train_loss0.2904086017623361\n",
      "STEP: Train Epoch: 1/1 Iteration: 1469/1875 train_loss0.2904494980615142\n",
      "STEP: Train Epoch: 1/1 Iteration: 1470/1875 train_loss0.29037344265779874\n",
      "STEP: Train Epoch: 1/1 Iteration: 1471/1875 train_loss0.2906790856337827\n",
      "STEP: Train Epoch: 1/1 Iteration: 1472/1875 train_loss0.29065682518969127\n",
      "STEP: Train Epoch: 1/1 Iteration: 1473/1875 train_loss0.29054729302223453\n",
      "STEP: Train Epoch: 1/1 Iteration: 1474/1875 train_loss0.29047545897452165\n",
      "STEP: Train Epoch: 1/1 Iteration: 1475/1875 train_loss0.29053574983977665\n",
      "STEP: Train Epoch: 1/1 Iteration: 1476/1875 train_loss0.29069556887097514\n",
      "STEP: Train Epoch: 1/1 Iteration: 1477/1875 train_loss0.29055512577078396\n",
      "STEP: Train Epoch: 1/1 Iteration: 1478/1875 train_loss0.2904848337226392\n",
      "STEP: Train Epoch: 1/1 Iteration: 1479/1875 train_loss0.2903841553398227\n",
      "STEP: Train Epoch: 1/1 Iteration: 1480/1875 train_loss0.29045141333386904\n",
      "STEP: Train Epoch: 1/1 Iteration: 1481/1875 train_loss0.2903666245656127\n",
      "STEP: Train Epoch: 1/1 Iteration: 1482/1875 train_loss0.2902817705133364\n",
      "STEP: Train Epoch: 1/1 Iteration: 1483/1875 train_loss0.29025349079361856\n",
      "STEP: Train Epoch: 1/1 Iteration: 1484/1875 train_loss0.2902867264475804\n",
      "STEP: Train Epoch: 1/1 Iteration: 1485/1875 train_loss0.29024153401077996\n",
      "STEP: Train Epoch: 1/1 Iteration: 1486/1875 train_loss0.2902494581431291\n",
      "STEP: Train Epoch: 1/1 Iteration: 1487/1875 train_loss0.290176501232071\n",
      "STEP: Train Epoch: 1/1 Iteration: 1488/1875 train_loss0.29016824713682815\n",
      "STEP: Train Epoch: 1/1 Iteration: 1489/1875 train_loss0.29021123339984345\n",
      "STEP: Train Epoch: 1/1 Iteration: 1490/1875 train_loss0.2901433698695478\n",
      "STEP: Train Epoch: 1/1 Iteration: 1491/1875 train_loss0.29006938081756883\n",
      "STEP: Train Epoch: 1/1 Iteration: 1492/1875 train_loss0.29011378946761546\n",
      "STEP: Train Epoch: 1/1 Iteration: 1493/1875 train_loss0.29007787792023326\n",
      "STEP: Train Epoch: 1/1 Iteration: 1494/1875 train_loss0.2899834243480959\n",
      "STEP: Train Epoch: 1/1 Iteration: 1495/1875 train_loss0.2898839432970437\n",
      "STEP: Train Epoch: 1/1 Iteration: 1496/1875 train_loss0.2898625342317304\n",
      "STEP: Train Epoch: 1/1 Iteration: 1497/1875 train_loss0.28982343496449925\n",
      "STEP: Train Epoch: 1/1 Iteration: 1498/1875 train_loss0.289847910654133\n",
      "STEP: Train Epoch: 1/1 Iteration: 1499/1875 train_loss0.2898538939026076\n",
      "STEP: Train Epoch: 1/1 Iteration: 1500/1875 train_loss0.28973198639477293\n",
      "STEP: Train Epoch: 1/1 Iteration: 1501/1875 train_loss0.2896438201704496\n",
      "STEP: Train Epoch: 1/1 Iteration: 1502/1875 train_loss0.2896721851385723\n",
      "STEP: Train Epoch: 1/1 Iteration: 1503/1875 train_loss0.2896196862431125\n",
      "STEP: Train Epoch: 1/1 Iteration: 1504/1875 train_loss0.2894807883917275\n",
      "STEP: Train Epoch: 1/1 Iteration: 1505/1875 train_loss0.28952630421276704\n",
      "STEP: Train Epoch: 1/1 Iteration: 1506/1875 train_loss0.28954740562865816\n",
      "STEP: Train Epoch: 1/1 Iteration: 1507/1875 train_loss0.28943070458951536\n",
      "STEP: Train Epoch: 1/1 Iteration: 1508/1875 train_loss0.2894294186450778\n",
      "STEP: Train Epoch: 1/1 Iteration: 1509/1875 train_loss0.28932149819980363\n",
      "STEP: Train Epoch: 1/1 Iteration: 1510/1875 train_loss0.2893676223356696\n",
      "STEP: Train Epoch: 1/1 Iteration: 1511/1875 train_loss0.2892498650916616\n",
      "STEP: Train Epoch: 1/1 Iteration: 1512/1875 train_loss0.28912951288838434\n",
      "STEP: Train Epoch: 1/1 Iteration: 1513/1875 train_loss0.28911681797402194\n",
      "STEP: Train Epoch: 1/1 Iteration: 1514/1875 train_loss0.2891531363240341\n",
      "STEP: Train Epoch: 1/1 Iteration: 1515/1875 train_loss0.28922007741043865\n",
      "STEP: Train Epoch: 1/1 Iteration: 1516/1875 train_loss0.2891887492637855\n",
      "STEP: Train Epoch: 1/1 Iteration: 1517/1875 train_loss0.2892649755846947\n",
      "STEP: Train Epoch: 1/1 Iteration: 1518/1875 train_loss0.2891907639745058\n",
      "STEP: Train Epoch: 1/1 Iteration: 1519/1875 train_loss0.2891402781375091\n",
      "STEP: Train Epoch: 1/1 Iteration: 1520/1875 train_loss0.2892373428937342\n",
      "STEP: Train Epoch: 1/1 Iteration: 1521/1875 train_loss0.28929351768986045\n",
      "STEP: Train Epoch: 1/1 Iteration: 1522/1875 train_loss0.2892438062869316\n",
      "STEP: Train Epoch: 1/1 Iteration: 1523/1875 train_loss0.2891687967839577\n",
      "STEP: Train Epoch: 1/1 Iteration: 1524/1875 train_loss0.2890935270543506\n",
      "STEP: Train Epoch: 1/1 Iteration: 1525/1875 train_loss0.28906013039414025\n",
      "STEP: Train Epoch: 1/1 Iteration: 1526/1875 train_loss0.2889405394261056\n",
      "STEP: Train Epoch: 1/1 Iteration: 1527/1875 train_loss0.2889125754272403\n",
      "STEP: Train Epoch: 1/1 Iteration: 1528/1875 train_loss0.28888955113663295\n",
      "STEP: Train Epoch: 1/1 Iteration: 1529/1875 train_loss0.2887950321784922\n",
      "STEP: Train Epoch: 1/1 Iteration: 1530/1875 train_loss0.2889501546053033\n",
      "STEP: Train Epoch: 1/1 Iteration: 1531/1875 train_loss0.28893696472588937\n",
      "STEP: Train Epoch: 1/1 Iteration: 1532/1875 train_loss0.2887752939236907\n",
      "STEP: Train Epoch: 1/1 Iteration: 1533/1875 train_loss0.2887051506690919\n",
      "STEP: Train Epoch: 1/1 Iteration: 1534/1875 train_loss0.28876975898884694\n",
      "STEP: Train Epoch: 1/1 Iteration: 1535/1875 train_loss0.28881660707003715\n",
      "STEP: Train Epoch: 1/1 Iteration: 1536/1875 train_loss0.2887920192382201\n",
      "STEP: Train Epoch: 1/1 Iteration: 1537/1875 train_loss0.28873138763155926\n",
      "STEP: Train Epoch: 1/1 Iteration: 1538/1875 train_loss0.2886762260671303\n",
      "STEP: Train Epoch: 1/1 Iteration: 1539/1875 train_loss0.2887729148358068\n",
      "STEP: Train Epoch: 1/1 Iteration: 1540/1875 train_loss0.28884308296920996\n",
      "STEP: Train Epoch: 1/1 Iteration: 1541/1875 train_loss0.288779479135396\n",
      "STEP: Train Epoch: 1/1 Iteration: 1542/1875 train_loss0.2887846443417993\n",
      "STEP: Train Epoch: 1/1 Iteration: 1543/1875 train_loss0.2887100458992567\n",
      "STEP: Train Epoch: 1/1 Iteration: 1544/1875 train_loss0.2886406241571096\n",
      "STEP: Train Epoch: 1/1 Iteration: 1545/1875 train_loss0.2886373585518028\n",
      "STEP: Train Epoch: 1/1 Iteration: 1546/1875 train_loss0.28849778078462723\n",
      "STEP: Train Epoch: 1/1 Iteration: 1547/1875 train_loss0.28845671401916007\n",
      "STEP: Train Epoch: 1/1 Iteration: 1548/1875 train_loss0.2883346558413745\n",
      "STEP: Train Epoch: 1/1 Iteration: 1549/1875 train_loss0.288357327855147\n",
      "STEP: Train Epoch: 1/1 Iteration: 1550/1875 train_loss0.2883562249702311\n",
      "STEP: Train Epoch: 1/1 Iteration: 1551/1875 train_loss0.288466119634373\n",
      "STEP: Train Epoch: 1/1 Iteration: 1552/1875 train_loss0.288347669429453\n",
      "STEP: Train Epoch: 1/1 Iteration: 1553/1875 train_loss0.2884885680112755\n",
      "STEP: Train Epoch: 1/1 Iteration: 1554/1875 train_loss0.28846378283433743\n",
      "STEP: Train Epoch: 1/1 Iteration: 1555/1875 train_loss0.2883734212751078\n",
      "STEP: Train Epoch: 1/1 Iteration: 1556/1875 train_loss0.28824660026738874\n",
      "STEP: Train Epoch: 1/1 Iteration: 1557/1875 train_loss0.28815003901160174\n",
      "STEP: Train Epoch: 1/1 Iteration: 1558/1875 train_loss0.28808691088511906\n",
      "STEP: Train Epoch: 1/1 Iteration: 1559/1875 train_loss0.28806875967148826\n",
      "STEP: Train Epoch: 1/1 Iteration: 1560/1875 train_loss0.2881592984084422\n",
      "STEP: Train Epoch: 1/1 Iteration: 1561/1875 train_loss0.2882108997534495\n",
      "STEP: Train Epoch: 1/1 Iteration: 1562/1875 train_loss0.2881377774938254\n",
      "STEP: Train Epoch: 1/1 Iteration: 1563/1875 train_loss0.2880390906505053\n",
      "STEP: Train Epoch: 1/1 Iteration: 1564/1875 train_loss0.2879290553441038\n",
      "STEP: Train Epoch: 1/1 Iteration: 1565/1875 train_loss0.2877946070481699\n",
      "STEP: Train Epoch: 1/1 Iteration: 1566/1875 train_loss0.2878586416088739\n",
      "STEP: Train Epoch: 1/1 Iteration: 1567/1875 train_loss0.2878804381903506\n",
      "STEP: Train Epoch: 1/1 Iteration: 1568/1875 train_loss0.28773255925269187\n",
      "STEP: Train Epoch: 1/1 Iteration: 1569/1875 train_loss0.28769575637439354\n",
      "STEP: Train Epoch: 1/1 Iteration: 1570/1875 train_loss0.28765038537205595\n",
      "STEP: Train Epoch: 1/1 Iteration: 1571/1875 train_loss0.2875307819691625\n",
      "STEP: Train Epoch: 1/1 Iteration: 1572/1875 train_loss0.2875188838076747\n",
      "STEP: Train Epoch: 1/1 Iteration: 1573/1875 train_loss0.2875000197554063\n",
      "STEP: Train Epoch: 1/1 Iteration: 1574/1875 train_loss0.28749438068146665\n",
      "STEP: Train Epoch: 1/1 Iteration: 1575/1875 train_loss0.287514602784363\n",
      "STEP: Train Epoch: 1/1 Iteration: 1576/1875 train_loss0.28765300361245755\n",
      "STEP: Train Epoch: 1/1 Iteration: 1577/1875 train_loss0.28771999329635634\n",
      "STEP: Train Epoch: 1/1 Iteration: 1578/1875 train_loss0.28764898151145124\n",
      "STEP: Train Epoch: 1/1 Iteration: 1579/1875 train_loss0.2877806480171807\n",
      "STEP: Train Epoch: 1/1 Iteration: 1580/1875 train_loss0.28770500314598785\n",
      "STEP: Train Epoch: 1/1 Iteration: 1581/1875 train_loss0.28767030105668673\n",
      "STEP: Train Epoch: 1/1 Iteration: 1582/1875 train_loss0.2875946048522361\n",
      "STEP: Train Epoch: 1/1 Iteration: 1583/1875 train_loss0.2874989720309696\n",
      "STEP: Train Epoch: 1/1 Iteration: 1584/1875 train_loss0.2874385710356456\n",
      "STEP: Train Epoch: 1/1 Iteration: 1585/1875 train_loss0.28731527495012865\n",
      "STEP: Train Epoch: 1/1 Iteration: 1586/1875 train_loss0.2871619132000948\n",
      "STEP: Train Epoch: 1/1 Iteration: 1587/1875 train_loss0.28717347637180385\n",
      "STEP: Train Epoch: 1/1 Iteration: 1588/1875 train_loss0.2870373404490989\n",
      "STEP: Train Epoch: 1/1 Iteration: 1589/1875 train_loss0.28695562123534923\n",
      "STEP: Train Epoch: 1/1 Iteration: 1590/1875 train_loss0.2868553883612812\n",
      "STEP: Train Epoch: 1/1 Iteration: 1591/1875 train_loss0.2870022131154685\n",
      "STEP: Train Epoch: 1/1 Iteration: 1592/1875 train_loss0.28697207940616015\n",
      "STEP: Train Epoch: 1/1 Iteration: 1593/1875 train_loss0.28692615586780224\n",
      "STEP: Train Epoch: 1/1 Iteration: 1594/1875 train_loss0.286920363968063\n",
      "STEP: Train Epoch: 1/1 Iteration: 1595/1875 train_loss0.2869365265271787\n",
      "STEP: Train Epoch: 1/1 Iteration: 1596/1875 train_loss0.28708628031838324\n",
      "STEP: Train Epoch: 1/1 Iteration: 1597/1875 train_loss0.2871043573570442\n",
      "STEP: Train Epoch: 1/1 Iteration: 1598/1875 train_loss0.28702885100736897\n",
      "STEP: Train Epoch: 1/1 Iteration: 1599/1875 train_loss0.2869133972053214\n",
      "STEP: Train Epoch: 1/1 Iteration: 1600/1875 train_loss0.286921751688933\n",
      "STEP: Train Epoch: 1/1 Iteration: 1601/1875 train_loss0.28681598824194676\n",
      "STEP: Train Epoch: 1/1 Iteration: 1602/1875 train_loss0.286688475869596\n",
      "STEP: Train Epoch: 1/1 Iteration: 1603/1875 train_loss0.2866738282983015\n",
      "STEP: Train Epoch: 1/1 Iteration: 1604/1875 train_loss0.286820577990561\n",
      "STEP: Train Epoch: 1/1 Iteration: 1605/1875 train_loss0.2868226909551572\n",
      "STEP: Train Epoch: 1/1 Iteration: 1606/1875 train_loss0.28678706490491535\n",
      "STEP: Train Epoch: 1/1 Iteration: 1607/1875 train_loss0.28686438753908844\n",
      "STEP: Train Epoch: 1/1 Iteration: 1608/1875 train_loss0.2868177309268576\n",
      "STEP: Train Epoch: 1/1 Iteration: 1609/1875 train_loss0.2867300362957698\n",
      "STEP: Train Epoch: 1/1 Iteration: 1610/1875 train_loss0.2866256070005135\n",
      "STEP: Train Epoch: 1/1 Iteration: 1611/1875 train_loss0.28652526655487026\n",
      "STEP: Train Epoch: 1/1 Iteration: 1612/1875 train_loss0.2865862790344944\n",
      "STEP: Train Epoch: 1/1 Iteration: 1613/1875 train_loss0.28655073439190826\n",
      "STEP: Train Epoch: 1/1 Iteration: 1614/1875 train_loss0.2864488095005852\n",
      "STEP: Train Epoch: 1/1 Iteration: 1615/1875 train_loss0.2864282776362294\n",
      "STEP: Train Epoch: 1/1 Iteration: 1616/1875 train_loss0.28649003824362035\n",
      "STEP: Train Epoch: 1/1 Iteration: 1617/1875 train_loss0.2864663365228481\n",
      "STEP: Train Epoch: 1/1 Iteration: 1618/1875 train_loss0.2864191429390214\n",
      "STEP: Train Epoch: 1/1 Iteration: 1619/1875 train_loss0.2863482354430872\n",
      "STEP: Train Epoch: 1/1 Iteration: 1620/1875 train_loss0.28634279301666\n",
      "STEP: Train Epoch: 1/1 Iteration: 1621/1875 train_loss0.286454554450716\n",
      "STEP: Train Epoch: 1/1 Iteration: 1622/1875 train_loss0.286600745427761\n",
      "STEP: Train Epoch: 1/1 Iteration: 1623/1875 train_loss0.28656772971130307\n",
      "STEP: Train Epoch: 1/1 Iteration: 1624/1875 train_loss0.28652476073756555\n",
      "STEP: Train Epoch: 1/1 Iteration: 1625/1875 train_loss0.28644525605325516\n",
      "STEP: Train Epoch: 1/1 Iteration: 1626/1875 train_loss0.2865758037542853\n",
      "STEP: Train Epoch: 1/1 Iteration: 1627/1875 train_loss0.28653920080899714\n",
      "STEP: Train Epoch: 1/1 Iteration: 1628/1875 train_loss0.2866034022765806\n",
      "STEP: Train Epoch: 1/1 Iteration: 1629/1875 train_loss0.28674097403431137\n",
      "STEP: Train Epoch: 1/1 Iteration: 1630/1875 train_loss0.2868283862659445\n",
      "STEP: Train Epoch: 1/1 Iteration: 1631/1875 train_loss0.2867451871246794\n",
      "STEP: Train Epoch: 1/1 Iteration: 1632/1875 train_loss0.2867608903028418\n",
      "STEP: Train Epoch: 1/1 Iteration: 1633/1875 train_loss0.2867315150626273\n",
      "STEP: Train Epoch: 1/1 Iteration: 1634/1875 train_loss0.2868424373344864\n",
      "STEP: Train Epoch: 1/1 Iteration: 1635/1875 train_loss0.28687379530058543\n",
      "STEP: Train Epoch: 1/1 Iteration: 1636/1875 train_loss0.286974423092478\n",
      "STEP: Train Epoch: 1/1 Iteration: 1637/1875 train_loss0.2869562006788186\n",
      "STEP: Train Epoch: 1/1 Iteration: 1638/1875 train_loss0.28689657942584107\n",
      "STEP: Train Epoch: 1/1 Iteration: 1639/1875 train_loss0.2868780962261414\n",
      "STEP: Train Epoch: 1/1 Iteration: 1640/1875 train_loss0.28685928459006654\n",
      "STEP: Train Epoch: 1/1 Iteration: 1641/1875 train_loss0.2869015812344899\n",
      "STEP: Train Epoch: 1/1 Iteration: 1642/1875 train_loss0.28687905095830724\n",
      "STEP: Train Epoch: 1/1 Iteration: 1643/1875 train_loss0.2867845907814152\n",
      "STEP: Train Epoch: 1/1 Iteration: 1644/1875 train_loss0.28678917805403653\n",
      "STEP: Train Epoch: 1/1 Iteration: 1645/1875 train_loss0.2869600231558266\n",
      "STEP: Train Epoch: 1/1 Iteration: 1646/1875 train_loss0.2869428216467271\n",
      "STEP: Train Epoch: 1/1 Iteration: 1647/1875 train_loss0.28685246505132755\n",
      "STEP: Train Epoch: 1/1 Iteration: 1648/1875 train_loss0.28681136291921683\n",
      "STEP: Train Epoch: 1/1 Iteration: 1649/1875 train_loss0.2869748090415868\n",
      "STEP: Train Epoch: 1/1 Iteration: 1650/1875 train_loss0.2868882808664983\n",
      "STEP: Train Epoch: 1/1 Iteration: 1651/1875 train_loss0.2868911041390221\n",
      "STEP: Train Epoch: 1/1 Iteration: 1652/1875 train_loss0.28686437391750425\n",
      "STEP: Train Epoch: 1/1 Iteration: 1653/1875 train_loss0.2868195386829967\n",
      "STEP: Train Epoch: 1/1 Iteration: 1654/1875 train_loss0.2868423271944528\n",
      "STEP: Train Epoch: 1/1 Iteration: 1655/1875 train_loss0.28699732200774364\n",
      "STEP: Train Epoch: 1/1 Iteration: 1656/1875 train_loss0.28698486045952254\n",
      "STEP: Train Epoch: 1/1 Iteration: 1657/1875 train_loss0.2868981470865932\n",
      "STEP: Train Epoch: 1/1 Iteration: 1658/1875 train_loss0.2870130646194189\n",
      "STEP: Train Epoch: 1/1 Iteration: 1659/1875 train_loss0.28704548230020843\n",
      "STEP: Train Epoch: 1/1 Iteration: 1660/1875 train_loss0.28696596050994044\n",
      "STEP: Train Epoch: 1/1 Iteration: 1661/1875 train_loss0.2869747555357713\n",
      "STEP: Train Epoch: 1/1 Iteration: 1662/1875 train_loss0.2870275663462984\n",
      "STEP: Train Epoch: 1/1 Iteration: 1663/1875 train_loss0.2870286462197928\n",
      "STEP: Train Epoch: 1/1 Iteration: 1664/1875 train_loss0.2869433799093643\n",
      "STEP: Train Epoch: 1/1 Iteration: 1665/1875 train_loss0.28690274405564453\n",
      "STEP: Train Epoch: 1/1 Iteration: 1666/1875 train_loss0.28691775268022\n",
      "STEP: Train Epoch: 1/1 Iteration: 1667/1875 train_loss0.28687071490387\n",
      "STEP: Train Epoch: 1/1 Iteration: 1668/1875 train_loss0.2868048032908756\n",
      "STEP: Train Epoch: 1/1 Iteration: 1669/1875 train_loss0.2868788158984206\n",
      "STEP: Train Epoch: 1/1 Iteration: 1670/1875 train_loss0.28689616196147516\n",
      "STEP: Train Epoch: 1/1 Iteration: 1671/1875 train_loss0.28688461592901365\n",
      "STEP: Train Epoch: 1/1 Iteration: 1672/1875 train_loss0.28684758374792785\n",
      "STEP: Train Epoch: 1/1 Iteration: 1673/1875 train_loss0.2867510096385443\n",
      "STEP: Train Epoch: 1/1 Iteration: 1674/1875 train_loss0.28675179565692566\n",
      "STEP: Train Epoch: 1/1 Iteration: 1675/1875 train_loss0.2869067777748873\n",
      "STEP: Train Epoch: 1/1 Iteration: 1676/1875 train_loss0.28694697933022584\n",
      "STEP: Train Epoch: 1/1 Iteration: 1677/1875 train_loss0.2869444019374975\n",
      "STEP: Train Epoch: 1/1 Iteration: 1678/1875 train_loss0.2869045014423235\n",
      "STEP: Train Epoch: 1/1 Iteration: 1679/1875 train_loss0.28683140715378924\n",
      "STEP: Train Epoch: 1/1 Iteration: 1680/1875 train_loss0.2868243103880169\n",
      "STEP: Train Epoch: 1/1 Iteration: 1681/1875 train_loss0.28678316925139846\n",
      "STEP: Train Epoch: 1/1 Iteration: 1682/1875 train_loss0.28679751980757423\n",
      "STEP: Train Epoch: 1/1 Iteration: 1683/1875 train_loss0.286789154157981\n",
      "STEP: Train Epoch: 1/1 Iteration: 1684/1875 train_loss0.28679865736164806\n",
      "STEP: Train Epoch: 1/1 Iteration: 1685/1875 train_loss0.28676536574362826\n",
      "STEP: Train Epoch: 1/1 Iteration: 1686/1875 train_loss0.286695632720112\n",
      "STEP: Train Epoch: 1/1 Iteration: 1687/1875 train_loss0.2866321021694277\n",
      "STEP: Train Epoch: 1/1 Iteration: 1688/1875 train_loss0.28661945986923415\n",
      "STEP: Train Epoch: 1/1 Iteration: 1689/1875 train_loss0.2866703548924661\n",
      "STEP: Train Epoch: 1/1 Iteration: 1690/1875 train_loss0.2867345630626619\n",
      "STEP: Train Epoch: 1/1 Iteration: 1691/1875 train_loss0.2866400167211617\n",
      "STEP: Train Epoch: 1/1 Iteration: 1692/1875 train_loss0.2866397165147319\n",
      "STEP: Train Epoch: 1/1 Iteration: 1693/1875 train_loss0.2865640626702077\n",
      "STEP: Train Epoch: 1/1 Iteration: 1694/1875 train_loss0.2866135602750966\n",
      "STEP: Train Epoch: 1/1 Iteration: 1695/1875 train_loss0.28673010621780315\n",
      "STEP: Train Epoch: 1/1 Iteration: 1696/1875 train_loss0.2866120148693949\n",
      "STEP: Train Epoch: 1/1 Iteration: 1697/1875 train_loss0.28652084825925955\n",
      "STEP: Train Epoch: 1/1 Iteration: 1698/1875 train_loss0.2869706389442683\n",
      "STEP: Train Epoch: 1/1 Iteration: 1699/1875 train_loss0.2868981845556304\n",
      "STEP: Train Epoch: 1/1 Iteration: 1700/1875 train_loss0.2871328158621841\n",
      "STEP: Train Epoch: 1/1 Iteration: 1701/1875 train_loss0.28708977313457096\n",
      "STEP: Train Epoch: 1/1 Iteration: 1702/1875 train_loss0.28720704490071564\n",
      "STEP: Train Epoch: 1/1 Iteration: 1703/1875 train_loss0.28711986395077044\n",
      "STEP: Train Epoch: 1/1 Iteration: 1704/1875 train_loss0.28709639264642317\n",
      "STEP: Train Epoch: 1/1 Iteration: 1705/1875 train_loss0.2870344027079358\n",
      "STEP: Train Epoch: 1/1 Iteration: 1706/1875 train_loss0.28705376725870974\n",
      "STEP: Train Epoch: 1/1 Iteration: 1707/1875 train_loss0.2870289255457362\n",
      "STEP: Train Epoch: 1/1 Iteration: 1708/1875 train_loss0.2870219461565327\n",
      "STEP: Train Epoch: 1/1 Iteration: 1709/1875 train_loss0.286920737393414\n",
      "STEP: Train Epoch: 1/1 Iteration: 1710/1875 train_loss0.28689490627628145\n",
      "STEP: Train Epoch: 1/1 Iteration: 1711/1875 train_loss0.2868472915687089\n",
      "STEP: Train Epoch: 1/1 Iteration: 1712/1875 train_loss0.2869241401829504\n",
      "STEP: Train Epoch: 1/1 Iteration: 1713/1875 train_loss0.286873894771349\n",
      "STEP: Train Epoch: 1/1 Iteration: 1714/1875 train_loss0.28690959255307774\n",
      "STEP: Train Epoch: 1/1 Iteration: 1715/1875 train_loss0.28696764891223203\n",
      "STEP: Train Epoch: 1/1 Iteration: 1716/1875 train_loss0.2871425872861835\n",
      "STEP: Train Epoch: 1/1 Iteration: 1717/1875 train_loss0.28716897989970935\n",
      "STEP: Train Epoch: 1/1 Iteration: 1718/1875 train_loss0.28718744200460256\n",
      "STEP: Train Epoch: 1/1 Iteration: 1719/1875 train_loss0.28718761895491224\n",
      "STEP: Train Epoch: 1/1 Iteration: 1720/1875 train_loss0.28720276928723376\n",
      "STEP: Train Epoch: 1/1 Iteration: 1721/1875 train_loss0.28723247351318615\n",
      "STEP: Train Epoch: 1/1 Iteration: 1722/1875 train_loss0.28720056726278154\n",
      "STEP: Train Epoch: 1/1 Iteration: 1723/1875 train_loss0.28739596768733694\n",
      "STEP: Train Epoch: 1/1 Iteration: 1724/1875 train_loss0.2873843147639799\n",
      "STEP: Train Epoch: 1/1 Iteration: 1725/1875 train_loss0.2874392834035815\n",
      "STEP: Train Epoch: 1/1 Iteration: 1726/1875 train_loss0.28743768128030095\n",
      "STEP: Train Epoch: 1/1 Iteration: 1727/1875 train_loss0.28735280541145497\n",
      "STEP: Train Epoch: 1/1 Iteration: 1728/1875 train_loss0.2873384387575142\n",
      "STEP: Train Epoch: 1/1 Iteration: 1729/1875 train_loss0.2872942468499417\n",
      "STEP: Train Epoch: 1/1 Iteration: 1730/1875 train_loss0.28718144285433383\n",
      "STEP: Train Epoch: 1/1 Iteration: 1731/1875 train_loss0.28712650650497773\n",
      "STEP: Train Epoch: 1/1 Iteration: 1732/1875 train_loss0.2871333252860117\n",
      "STEP: Train Epoch: 1/1 Iteration: 1733/1875 train_loss0.28711577874070393\n",
      "STEP: Train Epoch: 1/1 Iteration: 1734/1875 train_loss0.286992992714881\n",
      "STEP: Train Epoch: 1/1 Iteration: 1735/1875 train_loss0.2869120773937156\n",
      "STEP: Train Epoch: 1/1 Iteration: 1736/1875 train_loss0.2868552239051664\n",
      "STEP: Train Epoch: 1/1 Iteration: 1737/1875 train_loss0.28694223462312846\n",
      "STEP: Train Epoch: 1/1 Iteration: 1738/1875 train_loss0.28691062247498506\n",
      "STEP: Train Epoch: 1/1 Iteration: 1739/1875 train_loss0.28687120141496836\n",
      "STEP: Train Epoch: 1/1 Iteration: 1740/1875 train_loss0.2869151899473335\n",
      "STEP: Train Epoch: 1/1 Iteration: 1741/1875 train_loss0.2869570011980211\n",
      "STEP: Train Epoch: 1/1 Iteration: 1742/1875 train_loss0.2869321561432374\n",
      "STEP: Train Epoch: 1/1 Iteration: 1743/1875 train_loss0.28687935401068393\n",
      "STEP: Train Epoch: 1/1 Iteration: 1744/1875 train_loss0.2869108779507544\n",
      "STEP: Train Epoch: 1/1 Iteration: 1745/1875 train_loss0.2868426737383563\n",
      "STEP: Train Epoch: 1/1 Iteration: 1746/1875 train_loss0.28688010178561846\n",
      "STEP: Train Epoch: 1/1 Iteration: 1747/1875 train_loss0.28688069143714773\n",
      "STEP: Train Epoch: 1/1 Iteration: 1748/1875 train_loss0.2869014593312257\n",
      "STEP: Train Epoch: 1/1 Iteration: 1749/1875 train_loss0.28681156025925764\n",
      "STEP: Train Epoch: 1/1 Iteration: 1750/1875 train_loss0.28673068976721594\n",
      "STEP: Train Epoch: 1/1 Iteration: 1751/1875 train_loss0.28669914470795116\n",
      "STEP: Train Epoch: 1/1 Iteration: 1752/1875 train_loss0.28669331323453245\n",
      "STEP: Train Epoch: 1/1 Iteration: 1753/1875 train_loss0.2867661230574233\n",
      "STEP: Train Epoch: 1/1 Iteration: 1754/1875 train_loss0.286781821059044\n",
      "STEP: Train Epoch: 1/1 Iteration: 1755/1875 train_loss0.2867327719279419\n",
      "STEP: Train Epoch: 1/1 Iteration: 1756/1875 train_loss0.28668428324735756\n",
      "STEP: Train Epoch: 1/1 Iteration: 1757/1875 train_loss0.28682736770340544\n",
      "STEP: Train Epoch: 1/1 Iteration: 1758/1875 train_loss0.2868404375745219\n",
      "STEP: Train Epoch: 1/1 Iteration: 1759/1875 train_loss0.286864197921514\n",
      "STEP: Train Epoch: 1/1 Iteration: 1760/1875 train_loss0.2868073304469528\n",
      "STEP: Train Epoch: 1/1 Iteration: 1761/1875 train_loss0.28690345985904836\n",
      "STEP: Train Epoch: 1/1 Iteration: 1762/1875 train_loss0.28683747551916644\n",
      "STEP: Train Epoch: 1/1 Iteration: 1763/1875 train_loss0.28679794847639833\n",
      "STEP: Train Epoch: 1/1 Iteration: 1764/1875 train_loss0.28672594643280314\n",
      "STEP: Train Epoch: 1/1 Iteration: 1765/1875 train_loss0.28689879918887834\n",
      "STEP: Train Epoch: 1/1 Iteration: 1766/1875 train_loss0.28678371128968955\n",
      "STEP: Train Epoch: 1/1 Iteration: 1767/1875 train_loss0.2868135286475061\n",
      "STEP: Train Epoch: 1/1 Iteration: 1768/1875 train_loss0.2868171373911084\n",
      "STEP: Train Epoch: 1/1 Iteration: 1769/1875 train_loss0.28673114253491283\n",
      "STEP: Train Epoch: 1/1 Iteration: 1770/1875 train_loss0.2867867098468761\n",
      "STEP: Train Epoch: 1/1 Iteration: 1771/1875 train_loss0.2867013112978956\n",
      "STEP: Train Epoch: 1/1 Iteration: 1772/1875 train_loss0.2868174856673699\n",
      "STEP: Train Epoch: 1/1 Iteration: 1773/1875 train_loss0.2868156300598175\n",
      "STEP: Train Epoch: 1/1 Iteration: 1774/1875 train_loss0.2868167689225501\n",
      "STEP: Train Epoch: 1/1 Iteration: 1775/1875 train_loss0.28673112933379663\n",
      "STEP: Train Epoch: 1/1 Iteration: 1776/1875 train_loss0.2866810882780785\n",
      "STEP: Train Epoch: 1/1 Iteration: 1777/1875 train_loss0.2867528766877928\n",
      "STEP: Train Epoch: 1/1 Iteration: 1778/1875 train_loss0.28684391475547655\n",
      "STEP: Train Epoch: 1/1 Iteration: 1779/1875 train_loss0.2867279170199332\n",
      "STEP: Train Epoch: 1/1 Iteration: 1780/1875 train_loss0.28670010006976093\n",
      "STEP: Train Epoch: 1/1 Iteration: 1781/1875 train_loss0.2867452482118207\n",
      "STEP: Train Epoch: 1/1 Iteration: 1782/1875 train_loss0.2866947776538672\n",
      "STEP: Train Epoch: 1/1 Iteration: 1783/1875 train_loss0.28666761062362356\n",
      "STEP: Train Epoch: 1/1 Iteration: 1784/1875 train_loss0.2866324072175413\n",
      "STEP: Train Epoch: 1/1 Iteration: 1785/1875 train_loss0.2865663705784984\n",
      "STEP: Train Epoch: 1/1 Iteration: 1786/1875 train_loss0.28653610407083824\n",
      "STEP: Train Epoch: 1/1 Iteration: 1787/1875 train_loss0.2865070529317532\n",
      "STEP: Train Epoch: 1/1 Iteration: 1788/1875 train_loss0.28646630901630793\n",
      "STEP: Train Epoch: 1/1 Iteration: 1789/1875 train_loss0.2864512618706088\n",
      "STEP: Train Epoch: 1/1 Iteration: 1790/1875 train_loss0.2863584391137991\n",
      "STEP: Train Epoch: 1/1 Iteration: 1791/1875 train_loss0.28624487052490133\n",
      "STEP: Train Epoch: 1/1 Iteration: 1792/1875 train_loss0.28616713865007376\n",
      "STEP: Train Epoch: 1/1 Iteration: 1793/1875 train_loss0.28620909560291014\n",
      "STEP: Train Epoch: 1/1 Iteration: 1794/1875 train_loss0.2862156529006535\n",
      "STEP: Train Epoch: 1/1 Iteration: 1795/1875 train_loss0.28615326925717854\n",
      "STEP: Train Epoch: 1/1 Iteration: 1796/1875 train_loss0.28629142063709273\n",
      "STEP: Train Epoch: 1/1 Iteration: 1797/1875 train_loss0.2863136661219793\n",
      "STEP: Train Epoch: 1/1 Iteration: 1798/1875 train_loss0.28630633050010224\n",
      "STEP: Train Epoch: 1/1 Iteration: 1799/1875 train_loss0.2862848602991143\n",
      "STEP: Train Epoch: 1/1 Iteration: 1800/1875 train_loss0.2862913198551784\n",
      "STEP: Train Epoch: 1/1 Iteration: 1801/1875 train_loss0.2861782607468862\n",
      "STEP: Train Epoch: 1/1 Iteration: 1802/1875 train_loss0.28616967083127565\n",
      "STEP: Train Epoch: 1/1 Iteration: 1803/1875 train_loss0.28615956427761896\n",
      "STEP: Train Epoch: 1/1 Iteration: 1804/1875 train_loss0.28627772480889563\n",
      "STEP: Train Epoch: 1/1 Iteration: 1805/1875 train_loss0.28635249925761835\n",
      "STEP: Train Epoch: 1/1 Iteration: 1806/1875 train_loss0.28632571443746224\n",
      "STEP: Train Epoch: 1/1 Iteration: 1807/1875 train_loss0.2863113659845823\n",
      "STEP: Train Epoch: 1/1 Iteration: 1808/1875 train_loss0.2863264757082839\n",
      "STEP: Train Epoch: 1/1 Iteration: 1809/1875 train_loss0.28642604506542135\n",
      "STEP: Train Epoch: 1/1 Iteration: 1810/1875 train_loss0.2864363006002254\n",
      "STEP: Train Epoch: 1/1 Iteration: 1811/1875 train_loss0.28632681169158875\n",
      "STEP: Train Epoch: 1/1 Iteration: 1812/1875 train_loss0.28636586228097743\n",
      "STEP: Train Epoch: 1/1 Iteration: 1813/1875 train_loss0.2862999970468364\n",
      "STEP: Train Epoch: 1/1 Iteration: 1814/1875 train_loss0.2864130621164002\n",
      "STEP: Train Epoch: 1/1 Iteration: 1815/1875 train_loss0.2863383873219579\n",
      "STEP: Train Epoch: 1/1 Iteration: 1816/1875 train_loss0.28633416171128234\n",
      "STEP: Train Epoch: 1/1 Iteration: 1817/1875 train_loss0.2862696250665873\n",
      "STEP: Train Epoch: 1/1 Iteration: 1818/1875 train_loss0.2862223836207072\n",
      "STEP: Train Epoch: 1/1 Iteration: 1819/1875 train_loss0.28613005106149464\n",
      "STEP: Train Epoch: 1/1 Iteration: 1820/1875 train_loss0.28614215946111543\n",
      "STEP: Train Epoch: 1/1 Iteration: 1821/1875 train_loss0.2860654516197753\n",
      "STEP: Train Epoch: 1/1 Iteration: 1822/1875 train_loss0.2860139896459346\n",
      "STEP: Train Epoch: 1/1 Iteration: 1823/1875 train_loss0.2859898030428901\n",
      "STEP: Train Epoch: 1/1 Iteration: 1824/1875 train_loss0.28593749825315745\n",
      "STEP: Train Epoch: 1/1 Iteration: 1825/1875 train_loss0.2859638664606091\n",
      "STEP: Train Epoch: 1/1 Iteration: 1826/1875 train_loss0.28584462536349975\n",
      "STEP: Train Epoch: 1/1 Iteration: 1827/1875 train_loss0.2857645176336797\n",
      "STEP: Train Epoch: 1/1 Iteration: 1828/1875 train_loss0.28578917567946294\n",
      "STEP: Train Epoch: 1/1 Iteration: 1829/1875 train_loss0.2857452954836251\n",
      "STEP: Train Epoch: 1/1 Iteration: 1830/1875 train_loss0.2859142962283368\n",
      "STEP: Train Epoch: 1/1 Iteration: 1831/1875 train_loss0.28585093765779473\n",
      "STEP: Train Epoch: 1/1 Iteration: 1832/1875 train_loss0.2858306766603565\n",
      "STEP: Train Epoch: 1/1 Iteration: 1833/1875 train_loss0.2857879275457087\n",
      "STEP: Train Epoch: 1/1 Iteration: 1834/1875 train_loss0.2858656330619133\n",
      "STEP: Train Epoch: 1/1 Iteration: 1835/1875 train_loss0.2857471267385687\n",
      "STEP: Train Epoch: 1/1 Iteration: 1836/1875 train_loss0.285739776691572\n",
      "STEP: Train Epoch: 1/1 Iteration: 1837/1875 train_loss0.28566961833536947\n",
      "STEP: Train Epoch: 1/1 Iteration: 1838/1875 train_loss0.2857092841598281\n",
      "STEP: Train Epoch: 1/1 Iteration: 1839/1875 train_loss0.2857366913940723\n",
      "STEP: Train Epoch: 1/1 Iteration: 1840/1875 train_loss0.28577023409610697\n",
      "STEP: Train Epoch: 1/1 Iteration: 1841/1875 train_loss0.28576709400803774\n",
      "STEP: Train Epoch: 1/1 Iteration: 1842/1875 train_loss0.2857446134622319\n",
      "STEP: Train Epoch: 1/1 Iteration: 1843/1875 train_loss0.28577582544899316\n",
      "STEP: Train Epoch: 1/1 Iteration: 1844/1875 train_loss0.2858248499541295\n",
      "STEP: Train Epoch: 1/1 Iteration: 1845/1875 train_loss0.2857296958111408\n",
      "STEP: Train Epoch: 1/1 Iteration: 1846/1875 train_loss0.2858041834281929\n",
      "STEP: Train Epoch: 1/1 Iteration: 1847/1875 train_loss0.28573787101742604\n",
      "STEP: Train Epoch: 1/1 Iteration: 1848/1875 train_loss0.28570045782273307\n",
      "STEP: Train Epoch: 1/1 Iteration: 1849/1875 train_loss0.28571536121492036\n",
      "STEP: Train Epoch: 1/1 Iteration: 1850/1875 train_loss0.2856563312530115\n",
      "STEP: Train Epoch: 1/1 Iteration: 1851/1875 train_loss0.28568500207021613\n",
      "STEP: Train Epoch: 1/1 Iteration: 1852/1875 train_loss0.2856800592621109\n",
      "STEP: Train Epoch: 1/1 Iteration: 1853/1875 train_loss0.28563796211269277\n",
      "STEP: Train Epoch: 1/1 Iteration: 1854/1875 train_loss0.28564576809359826\n",
      "STEP: Train Epoch: 1/1 Iteration: 1855/1875 train_loss0.28571626407337797\n",
      "STEP: Train Epoch: 1/1 Iteration: 1856/1875 train_loss0.2856918100402128\n",
      "STEP: Train Epoch: 1/1 Iteration: 1857/1875 train_loss0.285618834770257\n",
      "STEP: Train Epoch: 1/1 Iteration: 1858/1875 train_loss0.28559049250872803\n",
      "STEP: Train Epoch: 1/1 Iteration: 1859/1875 train_loss0.2855974299770055\n",
      "STEP: Train Epoch: 1/1 Iteration: 1860/1875 train_loss0.2855167702711638\n",
      "STEP: Train Epoch: 1/1 Iteration: 1861/1875 train_loss0.28563917103468217\n",
      "STEP: Train Epoch: 1/1 Iteration: 1862/1875 train_loss0.2857103137695556\n",
      "STEP: Train Epoch: 1/1 Iteration: 1863/1875 train_loss0.28569426803769155\n",
      "STEP: Train Epoch: 1/1 Iteration: 1864/1875 train_loss0.285676900336375\n",
      "STEP: Train Epoch: 1/1 Iteration: 1865/1875 train_loss0.2855845957904575\n",
      "STEP: Train Epoch: 1/1 Iteration: 1866/1875 train_loss0.2855826496492995\n",
      "STEP: Train Epoch: 1/1 Iteration: 1867/1875 train_loss0.2856595216894619\n",
      "STEP: Train Epoch: 1/1 Iteration: 1868/1875 train_loss0.2855873461044942\n",
      "STEP: Train Epoch: 1/1 Iteration: 1869/1875 train_loss0.28551897995668246\n",
      "STEP: Train Epoch: 1/1 Iteration: 1870/1875 train_loss0.28540245351844773\n",
      "STEP: Train Epoch: 1/1 Iteration: 1871/1875 train_loss0.28534144380456905\n",
      "STEP: Train Epoch: 1/1 Iteration: 1872/1875 train_loss0.28532564940038496\n",
      "STEP: Train Epoch: 1/1 Iteration: 1873/1875 train_loss0.28534734311866145\n",
      "STEP: Train Epoch: 1/1 Iteration: 1874/1875 train_loss0.2853155106757325\n",
      "STEP: Train Epoch: 1/1 Iteration: 1875/1875 train_loss0.28531920634011426\n",
      "STEP: VAL Epoch: 1/1 Iteration: 1/313 val_loss0.481635183095932 val_correct: 0.90625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 2/313 val_loss0.36386266350746155 val_correct: 0.90625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 3/313 val_loss0.3347005645434062 val_correct: 0.90625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 4/313 val_loss0.2941618040204048 val_correct: 0.921875\n",
      "STEP: VAL Epoch: 1/1 Iteration: 5/313 val_loss0.30206066370010376 val_correct: 0.9125\n",
      "STEP: VAL Epoch: 1/1 Iteration: 6/313 val_loss0.27977583309014636 val_correct: 0.9114583333333334\n",
      "STEP: VAL Epoch: 1/1 Iteration: 7/313 val_loss0.2513600332396371 val_correct: 0.9241071428571429\n",
      "STEP: VAL Epoch: 1/1 Iteration: 8/313 val_loss0.26676931604743004 val_correct: 0.9140625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 9/313 val_loss0.253521333138148 val_correct: 0.9166666666666666\n",
      "STEP: VAL Epoch: 1/1 Iteration: 10/313 val_loss0.24872382283210753 val_correct: 0.91875\n",
      "STEP: VAL Epoch: 1/1 Iteration: 11/313 val_loss0.2558411684903232 val_correct: 0.9147727272727273\n",
      "STEP: VAL Epoch: 1/1 Iteration: 12/313 val_loss0.2698039511839549 val_correct: 0.9140625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 13/313 val_loss0.2697062652844649 val_correct: 0.9158653846153846\n",
      "STEP: VAL Epoch: 1/1 Iteration: 14/313 val_loss0.2629765697887966 val_correct: 0.9174107142857143\n",
      "STEP: VAL Epoch: 1/1 Iteration: 15/313 val_loss0.2725017249584198 val_correct: 0.9125\n",
      "STEP: VAL Epoch: 1/1 Iteration: 16/313 val_loss0.26471083145588636 val_correct: 0.9140625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 17/313 val_loss0.25974638409474315 val_correct: 0.9136029411764706\n",
      "STEP: VAL Epoch: 1/1 Iteration: 18/313 val_loss0.2582152717643314 val_correct: 0.9149305555555556\n",
      "STEP: VAL Epoch: 1/1 Iteration: 19/313 val_loss0.2556304029728237 val_correct: 0.9177631578947368\n",
      "STEP: VAL Epoch: 1/1 Iteration: 20/313 val_loss0.25745361521840093 val_correct: 0.9171875\n",
      "STEP: VAL Epoch: 1/1 Iteration: 21/313 val_loss0.2601679088104339 val_correct: 0.9166666666666666\n",
      "STEP: VAL Epoch: 1/1 Iteration: 22/313 val_loss0.2623344124718146 val_correct: 0.9133522727272727\n",
      "STEP: VAL Epoch: 1/1 Iteration: 23/313 val_loss0.275533443559771 val_correct: 0.907608695652174\n",
      "STEP: VAL Epoch: 1/1 Iteration: 24/313 val_loss0.2758569984386365 val_correct: 0.9088541666666666\n",
      "STEP: VAL Epoch: 1/1 Iteration: 25/313 val_loss0.278822825551033 val_correct: 0.90875\n",
      "STEP: VAL Epoch: 1/1 Iteration: 26/313 val_loss0.27745951826755816 val_correct: 0.9074519230769231\n",
      "STEP: VAL Epoch: 1/1 Iteration: 27/313 val_loss0.2707619454573702 val_correct: 0.9085648148148148\n",
      "STEP: VAL Epoch: 1/1 Iteration: 28/313 val_loss0.265022817200848 val_correct: 0.9107142857142857\n",
      "STEP: VAL Epoch: 1/1 Iteration: 29/313 val_loss0.2682575272588894 val_correct: 0.9094827586206896\n",
      "STEP: VAL Epoch: 1/1 Iteration: 30/313 val_loss0.2686390725274881 val_correct: 0.9083333333333333\n",
      "STEP: VAL Epoch: 1/1 Iteration: 31/313 val_loss0.27559633673198763 val_correct: 0.90625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 32/313 val_loss0.2784011100884527 val_correct: 0.9052734375\n",
      "STEP: VAL Epoch: 1/1 Iteration: 33/313 val_loss0.27909491166020883 val_correct: 0.9053030303030303\n",
      "STEP: VAL Epoch: 1/1 Iteration: 34/313 val_loss0.27623356024132056 val_correct: 0.9053308823529411\n",
      "STEP: VAL Epoch: 1/1 Iteration: 35/313 val_loss0.2790730870195797 val_correct: 0.9053571428571429\n",
      "STEP: VAL Epoch: 1/1 Iteration: 36/313 val_loss0.27841481007635593 val_correct: 0.9053819444444444\n",
      "STEP: VAL Epoch: 1/1 Iteration: 37/313 val_loss0.27896877418498733 val_correct: 0.9028716216216216\n",
      "STEP: VAL Epoch: 1/1 Iteration: 38/313 val_loss0.2780913787060662 val_correct: 0.9029605263157895\n",
      "STEP: VAL Epoch: 1/1 Iteration: 39/313 val_loss0.2806414998112581 val_correct: 0.9006410256410257\n",
      "STEP: VAL Epoch: 1/1 Iteration: 40/313 val_loss0.2786544283851981 val_correct: 0.9015625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 41/313 val_loss0.28191383637306167 val_correct: 0.899390243902439\n",
      "STEP: VAL Epoch: 1/1 Iteration: 42/313 val_loss0.28471694372239564 val_correct: 0.8965773809523809\n",
      "STEP: VAL Epoch: 1/1 Iteration: 43/313 val_loss0.2828267804065416 val_correct: 0.8975290697674418\n",
      "STEP: VAL Epoch: 1/1 Iteration: 44/313 val_loss0.2829155987975272 val_correct: 0.8970170454545454\n",
      "STEP: VAL Epoch: 1/1 Iteration: 45/313 val_loss0.2798354604178005 val_correct: 0.8986111111111111\n",
      "STEP: VAL Epoch: 1/1 Iteration: 46/313 val_loss0.282191484356704 val_correct: 0.8967391304347826\n",
      "STEP: VAL Epoch: 1/1 Iteration: 47/313 val_loss0.28160214344871803 val_correct: 0.8976063829787234\n",
      "STEP: VAL Epoch: 1/1 Iteration: 48/313 val_loss0.28260094532743096 val_correct: 0.8977864583333334\n",
      "STEP: VAL Epoch: 1/1 Iteration: 49/313 val_loss0.27850036718407456 val_correct: 0.8998724489795918\n",
      "STEP: VAL Epoch: 1/1 Iteration: 50/313 val_loss0.2759040755033493 val_correct: 0.90125\n",
      "STEP: VAL Epoch: 1/1 Iteration: 51/313 val_loss0.27393366834696603 val_correct: 0.9007352941176471\n",
      "STEP: VAL Epoch: 1/1 Iteration: 52/313 val_loss0.2765604274777266 val_correct: 0.9002403846153846\n",
      "STEP: VAL Epoch: 1/1 Iteration: 53/313 val_loss0.2745750237186 val_correct: 0.9009433962264151\n",
      "STEP: VAL Epoch: 1/1 Iteration: 54/313 val_loss0.27198273798933736 val_correct: 0.9016203703703703\n",
      "STEP: VAL Epoch: 1/1 Iteration: 55/313 val_loss0.27351876307617534 val_correct: 0.9011363636363636\n",
      "STEP: VAL Epoch: 1/1 Iteration: 56/313 val_loss0.2725656157625573 val_correct: 0.90234375\n",
      "STEP: VAL Epoch: 1/1 Iteration: 57/313 val_loss0.27130007168702913 val_correct: 0.9024122807017544\n",
      "STEP: VAL Epoch: 1/1 Iteration: 58/313 val_loss0.27213089270838375 val_correct: 0.9019396551724138\n",
      "STEP: VAL Epoch: 1/1 Iteration: 59/313 val_loss0.270603746175766 val_correct: 0.9014830508474576\n",
      "STEP: VAL Epoch: 1/1 Iteration: 60/313 val_loss0.26919306218624117 val_correct: 0.9020833333333333\n",
      "STEP: VAL Epoch: 1/1 Iteration: 61/313 val_loss0.2697396561747692 val_correct: 0.9016393442622951\n",
      "STEP: VAL Epoch: 1/1 Iteration: 62/313 val_loss0.27202295584063374 val_correct: 0.9007056451612904\n",
      "STEP: VAL Epoch: 1/1 Iteration: 63/313 val_loss0.2715788450505998 val_correct: 0.9017857142857143\n",
      "STEP: VAL Epoch: 1/1 Iteration: 64/313 val_loss0.2760884710587561 val_correct: 0.900390625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 65/313 val_loss0.2770294955143562 val_correct: 0.8995192307692308\n",
      "STEP: VAL Epoch: 1/1 Iteration: 66/313 val_loss0.2746988103696794 val_correct: 0.9010416666666666\n",
      "STEP: VAL Epoch: 1/1 Iteration: 67/313 val_loss0.27138728642864013 val_correct: 0.902518656716418\n",
      "STEP: VAL Epoch: 1/1 Iteration: 68/313 val_loss0.27027574515737157 val_correct: 0.9030330882352942\n",
      "STEP: VAL Epoch: 1/1 Iteration: 69/313 val_loss0.2698347327177939 val_correct: 0.9030797101449275\n",
      "STEP: VAL Epoch: 1/1 Iteration: 70/313 val_loss0.2706889072167022 val_correct: 0.9026785714285714\n",
      "STEP: VAL Epoch: 1/1 Iteration: 71/313 val_loss0.26843397009750486 val_correct: 0.9036091549295775\n",
      "STEP: VAL Epoch: 1/1 Iteration: 72/313 val_loss0.26813826590983403 val_correct: 0.9040798611111112\n",
      "STEP: VAL Epoch: 1/1 Iteration: 73/313 val_loss0.2709789758369531 val_correct: 0.9032534246575342\n",
      "STEP: VAL Epoch: 1/1 Iteration: 74/313 val_loss0.2696818476291122 val_correct: 0.9037162162162162\n",
      "STEP: VAL Epoch: 1/1 Iteration: 75/313 val_loss0.26806414787967997 val_correct: 0.9045833333333333\n",
      "STEP: VAL Epoch: 1/1 Iteration: 76/313 val_loss0.26835620427798285 val_correct: 0.9046052631578947\n",
      "STEP: VAL Epoch: 1/1 Iteration: 77/313 val_loss0.26702295151817335 val_correct: 0.9054383116883117\n",
      "STEP: VAL Epoch: 1/1 Iteration: 78/313 val_loss0.26908180237007445 val_correct: 0.9038461538461539\n",
      "STEP: VAL Epoch: 1/1 Iteration: 79/313 val_loss0.2710411609445192 val_correct: 0.9026898734177216\n",
      "STEP: VAL Epoch: 1/1 Iteration: 80/313 val_loss0.27228476558811965 val_correct: 0.90234375\n",
      "STEP: VAL Epoch: 1/1 Iteration: 81/313 val_loss0.2728294745363571 val_correct: 0.9016203703703703\n",
      "STEP: VAL Epoch: 1/1 Iteration: 82/313 val_loss0.27470316033719516 val_correct: 0.9005335365853658\n",
      "STEP: VAL Epoch: 1/1 Iteration: 83/313 val_loss0.27530980976410657 val_correct: 0.8998493975903614\n",
      "STEP: VAL Epoch: 1/1 Iteration: 84/313 val_loss0.27556474937037345 val_correct: 0.8991815476190477\n",
      "STEP: VAL Epoch: 1/1 Iteration: 85/313 val_loss0.2753308047704837 val_correct: 0.8988970588235294\n",
      "STEP: VAL Epoch: 1/1 Iteration: 86/313 val_loss0.27517641773230805 val_correct: 0.8993459302325582\n",
      "STEP: VAL Epoch: 1/1 Iteration: 87/313 val_loss0.2743780762385363 val_correct: 0.8994252873563219\n",
      "STEP: VAL Epoch: 1/1 Iteration: 88/313 val_loss0.2746680326844481 val_correct: 0.8995028409090909\n",
      "STEP: VAL Epoch: 1/1 Iteration: 89/313 val_loss0.2771179738423128 val_correct: 0.898876404494382\n",
      "STEP: VAL Epoch: 1/1 Iteration: 90/313 val_loss0.2770659524947405 val_correct: 0.8989583333333333\n",
      "STEP: VAL Epoch: 1/1 Iteration: 91/313 val_loss0.28108881057782487 val_correct: 0.8976648351648352\n",
      "STEP: VAL Epoch: 1/1 Iteration: 92/313 val_loss0.2860873839210557 val_correct: 0.8960597826086957\n",
      "STEP: VAL Epoch: 1/1 Iteration: 93/313 val_loss0.2878985278789074 val_correct: 0.8961693548387096\n",
      "STEP: VAL Epoch: 1/1 Iteration: 94/313 val_loss0.2886634762696129 val_correct: 0.8956117021276596\n",
      "STEP: VAL Epoch: 1/1 Iteration: 95/313 val_loss0.28989143673526613 val_correct: 0.8953947368421052\n",
      "STEP: VAL Epoch: 1/1 Iteration: 96/313 val_loss0.28871378911814344 val_correct: 0.8961588541666666\n",
      "STEP: VAL Epoch: 1/1 Iteration: 97/313 val_loss0.2877899296581745 val_correct: 0.8969072164948454\n",
      "STEP: VAL Epoch: 1/1 Iteration: 98/313 val_loss0.2888150436005422 val_correct: 0.8966836734693877\n",
      "STEP: VAL Epoch: 1/1 Iteration: 99/313 val_loss0.2867003128007807 val_correct: 0.8977272727272727\n",
      "STEP: VAL Epoch: 1/1 Iteration: 100/313 val_loss0.2872177901491523 val_correct: 0.896875\n",
      "STEP: VAL Epoch: 1/1 Iteration: 101/313 val_loss0.2872722101314823 val_correct: 0.8963490099009901\n",
      "STEP: VAL Epoch: 1/1 Iteration: 102/313 val_loss0.29004470211471994 val_correct: 0.8952205882352942\n",
      "STEP: VAL Epoch: 1/1 Iteration: 103/313 val_loss0.2911929198477453 val_correct: 0.8941140776699029\n",
      "STEP: VAL Epoch: 1/1 Iteration: 104/313 val_loss0.2918201022638151 val_correct: 0.8936298076923077\n",
      "STEP: VAL Epoch: 1/1 Iteration: 105/313 val_loss0.2933875060862019 val_correct: 0.8928571428571429\n",
      "STEP: VAL Epoch: 1/1 Iteration: 106/313 val_loss0.29190724100566134 val_correct: 0.8929834905660378\n",
      "STEP: VAL Epoch: 1/1 Iteration: 107/313 val_loss0.29099310332230316 val_correct: 0.8933995327102804\n",
      "STEP: VAL Epoch: 1/1 Iteration: 108/313 val_loss0.2905335928209954 val_correct: 0.8938078703703703\n",
      "STEP: VAL Epoch: 1/1 Iteration: 109/313 val_loss0.2920728295426303 val_correct: 0.8927752293577982\n",
      "STEP: VAL Epoch: 1/1 Iteration: 110/313 val_loss0.29396033662964 val_correct: 0.8920454545454546\n",
      "STEP: VAL Epoch: 1/1 Iteration: 111/313 val_loss0.29584241178524384 val_correct: 0.8916103603603603\n",
      "STEP: VAL Epoch: 1/1 Iteration: 112/313 val_loss0.29762884651842925 val_correct: 0.8914620535714286\n",
      "STEP: VAL Epoch: 1/1 Iteration: 113/313 val_loss0.2975995901609417 val_correct: 0.891316371681416\n",
      "STEP: VAL Epoch: 1/1 Iteration: 114/313 val_loss0.2967898837503111 val_correct: 0.8917214912280702\n",
      "STEP: VAL Epoch: 1/1 Iteration: 115/313 val_loss0.29784879629378735 val_correct: 0.8913043478260869\n",
      "STEP: VAL Epoch: 1/1 Iteration: 116/313 val_loss0.29705621761366213 val_correct: 0.8914331896551724\n",
      "STEP: VAL Epoch: 1/1 Iteration: 117/313 val_loss0.29773843896567315 val_correct: 0.8912927350427351\n",
      "STEP: VAL Epoch: 1/1 Iteration: 118/313 val_loss0.29668534076693703 val_correct: 0.8919491525423728\n",
      "STEP: VAL Epoch: 1/1 Iteration: 119/313 val_loss0.2981131872897889 val_correct: 0.8918067226890757\n",
      "STEP: VAL Epoch: 1/1 Iteration: 120/313 val_loss0.29955644480263194 val_correct: 0.8911458333333333\n",
      "STEP: VAL Epoch: 1/1 Iteration: 121/313 val_loss0.29976350804867824 val_correct: 0.8912706611570248\n",
      "STEP: VAL Epoch: 1/1 Iteration: 122/313 val_loss0.3004161243739187 val_correct: 0.8911372950819673\n",
      "STEP: VAL Epoch: 1/1 Iteration: 123/313 val_loss0.2989303021108716 val_correct: 0.8917682926829268\n",
      "STEP: VAL Epoch: 1/1 Iteration: 124/313 val_loss0.2996119325379691 val_correct: 0.8911290322580645\n",
      "STEP: VAL Epoch: 1/1 Iteration: 125/313 val_loss0.3006575147807598 val_correct: 0.8905\n",
      "STEP: VAL Epoch: 1/1 Iteration: 126/313 val_loss0.30043563054549316 val_correct: 0.8901289682539683\n",
      "STEP: VAL Epoch: 1/1 Iteration: 127/313 val_loss0.3013784913272839 val_correct: 0.8895177165354331\n",
      "STEP: VAL Epoch: 1/1 Iteration: 128/313 val_loss0.300654021644732 val_correct: 0.889892578125\n",
      "STEP: VAL Epoch: 1/1 Iteration: 129/313 val_loss0.3021222996099513 val_correct: 0.8890503875968992\n",
      "STEP: VAL Epoch: 1/1 Iteration: 130/313 val_loss0.30459694458315006 val_correct: 0.8875\n",
      "STEP: VAL Epoch: 1/1 Iteration: 131/313 val_loss0.3039387427839159 val_correct: 0.8876431297709924\n",
      "STEP: VAL Epoch: 1/1 Iteration: 132/313 val_loss0.3035929139534181 val_correct: 0.8875473484848485\n",
      "STEP: VAL Epoch: 1/1 Iteration: 133/313 val_loss0.3045328528081116 val_correct: 0.8872180451127819\n",
      "STEP: VAL Epoch: 1/1 Iteration: 134/313 val_loss0.30405238101175475 val_correct: 0.8873600746268657\n",
      "STEP: VAL Epoch: 1/1 Iteration: 135/313 val_loss0.3031568759845363 val_correct: 0.8877314814814815\n",
      "STEP: VAL Epoch: 1/1 Iteration: 136/313 val_loss0.3018013851114494 val_correct: 0.8883272058823529\n",
      "STEP: VAL Epoch: 1/1 Iteration: 137/313 val_loss0.300256368751726 val_correct: 0.8891423357664233\n",
      "STEP: VAL Epoch: 1/1 Iteration: 138/313 val_loss0.29997940650344757 val_correct: 0.8894927536231884\n",
      "STEP: VAL Epoch: 1/1 Iteration: 139/313 val_loss0.29944508311023815 val_correct: 0.8898381294964028\n",
      "STEP: VAL Epoch: 1/1 Iteration: 140/313 val_loss0.29865804578044586 val_correct: 0.8899553571428571\n",
      "STEP: VAL Epoch: 1/1 Iteration: 141/313 val_loss0.29800032911465524 val_correct: 0.8900709219858156\n",
      "STEP: VAL Epoch: 1/1 Iteration: 142/313 val_loss0.29742158988726813 val_correct: 0.8904049295774648\n",
      "STEP: VAL Epoch: 1/1 Iteration: 143/313 val_loss0.2969060475332337 val_correct: 0.8907342657342657\n",
      "STEP: VAL Epoch: 1/1 Iteration: 144/313 val_loss0.29651304373207193 val_correct: 0.8910590277777778\n",
      "STEP: VAL Epoch: 1/1 Iteration: 145/313 val_loss0.2954361299759355 val_correct: 0.8913793103448275\n",
      "STEP: VAL Epoch: 1/1 Iteration: 146/313 val_loss0.2954952244021713 val_correct: 0.8912671232876712\n",
      "STEP: VAL Epoch: 1/1 Iteration: 147/313 val_loss0.2977928244287059 val_correct: 0.890093537414966\n",
      "STEP: VAL Epoch: 1/1 Iteration: 148/313 val_loss0.29831147563920635 val_correct: 0.8902027027027027\n",
      "STEP: VAL Epoch: 1/1 Iteration: 149/313 val_loss0.2986006985524757 val_correct: 0.8903104026845637\n",
      "STEP: VAL Epoch: 1/1 Iteration: 150/313 val_loss0.2977281579126914 val_correct: 0.8908333333333334\n",
      "STEP: VAL Epoch: 1/1 Iteration: 151/313 val_loss0.29836701012111655 val_correct: 0.890521523178808\n",
      "STEP: VAL Epoch: 1/1 Iteration: 152/313 val_loss0.29861828829406906 val_correct: 0.8902138157894737\n",
      "STEP: VAL Epoch: 1/1 Iteration: 153/313 val_loss0.2982059783162245 val_correct: 0.8901143790849673\n",
      "STEP: VAL Epoch: 1/1 Iteration: 154/313 val_loss0.29929049484818787 val_correct: 0.8898133116883117\n",
      "STEP: VAL Epoch: 1/1 Iteration: 155/313 val_loss0.29947052499459637 val_correct: 0.8899193548387097\n",
      "STEP: VAL Epoch: 1/1 Iteration: 156/313 val_loss0.2981687399487083 val_correct: 0.8904246794871795\n",
      "STEP: VAL Epoch: 1/1 Iteration: 157/313 val_loss0.29886480987926195 val_correct: 0.8905254777070064\n",
      "STEP: VAL Epoch: 1/1 Iteration: 158/313 val_loss0.29962534327767315 val_correct: 0.8898338607594937\n",
      "STEP: VAL Epoch: 1/1 Iteration: 159/313 val_loss0.30157917374803583 val_correct: 0.8893474842767296\n",
      "STEP: VAL Epoch: 1/1 Iteration: 160/313 val_loss0.30109824317041783 val_correct: 0.8896484375\n",
      "STEP: VAL Epoch: 1/1 Iteration: 161/313 val_loss0.3016540106640469 val_correct: 0.889751552795031\n",
      "STEP: VAL Epoch: 1/1 Iteration: 162/313 val_loss0.30263378593012874 val_correct: 0.8894675925925926\n",
      "STEP: VAL Epoch: 1/1 Iteration: 163/313 val_loss0.3030168112664501 val_correct: 0.8895705521472392\n",
      "STEP: VAL Epoch: 1/1 Iteration: 164/313 val_loss0.30200027095199355 val_correct: 0.8898628048780488\n",
      "STEP: VAL Epoch: 1/1 Iteration: 165/313 val_loss0.3034629119390791 val_correct: 0.8895833333333333\n",
      "STEP: VAL Epoch: 1/1 Iteration: 166/313 val_loss0.3024578771542713 val_correct: 0.8898719879518072\n",
      "STEP: VAL Epoch: 1/1 Iteration: 167/313 val_loss0.30467585873550285 val_correct: 0.8895958083832335\n",
      "STEP: VAL Epoch: 1/1 Iteration: 168/313 val_loss0.3034560168827219 val_correct: 0.8900669642857143\n",
      "STEP: VAL Epoch: 1/1 Iteration: 169/313 val_loss0.3039204468785306 val_correct: 0.8903476331360947\n",
      "STEP: VAL Epoch: 1/1 Iteration: 170/313 val_loss0.3042862805592663 val_correct: 0.8902573529411765\n",
      "STEP: VAL Epoch: 1/1 Iteration: 171/313 val_loss0.30472524989156696 val_correct: 0.8901681286549707\n",
      "STEP: VAL Epoch: 1/1 Iteration: 172/313 val_loss0.304206959145187 val_correct: 0.8902616279069767\n",
      "STEP: VAL Epoch: 1/1 Iteration: 173/313 val_loss0.30685898310617904 val_correct: 0.8892702312138728\n",
      "STEP: VAL Epoch: 1/1 Iteration: 174/313 val_loss0.30558402688595754 val_correct: 0.8897270114942529\n",
      "STEP: VAL Epoch: 1/1 Iteration: 175/313 val_loss0.30855648523994855 val_correct: 0.8892857142857142\n",
      "STEP: VAL Epoch: 1/1 Iteration: 176/313 val_loss0.3083859083285047 val_correct: 0.8892045454545454\n",
      "STEP: VAL Epoch: 1/1 Iteration: 177/313 val_loss0.30783273605892886 val_correct: 0.8893008474576272\n",
      "STEP: VAL Epoch: 1/1 Iteration: 178/313 val_loss0.30745165550223225 val_correct: 0.8892205056179775\n",
      "STEP: VAL Epoch: 1/1 Iteration: 179/313 val_loss0.30757265864720557 val_correct: 0.8891410614525139\n",
      "STEP: VAL Epoch: 1/1 Iteration: 180/313 val_loss0.30684008232007426 val_correct: 0.8894097222222223\n",
      "STEP: VAL Epoch: 1/1 Iteration: 181/313 val_loss0.3063526364115391 val_correct: 0.8896754143646409\n",
      "STEP: VAL Epoch: 1/1 Iteration: 182/313 val_loss0.30650306908557046 val_correct: 0.8897664835164835\n",
      "STEP: VAL Epoch: 1/1 Iteration: 183/313 val_loss0.3063559189968096 val_correct: 0.8898565573770492\n",
      "STEP: VAL Epoch: 1/1 Iteration: 184/313 val_loss0.305181918847982 val_correct: 0.8902853260869565\n",
      "STEP: VAL Epoch: 1/1 Iteration: 185/313 val_loss0.30550217268032 val_correct: 0.8902027027027027\n",
      "STEP: VAL Epoch: 1/1 Iteration: 186/313 val_loss0.3051171190035279 val_correct: 0.8902889784946236\n",
      "STEP: VAL Epoch: 1/1 Iteration: 187/313 val_loss0.3054154347329535 val_correct: 0.8900401069518716\n",
      "STEP: VAL Epoch: 1/1 Iteration: 188/313 val_loss0.30559479486831326 val_correct: 0.8902925531914894\n",
      "STEP: VAL Epoch: 1/1 Iteration: 189/313 val_loss0.3048347867236882 val_correct: 0.890707671957672\n",
      "STEP: VAL Epoch: 1/1 Iteration: 190/313 val_loss0.30424167641291494 val_correct: 0.8911184210526316\n",
      "STEP: VAL Epoch: 1/1 Iteration: 191/313 val_loss0.30406481045156875 val_correct: 0.8908704188481675\n",
      "STEP: VAL Epoch: 1/1 Iteration: 192/313 val_loss0.30422280544492725 val_correct: 0.8909505208333334\n",
      "STEP: VAL Epoch: 1/1 Iteration: 193/313 val_loss0.30468676484387774 val_correct: 0.8902202072538861\n",
      "STEP: VAL Epoch: 1/1 Iteration: 194/313 val_loss0.30389638646438566 val_correct: 0.8904639175257731\n",
      "STEP: VAL Epoch: 1/1 Iteration: 195/313 val_loss0.3027527734828301 val_correct: 0.8910256410256411\n",
      "STEP: VAL Epoch: 1/1 Iteration: 196/313 val_loss0.3033189395518631 val_correct: 0.8907844387755102\n",
      "STEP: VAL Epoch: 1/1 Iteration: 197/313 val_loss0.3035999440261858 val_correct: 0.8905456852791879\n",
      "STEP: VAL Epoch: 1/1 Iteration: 198/313 val_loss0.3036046442622789 val_correct: 0.8907828282828283\n",
      "STEP: VAL Epoch: 1/1 Iteration: 199/313 val_loss0.30330850638861034 val_correct: 0.8910175879396985\n",
      "STEP: VAL Epoch: 1/1 Iteration: 200/313 val_loss0.30265377862378956 val_correct: 0.89140625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 201/313 val_loss0.30287189950323223 val_correct: 0.8911691542288557\n",
      "STEP: VAL Epoch: 1/1 Iteration: 202/313 val_loss0.30308422577189337 val_correct: 0.8912438118811881\n",
      "STEP: VAL Epoch: 1/1 Iteration: 203/313 val_loss0.30298785122083915 val_correct: 0.8911637931034483\n",
      "STEP: VAL Epoch: 1/1 Iteration: 204/313 val_loss0.3031269158327989 val_correct: 0.8909313725490197\n",
      "STEP: VAL Epoch: 1/1 Iteration: 205/313 val_loss0.30393110817162 val_correct: 0.8908536585365854\n",
      "STEP: VAL Epoch: 1/1 Iteration: 206/313 val_loss0.30420099327214134 val_correct: 0.8909283980582524\n",
      "STEP: VAL Epoch: 1/1 Iteration: 207/313 val_loss0.3033845777204935 val_correct: 0.8911533816425121\n",
      "STEP: VAL Epoch: 1/1 Iteration: 208/313 val_loss0.3030917531846521 val_correct: 0.8913762019230769\n",
      "STEP: VAL Epoch: 1/1 Iteration: 209/313 val_loss0.3035913720620021 val_correct: 0.8909988038277512\n",
      "STEP: VAL Epoch: 1/1 Iteration: 210/313 val_loss0.3040999895582596 val_correct: 0.890625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 211/313 val_loss0.3038672450841603 val_correct: 0.8904028436018957\n",
      "STEP: VAL Epoch: 1/1 Iteration: 212/313 val_loss0.30324177716349093 val_correct: 0.8904775943396226\n",
      "STEP: VAL Epoch: 1/1 Iteration: 213/313 val_loss0.3028308076722801 val_correct: 0.8905516431924883\n",
      "STEP: VAL Epoch: 1/1 Iteration: 214/313 val_loss0.30329330744239213 val_correct: 0.8904789719626168\n",
      "STEP: VAL Epoch: 1/1 Iteration: 215/313 val_loss0.3046344196207302 val_correct: 0.8901162790697674\n",
      "STEP: VAL Epoch: 1/1 Iteration: 216/313 val_loss0.3038877223756302 val_correct: 0.8903356481481481\n",
      "STEP: VAL Epoch: 1/1 Iteration: 217/313 val_loss0.3036461611050889 val_correct: 0.8904089861751152\n",
      "STEP: VAL Epoch: 1/1 Iteration: 218/313 val_loss0.30320109418431007 val_correct: 0.8904816513761468\n",
      "STEP: VAL Epoch: 1/1 Iteration: 219/313 val_loss0.3030886544450505 val_correct: 0.8902682648401826\n",
      "STEP: VAL Epoch: 1/1 Iteration: 220/313 val_loss0.3030187778005546 val_correct: 0.8901988636363637\n",
      "STEP: VAL Epoch: 1/1 Iteration: 221/313 val_loss0.30268938716383004 val_correct: 0.8901300904977375\n",
      "STEP: VAL Epoch: 1/1 Iteration: 222/313 val_loss0.3024756281997438 val_correct: 0.8899211711711712\n",
      "STEP: VAL Epoch: 1/1 Iteration: 223/313 val_loss0.3023756726941453 val_correct: 0.8897141255605381\n",
      "STEP: VAL Epoch: 1/1 Iteration: 224/313 val_loss0.30220194059490624 val_correct: 0.8899274553571429\n",
      "STEP: VAL Epoch: 1/1 Iteration: 225/313 val_loss0.30251012674636313 val_correct: 0.8897222222222222\n",
      "STEP: VAL Epoch: 1/1 Iteration: 226/313 val_loss0.302742869238806 val_correct: 0.8895188053097345\n",
      "STEP: VAL Epoch: 1/1 Iteration: 227/313 val_loss0.30254423754527704 val_correct: 0.8895925110132159\n",
      "STEP: VAL Epoch: 1/1 Iteration: 228/313 val_loss0.30169263761490583 val_correct: 0.8899396929824561\n",
      "STEP: VAL Epoch: 1/1 Iteration: 229/313 val_loss0.30200896661029114 val_correct: 0.8900109170305677\n",
      "STEP: VAL Epoch: 1/1 Iteration: 230/313 val_loss0.30196838565170764 val_correct: 0.8900815217391305\n",
      "STEP: VAL Epoch: 1/1 Iteration: 231/313 val_loss0.30192331968125324 val_correct: 0.8901515151515151\n",
      "STEP: VAL Epoch: 1/1 Iteration: 232/313 val_loss0.30119027388442693 val_correct: 0.8903556034482759\n",
      "STEP: VAL Epoch: 1/1 Iteration: 233/313 val_loss0.3011886005333821 val_correct: 0.8902896995708155\n",
      "STEP: VAL Epoch: 1/1 Iteration: 234/313 val_loss0.30015944691104257 val_correct: 0.890625\n",
      "STEP: VAL Epoch: 1/1 Iteration: 235/313 val_loss0.29921123177764264 val_correct: 0.8910904255319149\n",
      "STEP: VAL Epoch: 1/1 Iteration: 236/313 val_loss0.29842724068627013 val_correct: 0.891551906779661\n",
      "STEP: VAL Epoch: 1/1 Iteration: 237/313 val_loss0.29807570223000984 val_correct: 0.8916139240506329\n",
      "STEP: VAL Epoch: 1/1 Iteration: 238/313 val_loss0.29765498354843184 val_correct: 0.8918067226890757\n",
      "STEP: VAL Epoch: 1/1 Iteration: 239/313 val_loss0.29741046117875863 val_correct: 0.8919979079497908\n",
      "STEP: VAL Epoch: 1/1 Iteration: 240/313 val_loss0.2969749687705189 val_correct: 0.8920572916666667\n",
      "STEP: VAL Epoch: 1/1 Iteration: 241/313 val_loss0.29628166950528056 val_correct: 0.8923755186721992\n",
      "STEP: VAL Epoch: 1/1 Iteration: 242/313 val_loss0.29563895352800523 val_correct: 0.8925619834710744\n",
      "STEP: VAL Epoch: 1/1 Iteration: 243/313 val_loss0.2954514942020791 val_correct: 0.8927469135802469\n",
      "STEP: VAL Epoch: 1/1 Iteration: 244/313 val_loss0.2955395152303772 val_correct: 0.8928022540983607\n",
      "STEP: VAL Epoch: 1/1 Iteration: 245/313 val_loss0.2954651151840784 val_correct: 0.8926020408163265\n",
      "STEP: VAL Epoch: 1/1 Iteration: 246/313 val_loss0.2950687374980227 val_correct: 0.8926575203252033\n",
      "STEP: VAL Epoch: 1/1 Iteration: 247/313 val_loss0.2946369369110839 val_correct: 0.8927125506072875\n",
      "STEP: VAL Epoch: 1/1 Iteration: 248/313 val_loss0.2943817884060404 val_correct: 0.8925151209677419\n",
      "STEP: VAL Epoch: 1/1 Iteration: 249/313 val_loss0.29490507916214476 val_correct: 0.8923192771084337\n",
      "STEP: VAL Epoch: 1/1 Iteration: 250/313 val_loss0.29562118946015836 val_correct: 0.89175\n",
      "STEP: VAL Epoch: 1/1 Iteration: 251/313 val_loss0.29564006405463256 val_correct: 0.891683266932271\n",
      "STEP: VAL Epoch: 1/1 Iteration: 252/313 val_loss0.2958532926434326 val_correct: 0.8914930555555556\n",
      "STEP: VAL Epoch: 1/1 Iteration: 253/313 val_loss0.29556053003715904 val_correct: 0.8915513833992095\n",
      "STEP: VAL Epoch: 1/1 Iteration: 254/313 val_loss0.29514584201938054 val_correct: 0.8917322834645669\n",
      "STEP: VAL Epoch: 1/1 Iteration: 255/313 val_loss0.29586540622746244 val_correct: 0.8917892156862746\n",
      "STEP: VAL Epoch: 1/1 Iteration: 256/313 val_loss0.2950579539319733 val_correct: 0.89208984375\n",
      "STEP: VAL Epoch: 1/1 Iteration: 257/313 val_loss0.2947227608861858 val_correct: 0.8921449416342413\n",
      "STEP: VAL Epoch: 1/1 Iteration: 258/313 val_loss0.29395878891092403 val_correct: 0.8924418604651163\n",
      "STEP: VAL Epoch: 1/1 Iteration: 259/313 val_loss0.29371541777345206 val_correct: 0.8926158301158301\n",
      "STEP: VAL Epoch: 1/1 Iteration: 260/313 val_loss0.2942401104678328 val_correct: 0.8925480769230769\n",
      "STEP: VAL Epoch: 1/1 Iteration: 261/313 val_loss0.29434572535627646 val_correct: 0.89272030651341\n",
      "STEP: VAL Epoch: 1/1 Iteration: 262/313 val_loss0.2940682417762871 val_correct: 0.8930104961832062\n",
      "STEP: VAL Epoch: 1/1 Iteration: 263/313 val_loss0.29346242658935573 val_correct: 0.8932984790874525\n",
      "STEP: VAL Epoch: 1/1 Iteration: 264/313 val_loss0.2930634977211329 val_correct: 0.8934659090909091\n",
      "STEP: VAL Epoch: 1/1 Iteration: 265/313 val_loss0.2930069138840684 val_correct: 0.8931603773584905\n",
      "STEP: VAL Epoch: 1/1 Iteration: 266/313 val_loss0.2922244896426013 val_correct: 0.8934445488721805\n",
      "STEP: VAL Epoch: 1/1 Iteration: 267/313 val_loss0.29258726943242414 val_correct: 0.8932584269662921\n",
      "STEP: VAL Epoch: 1/1 Iteration: 268/313 val_loss0.2921184181138428 val_correct: 0.8931902985074627\n",
      "STEP: VAL Epoch: 1/1 Iteration: 269/313 val_loss0.291888112458487 val_correct: 0.8933550185873605\n",
      "STEP: VAL Epoch: 1/1 Iteration: 270/313 val_loss0.2919167846303295 val_correct: 0.8935185185185185\n",
      "STEP: VAL Epoch: 1/1 Iteration: 271/313 val_loss0.29209064809103735 val_correct: 0.8935654981549815\n",
      "STEP: VAL Epoch: 1/1 Iteration: 272/313 val_loss0.291632182937225 val_correct: 0.8937270220588235\n",
      "STEP: VAL Epoch: 1/1 Iteration: 273/313 val_loss0.29249408725144227 val_correct: 0.8933150183150184\n",
      "STEP: VAL Epoch: 1/1 Iteration: 274/313 val_loss0.29346023580181774 val_correct: 0.8927919708029197\n",
      "STEP: VAL Epoch: 1/1 Iteration: 275/313 val_loss0.2933519636907361 val_correct: 0.8927272727272727\n",
      "STEP: VAL Epoch: 1/1 Iteration: 276/313 val_loss0.292828275444175 val_correct: 0.892776268115942\n",
      "STEP: VAL Epoch: 1/1 Iteration: 277/313 val_loss0.29253403190187166 val_correct: 0.892937725631769\n",
      "STEP: VAL Epoch: 1/1 Iteration: 278/313 val_loss0.2928630197085708 val_correct: 0.892648381294964\n",
      "STEP: VAL Epoch: 1/1 Iteration: 279/313 val_loss0.2937038867570807 val_correct: 0.8926971326164874\n",
      "STEP: VAL Epoch: 1/1 Iteration: 280/313 val_loss0.2941640832461417 val_correct: 0.8924107142857143\n",
      "STEP: VAL Epoch: 1/1 Iteration: 281/313 val_loss0.293938519572342 val_correct: 0.8925711743772242\n",
      "STEP: VAL Epoch: 1/1 Iteration: 282/313 val_loss0.29375259846396057 val_correct: 0.8926196808510638\n",
      "STEP: VAL Epoch: 1/1 Iteration: 283/313 val_loss0.2936631806931942 val_correct: 0.8926678445229682\n",
      "STEP: VAL Epoch: 1/1 Iteration: 284/313 val_loss0.2936741021711012 val_correct: 0.8924955985915493\n",
      "STEP: VAL Epoch: 1/1 Iteration: 285/313 val_loss0.2932597219813288 val_correct: 0.8927631578947368\n",
      "STEP: VAL Epoch: 1/1 Iteration: 286/313 val_loss0.29335690220700694 val_correct: 0.8925917832167832\n",
      "STEP: VAL Epoch: 1/1 Iteration: 287/313 val_loss0.2932960083545916 val_correct: 0.8924216027874564\n",
      "STEP: VAL Epoch: 1/1 Iteration: 288/313 val_loss0.2934878068950234 val_correct: 0.8922526041666666\n",
      "STEP: VAL Epoch: 1/1 Iteration: 289/313 val_loss0.29356205441495953 val_correct: 0.8921929065743944\n",
      "STEP: VAL Epoch: 1/1 Iteration: 290/313 val_loss0.29389221625852174 val_correct: 0.8920258620689655\n",
      "STEP: VAL Epoch: 1/1 Iteration: 291/313 val_loss0.2939712249408268 val_correct: 0.8919673539518901\n",
      "STEP: VAL Epoch: 1/1 Iteration: 292/313 val_loss0.2941627029763305 val_correct: 0.8920162671232876\n",
      "STEP: VAL Epoch: 1/1 Iteration: 293/313 val_loss0.2945111470087193 val_correct: 0.891744880546075\n",
      "STEP: VAL Epoch: 1/1 Iteration: 294/313 val_loss0.29402683689227316 val_correct: 0.8917942176870748\n",
      "STEP: VAL Epoch: 1/1 Iteration: 295/313 val_loss0.29352390380986665 val_correct: 0.8919491525423728\n",
      "STEP: VAL Epoch: 1/1 Iteration: 296/313 val_loss0.2933116921072675 val_correct: 0.8921030405405406\n",
      "STEP: VAL Epoch: 1/1 Iteration: 297/313 val_loss0.29367497828020794 val_correct: 0.8920454545454546\n",
      "STEP: VAL Epoch: 1/1 Iteration: 298/313 val_loss0.29344982882624104 val_correct: 0.8920931208053692\n",
      "STEP: VAL Epoch: 1/1 Iteration: 299/313 val_loss0.2932412324217849 val_correct: 0.8920359531772575\n",
      "STEP: VAL Epoch: 1/1 Iteration: 300/313 val_loss0.2937946345284581 val_correct: 0.8917708333333333\n",
      "STEP: VAL Epoch: 1/1 Iteration: 301/313 val_loss0.2932160186435891 val_correct: 0.8920265780730897\n",
      "STEP: VAL Epoch: 1/1 Iteration: 302/313 val_loss0.29268340807136717 val_correct: 0.8922806291390728\n",
      "STEP: VAL Epoch: 1/1 Iteration: 303/313 val_loss0.2931350254309256 val_correct: 0.892223597359736\n",
      "STEP: VAL Epoch: 1/1 Iteration: 304/313 val_loss0.29281426761544455 val_correct: 0.8924753289473685\n",
      "STEP: VAL Epoch: 1/1 Iteration: 305/313 val_loss0.2926887267070716 val_correct: 0.8923155737704918\n",
      "STEP: VAL Epoch: 1/1 Iteration: 306/313 val_loss0.2923095875048365 val_correct: 0.8924632352941176\n",
      "STEP: VAL Epoch: 1/1 Iteration: 307/313 val_loss0.29173819006023266 val_correct: 0.8927117263843648\n",
      "STEP: VAL Epoch: 1/1 Iteration: 308/313 val_loss0.29113979796068623 val_correct: 0.8928571428571429\n",
      "STEP: VAL Epoch: 1/1 Iteration: 309/313 val_loss0.29100555942234096 val_correct: 0.8929004854368932\n",
      "STEP: VAL Epoch: 1/1 Iteration: 310/313 val_loss0.2904749078015166 val_correct: 0.8931451612903226\n",
      "STEP: VAL Epoch: 1/1 Iteration: 311/313 val_loss0.29041773625844164 val_correct: 0.8931872990353698\n",
      "STEP: VAL Epoch: 1/1 Iteration: 312/313 val_loss0.29126229183748364 val_correct: 0.8927283653846154\n",
      "STEP: VAL Epoch: 1/1 Iteration: 313/313 val_loss0.29066723479202 val_correct: 0.891473642172524\n",
      "Model Save !\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "output_dir = \"./output\"\n",
    "check = None\n",
    "train_loss_list = []\n",
    "train_correct_list =[]\n",
    "val_loss_list = []\n",
    "val_correct_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step in [\"train\", \"val\"]:\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "        min_loss = np.inf\n",
    "        if step == \"train\":\n",
    "            model.train()\n",
    "            dataloader = train_dataloader\n",
    "        else:\n",
    "            model.eval()\n",
    "            dataloader = val_dataloader\n",
    "        for batch, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(step==\"train\"):\n",
    "                outputs = model(images)\n",
    "                pred = torch.argmax(outputs, dim=-1)\n",
    "                loss = (criterion(outputs, labels))\n",
    "                correct = (torch.sum(pred == labels)) / batch_size\n",
    "\n",
    "                if step == \"train\":\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += correct.item()\n",
    "            if step == \"train\":\n",
    "                print(f\"STEP: Train Epoch: {epoch + 1}/{epochs} Iteration: {batch + 1}/{len(dataloader)} train_loss{running_loss / (batch + 1)}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"STEP: VAL Epoch: {epoch + 1}/{epochs} Iteration: {batch + 1}/{len(dataloader)} val_loss{running_loss / (batch + 1)} val_correct: {running_corrects / (batch + 1)}\")\n",
    "                \n",
    "        if step == \"train\":\n",
    "            train_loss_list.append(running_loss / (batch + 1))\n",
    "            train_correct_list.append(running_corrects / (batch + 1))\n",
    "        else:\n",
    "            val_loss_list.append(running_loss / (batch + 1))\n",
    "            val_correct_list.append(running_corrects / (batch + 1))     \n",
    "\n",
    "    if running_loss < min_loss:\n",
    "        early_stopping.checkCount(True)\n",
    "        print(\"Model Save !\")\n",
    "        min_loss = running_loss\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, \"model.pth\"))\n",
    "    else:\n",
    "        check = early_stopping.checkCount(False)\n",
    "    if check == 0:\n",
    "        print(\"Early Stopping !\") \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 2.5)\n",
    "plt.plot(range(epochs), train_loss_list, label=\"train_loss\")\n",
    "plt.plot(range(epochs), val_loss_list, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.plot(range(epochs), train_correct_list, label=\"train_correct\")\n",
    "plt.plot(range(epochs), val_correct_list, label=\"val_correct\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index :19 Max correct : 0.9196285942492013\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Index :{np.argmax(val_correct_list)} Max correct : {np.max(val_correct_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
